[
  {
    "objectID": "Certification/DataQ_Example/DataQ_Example.html",
    "href": "Certification/DataQ_Example/DataQ_Example.html",
    "title": "빅분기 실기 - DataQ 제공문제",
    "section": "",
    "text": "DataQ 예시문제"
  },
  {
    "objectID": "Certification/DataQ_Example/DataQ_Example.html#작업형-1",
    "href": "Certification/DataQ_Example/DataQ_Example.html#작업형-1",
    "title": "빅분기 실기 - DataQ 제공문제",
    "section": "작업형 1",
    "text": "작업형 1\n\n\n\n\n\nCode\nmtcars = pd.read_csv('https://gist.githubusercontent.com/seankross/a412dfbd88b3db70b74b/raw/5f23f993cd87c283ce766e7ac6b329ee7cc2e1d1/mtcars.csv')\n\nmtcars['qsec'] = (mtcars['qsec'] - mtcars['qsec'].min()) / (mtcars['qsec'].max() - mtcars['qsec'].min())\nprint(len(mtcars[mtcars['qsec'] > 0.5]))\n\n\n9"
  },
  {
    "objectID": "Certification/DataQ_Example/DataQ_Example.html#작업형-2",
    "href": "Certification/DataQ_Example/DataQ_Example.html#작업형-2",
    "title": "빅분기 실기 - DataQ 제공문제",
    "section": "작업형 2",
    "text": "작업형 2\n \n\n\nCode\nx_train = pd.read_csv('https://ndisk2.youngjin.com/cd/bigdata/quiz2/X_train.csv')\ny_train = pd.read_csv('https://ndisk2.youngjin.com/cd/bigdata/quiz2/y_train.csv')\nx_test = pd.read_csv('https://ndisk2.youngjin.com/cd/bigdata/quiz2/X_test.csv')\n\nx_train.head()\n\n\n\n\n\n\n  \n    \n      \n      cust_id\n      총구매액\n      최대구매액\n      환불금액\n      주구매상품\n      주구매지점\n      내점일수\n      내점당구매건수\n      주말방문비율\n      구매주기\n    \n  \n  \n    \n      0\n      0\n      68282840\n      11264000\n      6860000.0\n      기타\n      강남점\n      19\n      3.894737\n      0.527027\n      17\n    \n    \n      1\n      1\n      2136000\n      2136000\n      300000.0\n      스포츠\n      잠실점\n      2\n      1.500000\n      0.000000\n      1\n    \n    \n      2\n      2\n      3197000\n      1639000\n      NaN\n      남성 캐주얼\n      관악점\n      2\n      2.000000\n      0.000000\n      1\n    \n    \n      3\n      3\n      16077620\n      4935000\n      NaN\n      기타\n      광주점\n      18\n      2.444444\n      0.318182\n      16\n    \n    \n      4\n      4\n      29050000\n      24000000\n      NaN\n      보석\n      본  점\n      2\n      1.500000\n      0.000000\n      85\n    \n  \n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nx_train['환불금액'] = x_train['환불금액'].fillna(x_train['환불금액'].mean())\nx_train['실제구매액'] = x_train['총구매액'] - x_train['환불금액']\n\nx_test['환불금액'] = x_test['환불금액'].fillna(x_test['환불금액'].mean())\nx_test['실제구매액'] = x_test['총구매액'] - x_test['환불금액']\n\nqual_col = ['주구매상품', '주구매지점']\n\nfor col in qual_col :\n    le = LabelEncoder()\n    x_train[col] = le.fit_transform(x_train[col])\n    x_test[col] = le.transform(x_test[col])\n\n\n\n\nCode\nskf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\nx_data, y_data = x_train.drop(columns = 'cust_id'), y_train['gender']\n\nrf_clf = RandomForestClassifier(random_state = 42)\n\nparam_grid = {'n_estimators' : [100, 200, 500],\n              'max_depth' : [5, 10, 20],\n              'max_leaf_nodes' : [5, 10],\n              'criterion' : ['entropy', 'gini']}\n\n\ngrid_search = GridSearchCV(rf_clf, param_grid = param_grid, cv = skf, scoring = 'roc_auc')\ngrid_search.fit(x_data, y_data)\n\nprint('Best Parameters : ', grid_search.best_params_)\nprint('Best Score : ', round(grid_search.best_score_, 4))\n\n\nBest Parameters :  {'criterion': 'gini', 'max_depth': 5, 'max_leaf_nodes': 10, 'n_estimators': 100}\nBest Score :  0.657\n\n\n\n\nCode\nvalid_rocauc = []\ncnt = 1\nskf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\nx_data, y_data = x_train.drop(columns = 'cust_id'), y_train['gender']\n\nfor train_idx, valid_idx in skf.split(x_data, y_data) :\n    x_train, y_train = x_data.iloc[train_idx], y_data.iloc[train_idx]\n    x_valid, y_valid = x_data.iloc[valid_idx], y_data.iloc[valid_idx]\n    \n    rf_clf = RandomForestClassifier(**grid_search.best_params_, random_state = 42)\n    rf_clf.fit(x_train, y_train)\n    \n    valid_score = roc_auc_score(rf_clf.predict(x_valid), y_valid)\n    print(f'{cnt}번째 Fold ROC-AUC Score : {valid_score}')\n    valid_rocauc.append(valid_score)\n    cnt += 1\n\nprint(f'모델 평균 ROC-AUC Score : {np.array(valid_rocauc).mean()}')\n\n\n1번째 Fold ROC-AUC Score : 0.6360665478312537\n2번째 Fold ROC-AUC Score : 0.6384848484848484\n3번째 Fold ROC-AUC Score : 0.6005844645550528\n4번째 Fold ROC-AUC Score : 0.5908670196736837\n5번째 Fold ROC-AUC Score : 0.5769110275689223\n모델 평균 ROC-AUC Score : 0.6085827816227523\n\n\n\n\nCode\npred = rf_clf.predict_proba(x_test.drop(columns = 'cust_id'))[:, 1]\npred\n# pd.DataFrame({'cust_id': x_test['cust_id'], 'gender': pred}).to_csv('000000000.csv', index = False)\n\n\narray([0.44694982, 0.17945736, 0.24673574, ..., 0.43038002, 0.33473529,\n       0.58964348])"
  },
  {
    "objectID": "Certification/DataQ_Example/DataQ_Example.html#작업형-3",
    "href": "Certification/DataQ_Example/DataQ_Example.html#작업형-3",
    "title": "빅분기 실기 - DataQ 제공문제",
    "section": "작업형 3",
    "text": "작업형 3\n\n\n\n\n\nCode\na = pd.read_csv('https://raw.githubusercontent.com/Opensourcefordatascience/Data-sets/master/blood_pressure.csv')\na.head()\n\n\n\n\n\n\n  \n    \n      \n      patient\n      sex\n      agegrp\n      bp_before\n      bp_after\n    \n  \n  \n    \n      0\n      1\n      Male\n      30-45\n      143\n      153\n    \n    \n      1\n      2\n      Male\n      30-45\n      163\n      170\n    \n    \n      2\n      3\n      Male\n      30-45\n      153\n      168\n    \n    \n      3\n      4\n      Male\n      30-45\n      153\n      142\n    \n    \n      4\n      5\n      Male\n      30-45\n      146\n      141"
  },
  {
    "objectID": "Certification/Preprocessing.html",
    "href": "Certification/Preprocessing.html",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "",
    "text": "판다스 연습 튜토리얼"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-1",
    "href": "Certification/Preprocessing.html#question-1",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 1",
    "text": "Question 1\n\nData\n\n롤 랭킹 데이터 : https://www.kaggle.com/datasnaek/league-of-legends\n\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/lol.csv’\n\n\n데이터를 로드하라. 데이터는 \\t을 기준으로 구분되어있다.\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/lol.csv', sep = '\\t')\ntype(df)\n\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-2",
    "href": "Certification/Preprocessing.html#question-2",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 2",
    "text": "Question 2\n데이터의 상위 5개 행을 출력하라\n\n\nCode\nAns = df.head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      gameId\n      creationTime\n      gameDuration\n      seasonId\n      winner\n      ...\n      t2_ban1\n      t2_ban2\n      t2_ban3\n      t2_ban4\n      t2_ban5\n    \n  \n  \n    \n      0\n      3326086514\n      1504279457970\n      1949\n      9\n      1\n      ...\n      114\n      67\n      43\n      16\n      51\n    \n    \n      1\n      3229566029\n      1497848803862\n      1851\n      9\n      1\n      ...\n      11\n      67\n      238\n      51\n      420\n    \n    \n      2\n      3327363504\n      1504360103310\n      1493\n      9\n      1\n      ...\n      157\n      238\n      121\n      57\n      28\n    \n    \n      3\n      3326856598\n      1504348503996\n      1758\n      9\n      1\n      ...\n      164\n      18\n      141\n      40\n      51\n    \n    \n      4\n      3330080762\n      1504554410899\n      2094\n      9\n      1\n      ...\n      86\n      11\n      201\n      122\n      18\n    \n  \n\n5 rows × 61 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-3",
    "href": "Certification/Preprocessing.html#question-3",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 3",
    "text": "Question 3\n데이터의 행과 열의 갯수를 파악하라\n\n\nCode\nAns = df.shape\nAns\n\n\n(51490, 61)"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-4",
    "href": "Certification/Preprocessing.html#question-4",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 4",
    "text": "Question 4\n전체 컬럼을 출력하라\n\n\nCode\nAns = df.columns\nAns\n\n\nIndex(['gameId', 'creationTime', 'gameDuration', 'seasonId', 'winner',\n       'firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron',\n       'firstDragon', 'firstRiftHerald', 't1_champ1id', 't1_champ1_sum1',\n       't1_champ1_sum2', 't1_champ2id', 't1_champ2_sum1', 't1_champ2_sum2',\n       't1_champ3id', 't1_champ3_sum1', 't1_champ3_sum2', 't1_champ4id',\n       't1_champ4_sum1', 't1_champ4_sum2', 't1_champ5id', 't1_champ5_sum1',\n       't1_champ5_sum2', 't1_towerKills', 't1_inhibitorKills', 't1_baronKills',\n       't1_dragonKills', 't1_riftHeraldKills', 't1_ban1', 't1_ban2', 't1_ban3',\n       't1_ban4', 't1_ban5', 't2_champ1id', 't2_champ1_sum1', 't2_champ1_sum2',\n       't2_champ2id', 't2_champ2_sum1', 't2_champ2_sum2', 't2_champ3id',\n       't2_champ3_sum1', 't2_champ3_sum2', 't2_champ4id', 't2_champ4_sum1',\n       't2_champ4_sum2', 't2_champ5id', 't2_champ5_sum1', 't2_champ5_sum2',\n       't2_towerKills', 't2_inhibitorKills', 't2_baronKills', 't2_dragonKills',\n       't2_riftHeraldKills', 't2_ban1', 't2_ban2', 't2_ban3', 't2_ban4',\n       't2_ban5'],\n      dtype='object')"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-5",
    "href": "Certification/Preprocessing.html#question-5",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 5",
    "text": "Question 5\n6번째 컬럼명을 출력하라\n\n\nCode\nAns = df.columns[5]\nAns\n\n\n'firstBlood'"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-6",
    "href": "Certification/Preprocessing.html#question-6",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 6",
    "text": "Question 6\n6번째 컬럼의 데이터 타입을 확인하라\n\n\nCode\nAns = df.iloc[:, 5].dtype\nAns\n\n\ndtype('int64')"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-7",
    "href": "Certification/Preprocessing.html#question-7",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 7",
    "text": "Question 7\n데이터셋의 인덱스 구성은 어떤가\n\n\nCode\nAns = df.index\nAns\n\n\nRangeIndex(start=0, stop=51490, step=1)"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-8",
    "href": "Certification/Preprocessing.html#question-8",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 8",
    "text": "Question 8\n6번째 컬럼의 3번째 값은 무엇인가?\n\n\nCode\nAns = df.iloc[2, 5]\nAns\n\n\n2"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-9",
    "href": "Certification/Preprocessing.html#question-9",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 9",
    "text": "Question 9\n\nData\n\n제주 날씨, 인구에 따른 교통량데이터 (출처 : 제주 데이터 허브)\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/Jeju.csv’\n\n\n데이터를 로드하라. 컬럼이 한글이기에 적절한 처리해줘야함\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/Jeju.csv', encoding = 'CP949')\ntype(df)\n\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-10",
    "href": "Certification/Preprocessing.html#question-10",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 10",
    "text": "Question 10\n데이터 마지막 3개행을 출력하라\n\n\nCode\nAns = df.tail(3)\nAns\n\n\n\n\n\n\n  \n    \n      \n      id\n      일자\n      시도명\n      읍면동명\n      거주인구\n      ...\n      평균 속도\n      평균 소요 시간\n      평균 기온\n      일강수량\n      평균 풍속\n    \n  \n  \n    \n      9618\n      32066\n      2020-04-30\n      제주시\n      도두동\n      28397.481\n      ...\n      41.053\n      29.421\n      20.3\n      0.0\n      3.0\n    \n    \n      9619\n      32067\n      2020-04-30\n      서귀포시\n      안덕면\n      348037.846\n      ...\n      46.595\n      49.189\n      17.6\n      0.0\n      3.5\n    \n    \n      9620\n      32068\n      2020-04-30\n      제주시\n      연동\n      1010643.372\n      ...\n      40.863\n      27.765\n      14.1\n      0.0\n      4.8\n    \n  \n\n3 rows × 13 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-11",
    "href": "Certification/Preprocessing.html#question-11",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 11",
    "text": "Question 11\n수치형 변수를 가진 컬럼을 출력하라\n\n\nCode\nAns = df.columns[df.dtypes != 'object']\nAns\n\n\nIndex(['id', '거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', '평균 소요 시간', '평균 기온',\n       '일강수량', '평균 풍속'],\n      dtype='object')\n\n\n\n\nCode\nAns = df.select_dtypes(exclude = 'object').columns\nAns\n\n\nIndex(['id', '거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', '평균 소요 시간', '평균 기온',\n       '일강수량', '평균 풍속'],\n      dtype='object')"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-12",
    "href": "Certification/Preprocessing.html#question-12",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 12",
    "text": "Question 12\n범주형 변수를 가진 컬럼을 출력하라\n\n\nCode\nAns = df.select_dtypes(include = 'object').columns\nAns\n\n\nIndex(['일자', '시도명', '읍면동명'], dtype='object')"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-13",
    "href": "Certification/Preprocessing.html#question-13",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 13",
    "text": "Question 13\n각 컬럼의 결측치 숫자를 파악하라\n\n\nCode\nAns = df.isnull().sum()\nAns\n\n\nid          0\n일자          0\n시도명         0\n읍면동명        0\n거주인구        0\n           ..\n평균 속도       0\n평균 소요 시간    0\n평균 기온       0\n일강수량        0\n평균 풍속       0\nLength: 13, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-14",
    "href": "Certification/Preprocessing.html#question-14",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 14",
    "text": "Question 14\n각 컬럼의 데이터수, 데이터타입을 한번에 확인하라\n\n\nCode\nAns = df.info()\nAns\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9621 entries, 0 to 9620\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   id        9621 non-null   int64  \n 1   일자        9621 non-null   object \n 2   시도명       9621 non-null   object \n 3   읍면동명      9621 non-null   object \n 4   거주인구      9621 non-null   float64\n 5   근무인구      9621 non-null   float64\n 6   방문인구      9621 non-null   float64\n 7   총 유동인구    9621 non-null   float64\n 8   평균 속도     9621 non-null   float64\n 9   평균 소요 시간  9621 non-null   float64\n 10  평균 기온     9621 non-null   float64\n 11  일강수량      9621 non-null   float64\n 12  평균 풍속     9621 non-null   float64\ndtypes: float64(9), int64(1), object(3)\nmemory usage: 977.3+ KB"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-15",
    "href": "Certification/Preprocessing.html#question-15",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 15",
    "text": "Question 15\n각 수치형 변수의 분포(사분위, 평균, 표준편차, 최대 , 최소)를 확인하라\n\n\nCode\nAns = df.describe()\nAns\n\n\n\n\n\n\n  \n    \n      \n      id\n      거주인구\n      근무인구\n      방문인구\n      총 유동인구\n      평균 속도\n      평균 소요 시간\n      평균 기온\n      일강수량\n      평균 풍속\n    \n  \n  \n    \n      count\n      9621.000000\n      9.621000e+03\n      9621.000000\n      9621.000000\n      9.621000e+03\n      9621.000000\n      9621.000000\n      9621.000000\n      9621.000000\n      9621.000000\n    \n    \n      mean\n      27258.000000\n      3.174315e+05\n      35471.201510\n      195889.561802\n      5.487922e+05\n      41.109084\n      37.215873\n      13.550828\n      6.972426\n      2.753171\n    \n    \n      std\n      2777.487804\n      2.982079e+05\n      40381.214775\n      140706.090325\n      4.608802e+05\n      8.758631\n      12.993786\n      7.745515\n      27.617260\n      1.498538\n    \n    \n      min\n      22448.000000\n      9.305552e+03\n      1407.936000\n      11538.322000\n      2.225181e+04\n      24.333000\n      12.667000\n      -9.600000\n      0.000000\n      0.000000\n    \n    \n      25%\n      24853.000000\n      9.539939e+04\n      12074.498000\n      99632.153000\n      2.216910e+05\n      34.250000\n      27.889000\n      7.600000\n      0.000000\n      1.700000\n    \n    \n      50%\n      27258.000000\n      2.221105e+05\n      21960.928000\n      152805.335000\n      3.866935e+05\n      39.640000\n      34.500000\n      13.400000\n      0.000000\n      2.400000\n    \n    \n      75%\n      29663.000000\n      4.106671e+05\n      40192.032000\n      236325.109000\n      6.406918e+05\n      49.105000\n      46.176000\n      19.700000\n      1.500000\n      3.400000\n    \n    \n      max\n      32068.000000\n      1.364504e+06\n      263476.965000\n      723459.209000\n      2.066484e+06\n      103.000000\n      172.200000\n      30.400000\n      587.500000\n      13.333000"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-16",
    "href": "Certification/Preprocessing.html#question-16",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 16",
    "text": "Question 16\n거주인구 컬럼의 값들을 출력하라\n\n\nCode\nAns = df['거주인구']\nAns\n\n\n0         32249.987\n1        213500.997\n2       1212382.218\n3         33991.653\n4        155036.925\n           ...     \n9616     228260.005\n9617     459959.064\n9618      28397.481\n9619     348037.846\n9620    1010643.372\nName: 거주인구, Length: 9621, dtype: float64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-17",
    "href": "Certification/Preprocessing.html#question-17",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 17",
    "text": "Question 17\n평균 속도 컬럼의 4분위 범위(IQR) 값을 구하여라\n\n\nCode\n# Ans = df['평균 속도'].describe()['75%'] - df['평균 속도'].describe()['25%']\nAns = df['평균 속도'].quantile(.75) - df['평균 속도'].quantile(.25)\nAns\n\n\n14.854999999999997"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-18",
    "href": "Certification/Preprocessing.html#question-18",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 18",
    "text": "Question 18\n읍면동명 컬럼의 유일값 갯수를 출력하라\n\n\nCode\nAns = df['읍면동명'].nunique()\nAns\n\n\n41"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-19",
    "href": "Certification/Preprocessing.html#question-19",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 19",
    "text": "Question 19\n읍면동명 컬럼의 유일값을 모두 출력하라\n\n\nCode\nAns = df['읍면동명'].unique()\nAns\n\n\narray(['도두동', '외도동', '이도2동', '일도1동', '대천동', '서홍동', '한경면', '송산동', '조천읍',\n       '일도2동', '영천동', '예래동', '대륜동', '삼도1동', '이호동', '건입동', '중앙동', '삼양동',\n       '삼도2동', '이도1동', '남원읍', '대정읍', '정방동', '효돈동', '아라동', '한림읍', '구좌읍',\n       '용담1동', '오라동', '화북동', '연동', '표선면', '중문동', '성산읍', '안덕면', '천지동',\n       '노형동', '동홍동', '용담2동', '봉개동', '애월읍'], dtype=object)"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-20",
    "href": "Certification/Preprocessing.html#question-20",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 20",
    "text": "Question 20\n\nData\n\n식당데이터 : https://github.com/justmarkham/DAT8/blob/master/data/chipotle.tsv\n\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/chipo.csv’\n\n\n데이터를 로드하라.\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/chipo.csv')\nAns = df.head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n    \n  \n  \n    \n      0\n      1\n      1\n      Chips and Fresh Tomato Salsa\n      NaN\n      $2.39\n    \n    \n      1\n      1\n      1\n      Izze\n      [Clementine]\n      $3.39\n    \n    \n      2\n      1\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n    \n    \n      3\n      1\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      NaN\n      $2.39\n    \n    \n      4\n      2\n      2\n      Chicken Bowl\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $16.98"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-21",
    "href": "Certification/Preprocessing.html#question-21",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 21",
    "text": "Question 21\nquantity컬럼 값이 3인 데이터를 추출하여 첫 5행을 출력하라\n\n\nCode\nAns = df[df['quantity'] == 3].head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n    \n  \n  \n    \n      409\n      178\n      3\n      Chicken Bowl\n      [[Fresh Tomato Salsa (Mild), Tomatillo-Green C...\n      $32.94\n    \n    \n      445\n      193\n      3\n      Bowl\n      [Braised Carnitas, Pinto Beans, [Sour Cream, C...\n      $22.20\n    \n    \n      689\n      284\n      3\n      Canned Soft Drink\n      [Diet Coke]\n      $3.75\n    \n    \n      818\n      338\n      3\n      Bottled Water\n      NaN\n      $3.27\n    \n    \n      850\n      350\n      3\n      Canned Soft Drink\n      [Sprite]\n      $3.75"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-22",
    "href": "Certification/Preprocessing.html#question-22",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 22",
    "text": "Question 22\nquantity컬럼 값이 3인 데이터를 추출하여 index를 0부터 정렬하고 첫 5행을 출력하라\n\n\nCode\nAns = df[df['quantity'] == 3].reset_index(drop = True).head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n    \n  \n  \n    \n      0\n      178\n      3\n      Chicken Bowl\n      [[Fresh Tomato Salsa (Mild), Tomatillo-Green C...\n      $32.94\n    \n    \n      1\n      193\n      3\n      Bowl\n      [Braised Carnitas, Pinto Beans, [Sour Cream, C...\n      $22.20\n    \n    \n      2\n      284\n      3\n      Canned Soft Drink\n      [Diet Coke]\n      $3.75\n    \n    \n      3\n      338\n      3\n      Bottled Water\n      NaN\n      $3.27\n    \n    \n      4\n      350\n      3\n      Canned Soft Drink\n      [Sprite]\n      $3.75"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-23",
    "href": "Certification/Preprocessing.html#question-23",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 23",
    "text": "Question 23\nquantity , item_price 두개의 컬럼으로 구성된 새로운 데이터 프레임을 정의하라\n\n\nCode\nAns = df[['quantity', 'item_price']]\nAns\n\n\n\n\n\n\n  \n    \n      \n      quantity\n      item_price\n    \n  \n  \n    \n      0\n      1\n      $2.39\n    \n    \n      1\n      1\n      $3.39\n    \n    \n      2\n      1\n      $3.39\n    \n    \n      3\n      1\n      $2.39\n    \n    \n      4\n      2\n      $16.98\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      4617\n      1\n      $11.75\n    \n    \n      4618\n      1\n      $11.75\n    \n    \n      4619\n      1\n      $11.25\n    \n    \n      4620\n      1\n      $8.75\n    \n    \n      4621\n      1\n      $8.75\n    \n  \n\n4622 rows × 2 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-24",
    "href": "Certification/Preprocessing.html#question-24",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 24",
    "text": "Question 24\nitem_price 컬럼의 달러표시 문자를 제거하고 float 타입으로 저장하여 new_price 컬럼에 저장하라\n\n\nCode\ndf['new_price'] = df['item_price'].str.replace('$', '').astype('float')\nAns = df['new_price']\nAns\n\n\n0        2.39\n1        3.39\n2        3.39\n3        2.39\n4       16.98\n        ...  \n4617    11.75\n4618    11.75\n4619    11.25\n4620     8.75\n4621     8.75\nName: new_price, Length: 4622, dtype: float64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-25",
    "href": "Certification/Preprocessing.html#question-25",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 25",
    "text": "Question 25\nnew_price 컬럼이 5이하의 값을 가지는 데이터프레임을 추출하고, 전체 갯수를 구하여라\n\n\nCode\nAns = len(df[df['new_price'] <= 5])\nAns\n\n\n1652"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-26",
    "href": "Certification/Preprocessing.html#question-26",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 26",
    "text": "Question 26\nitem_name명이 Chicken Salad Bowl 인 데이터 프레임을 추출하고, index 값을 초기화해라\n\n\nCode\nAns = df[df['item_name'] == 'Chicken Salad Bowl'].reset_index(drop = True)\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      0\n      20\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n    \n      1\n      60\n      2\n      Chicken Salad Bowl\n      [Tomatillo Green Chili Salsa, [Sour Cream, Che...\n      $22.50\n      22.50\n    \n    \n      2\n      94\n      2\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $22.50\n      22.50\n    \n    \n      3\n      111\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $8.75\n      8.75\n    \n    \n      4\n      137\n      2\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, Fajita Vegetables]\n      $17.50\n      17.50\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      105\n      1813\n      2\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $17.50\n      17.50\n    \n    \n      106\n      1822\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Black Beans, Cheese, Gua...\n      $11.25\n      11.25\n    \n    \n      107\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $11.25\n      11.25\n    \n    \n      108\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n      $8.75\n      8.75\n    \n    \n      109\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n  \n\n110 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-27",
    "href": "Certification/Preprocessing.html#question-27",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 27",
    "text": "Question 27\nnew_price값이 9 이하이고 item_name 값이 Chicken Salad Bowl 인 데이터 프레임을 추출하라\n\n\nCode\nAns = df[(df['new_price'] <= 9) & (df['item_name'] == 'Chicken Salad Bowl')]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      44\n      20\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n    \n      256\n      111\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $8.75\n      8.75\n    \n    \n      526\n      220\n      1\n      Chicken Salad Bowl\n      [Roasted Chili Corn Salsa, [Black Beans, Sour ...\n      $8.75\n      8.75\n    \n    \n      528\n      221\n      1\n      Chicken Salad Bowl\n      [Tomatillo Green Chili Salsa, [Fajita Vegetabl...\n      $8.75\n      8.75\n    \n    \n      529\n      221\n      1\n      Chicken Salad Bowl\n      [Tomatillo Green Chili Salsa, [Fajita Vegetabl...\n      $8.75\n      8.75\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4352\n      1738\n      1\n      Chicken Salad Bowl\n      [Tomatillo Red Chili Salsa, [Rice, Fajita Vege...\n      $8.75\n      8.75\n    \n    \n      4439\n      1769\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Rice, Black Beans, Chees...\n      $8.75\n      8.75\n    \n    \n      4520\n      1797\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, Lettuce]\n      $8.75\n      8.75\n    \n    \n      4620\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n      $8.75\n      8.75\n    \n    \n      4621\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n  \n\n56 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-28",
    "href": "Certification/Preprocessing.html#question-28",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 28",
    "text": "Question 28\ndf의 new_price 컬럼 값에 따라 오름차순으로 정리하고 index를 초기화 하여라\n\n\nCode\nAns = df.sort_values(by = 'new_price').reset_index(drop = True)\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      0\n      471\n      1\n      Bottled Water\n      NaN\n      $1.09\n      1.09\n    \n    \n      1\n      338\n      1\n      Canned Soda\n      [Coca Cola]\n      $1.09\n      1.09\n    \n    \n      2\n      1575\n      1\n      Canned Soda\n      [Dr. Pepper]\n      $1.09\n      1.09\n    \n    \n      3\n      47\n      1\n      Canned Soda\n      [Dr. Pepper]\n      $1.09\n      1.09\n    \n    \n      4\n      1014\n      1\n      Canned Soda\n      [Coca Cola]\n      $1.09\n      1.09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4617\n      1443\n      3\n      Veggie Burrito\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $33.75\n      33.75\n    \n    \n      4618\n      1443\n      4\n      Chicken Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Chees...\n      $35.00\n      35.00\n    \n    \n      4619\n      511\n      4\n      Chicken Burrito\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $35.00\n      35.00\n    \n    \n      4620\n      1398\n      3\n      Carnitas Bowl\n      [Roasted Chili Corn Salsa, [Fajita Vegetables,...\n      $35.25\n      35.25\n    \n    \n      4621\n      1443\n      15\n      Chips and Fresh Tomato Salsa\n      NaN\n      $44.25\n      44.25\n    \n  \n\n4622 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-29",
    "href": "Certification/Preprocessing.html#question-29",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 29",
    "text": "Question 29\ndf의 item_name 컬럼 값중 Chips 포함하는 경우의 데이터를 출력하라\n\n\nCode\nAns = df[df['item_name'].str.contains('Chips')]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      0\n      1\n      1\n      Chips and Fresh Tomato Salsa\n      NaN\n      $2.39\n      2.39\n    \n    \n      3\n      1\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      NaN\n      $2.39\n      2.39\n    \n    \n      6\n      3\n      1\n      Side of Chips\n      NaN\n      $1.69\n      1.69\n    \n    \n      10\n      5\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n    \n      14\n      7\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4596\n      1826\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n    \n      4600\n      1827\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n    \n      4605\n      1828\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n    \n      4613\n      1831\n      1\n      Chips\n      NaN\n      $2.15\n      2.15\n    \n    \n      4616\n      1832\n      1\n      Chips and Guacamole\n      NaN\n      $4.45\n      4.45\n    \n  \n\n1084 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-30",
    "href": "Certification/Preprocessing.html#question-30",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 30",
    "text": "Question 30\ndf의 짝수번째 컬럼만을 포함하는 데이터프레임을 출력하라\n\n\nCode\nAns = df.iloc[:, 0::2]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      item_name\n      item_price\n    \n  \n  \n    \n      0\n      1\n      Chips and Fresh Tomato Salsa\n      $2.39\n    \n    \n      1\n      1\n      Izze\n      $3.39\n    \n    \n      2\n      1\n      Nantucket Nectar\n      $3.39\n    \n    \n      3\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      $2.39\n    \n    \n      4\n      2\n      Chicken Bowl\n      $16.98\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4617\n      1833\n      Steak Burrito\n      $11.75\n    \n    \n      4618\n      1833\n      Steak Burrito\n      $11.75\n    \n    \n      4619\n      1834\n      Chicken Salad Bowl\n      $11.25\n    \n    \n      4620\n      1834\n      Chicken Salad Bowl\n      $8.75\n    \n    \n      4621\n      1834\n      Chicken Salad Bowl\n      $8.75\n    \n  \n\n4622 rows × 3 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-31",
    "href": "Certification/Preprocessing.html#question-31",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 31",
    "text": "Question 31\ndf의 new_price 컬럼 값에 따라 내림차순으로 정리하고 index를 초기화 하여라\n\n\nCode\nAns = df.sort_values(by = 'new_price', ascending = False).reset_index(drop = True)\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      0\n      1443\n      15\n      Chips and Fresh Tomato Salsa\n      NaN\n      $44.25\n      44.25\n    \n    \n      1\n      1398\n      3\n      Carnitas Bowl\n      [Roasted Chili Corn Salsa, [Fajita Vegetables,...\n      $35.25\n      35.25\n    \n    \n      2\n      511\n      4\n      Chicken Burrito\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $35.00\n      35.00\n    \n    \n      3\n      1443\n      4\n      Chicken Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Chees...\n      $35.00\n      35.00\n    \n    \n      4\n      1443\n      3\n      Veggie Burrito\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $33.75\n      33.75\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4617\n      1578\n      1\n      Canned Soda\n      [Diet Dr. Pepper]\n      $1.09\n      1.09\n    \n    \n      4618\n      1162\n      1\n      Bottled Water\n      NaN\n      $1.09\n      1.09\n    \n    \n      4619\n      567\n      1\n      Canned Soda\n      [Coca Cola]\n      $1.09\n      1.09\n    \n    \n      4620\n      1014\n      1\n      Canned Soda\n      [Coca Cola]\n      $1.09\n      1.09\n    \n    \n      4621\n      591\n      1\n      Canned Soda\n      [Sprite]\n      $1.09\n      1.09\n    \n  \n\n4622 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-32",
    "href": "Certification/Preprocessing.html#question-32",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 32",
    "text": "Question 32\ndf의 item_name 컬럼 값이 Steak Salad 또는 Bowl 인 데이터를 인덱싱하라\n\n\nCode\nAns = df[(df['item_name'] == 'Steak Salad') | (df['item_name'] == 'Bowl')]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      445\n      193\n      3\n      Bowl\n      [Braised Carnitas, Pinto Beans, [Sour Cream, C...\n      $22.20\n      22.20\n    \n    \n      664\n      276\n      1\n      Steak Salad\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $8.99\n      8.99\n    \n    \n      673\n      279\n      1\n      Bowl\n      [Adobo-Marinated and Grilled Steak, [Sour Crea...\n      $7.40\n      7.40\n    \n    \n      752\n      311\n      1\n      Steak Salad\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $8.99\n      8.99\n    \n    \n      893\n      369\n      1\n      Steak Salad\n      [Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n      $8.99\n      8.99\n    \n    \n      3502\n      1406\n      1\n      Steak Salad\n      [[Lettuce, Fajita Veggies]]\n      $8.69\n      8.69"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-33",
    "href": "Certification/Preprocessing.html#question-33",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 33",
    "text": "Question 33\ndf의 item_name 컬럼 값이 Steak Salad 또는 Bowl 인 데이터를 데이터 프레임화 한 후, item_name를 기준으로 중복행이 있으면 제거하되 첫번째 케이스만 남겨라\n\n\nCode\ndf_new = df[df['item_name'].isin(['Steak Salad', 'Bowl'])]\nAns = df_new.drop_duplicates(subset = 'item_name', keep = 'first')\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      445\n      193\n      3\n      Bowl\n      [Braised Carnitas, Pinto Beans, [Sour Cream, C...\n      $22.20\n      22.20\n    \n    \n      664\n      276\n      1\n      Steak Salad\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $8.99\n      8.99"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-34",
    "href": "Certification/Preprocessing.html#question-34",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 34",
    "text": "Question 34\ndf의 item_name 컬럼 값이 Steak Salad 또는 Bowl 인 데이터를 데이터 프레임화 한 후, item_name를 기준으로 중복행이 있으면 제거하되 마지막 케이스만 남겨라\n\n\nCode\ndf_new = df[df['item_name'].isin(['Steak Salad', 'Bowl'])]\nAns = df_new.drop_duplicates(subset = 'item_name', keep = 'last')\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      673\n      279\n      1\n      Bowl\n      [Adobo-Marinated and Grilled Steak, [Sour Crea...\n      $7.40\n      7.40\n    \n    \n      3502\n      1406\n      1\n      Steak Salad\n      [[Lettuce, Fajita Veggies]]\n      $8.69\n      8.69"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-35",
    "href": "Certification/Preprocessing.html#question-35",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 35",
    "text": "Question 35\ndf의 데이터 중 new_price값이 new_price값의 평균값 이상을 가지는 데이터들을 인덱싱하라\n\n\nCode\nAns = df[df['new_price'] >= df['new_price'].mean()]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      4\n      2\n      2\n      Chicken Bowl\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $16.98\n      16.98\n    \n    \n      5\n      3\n      1\n      Chicken Bowl\n      [Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n      $10.98\n      10.98\n    \n    \n      7\n      4\n      1\n      Steak Burrito\n      [Tomatillo Red Chili Salsa, [Fajita Vegetables...\n      $11.75\n      11.75\n    \n    \n      8\n      4\n      1\n      Steak Soft Tacos\n      [Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n      $9.25\n      9.25\n    \n    \n      9\n      5\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n      $9.25\n      9.25\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4617\n      1833\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Sour ...\n      $11.75\n      11.75\n    \n    \n      4618\n      1833\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n      $11.75\n      11.75\n    \n    \n      4619\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $11.25\n      11.25\n    \n    \n      4620\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n      $8.75\n      8.75\n    \n    \n      4621\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n  \n\n2890 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-36",
    "href": "Certification/Preprocessing.html#question-36",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 36",
    "text": "Question 36\ndf의 데이터 중 item_name의 값이 Izze 데이터를 Fizzy Lizzy로 수정하라\n\n\nCode\ndf.loc[df['item_name'] == 'Izze', 'item_name'] = 'Fizzy Lizzy'\nAns = df[df['item_name'] == 'Fizzy Lizzy']\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      1\n      1\n      1\n      Fizzy Lizzy\n      [Clementine]\n      $3.39\n      3.39\n    \n    \n      24\n      12\n      1\n      Fizzy Lizzy\n      [Grapefruit]\n      $3.39\n      3.39\n    \n    \n      47\n      21\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n    \n      66\n      30\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n    \n      359\n      155\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2619\n      1040\n      1\n      Fizzy Lizzy\n      [Clementine]\n      $3.39\n      3.39\n    \n    \n      2634\n      1046\n      1\n      Fizzy Lizzy\n      [Clementine]\n      $3.39\n      3.39\n    \n    \n      2758\n      1095\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n    \n      2891\n      1149\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n    \n      3669\n      1468\n      1\n      Fizzy Lizzy\n      [Blackberry]\n      $3.39\n      3.39\n    \n  \n\n20 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-37",
    "href": "Certification/Preprocessing.html#question-37",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 37",
    "text": "Question 37\ndf의 데이터 중 choice_description 값이 NaN 인 데이터의 갯수를 구하여라\n\n\nCode\nAns = len(df[df['choice_description'].isnull()])\nAns\n\n\n1246"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-38",
    "href": "Certification/Preprocessing.html#question-38",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 38",
    "text": "Question 38\ndf의 데이터 중 choice_description 값이 NaN 인 데이터를 NoData 값으로 대체하라(loc 이용)\n\n\nCode\ndf.loc[df['choice_description'].isnull(), 'choice_description'] = 'NoData'\nAns = len(df[df['choice_description'] == 'NoData'])\nAns\n\n\n1246"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-39",
    "href": "Certification/Preprocessing.html#question-39",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 39",
    "text": "Question 39\ndf의 데이터 중 choice_description 값에 Black이 들어가는 경우를 인덱싱하라\n\n\nCode\nAns = df[df['choice_description'].str.contains('Black')]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      4\n      2\n      2\n      Chicken Bowl\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $16.98\n      16.98\n    \n    \n      7\n      4\n      1\n      Steak Burrito\n      [Tomatillo Red Chili Salsa, [Fajita Vegetables...\n      $11.75\n      11.75\n    \n    \n      9\n      5\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n      $9.25\n      9.25\n    \n    \n      11\n      6\n      1\n      Chicken Crispy Tacos\n      [Roasted Chili Corn Salsa, [Fajita Vegetables,...\n      $8.75\n      8.75\n    \n    \n      12\n      6\n      1\n      Chicken Soft Tacos\n      [Roasted Chili Corn Salsa, [Rice, Black Beans,...\n      $8.75\n      8.75\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4604\n      1828\n      1\n      Chicken Bowl\n      [Fresh Tomato Salsa, [Rice, Black Beans, Chees...\n      $8.75\n      8.75\n    \n    \n      4608\n      1829\n      1\n      Veggie Burrito\n      [Tomatillo Red Chili Salsa, [Fajita Vegetables...\n      $11.25\n      11.25\n    \n    \n      4611\n      1830\n      1\n      Veggie Burrito\n      [Tomatillo Green Chili Salsa, [Rice, Fajita Ve...\n      $11.25\n      11.25\n    \n    \n      4612\n      1831\n      1\n      Carnitas Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n      $9.25\n      9.25\n    \n    \n      4617\n      1833\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Sour ...\n      $11.75\n      11.75\n    \n  \n\n1353 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-40",
    "href": "Certification/Preprocessing.html#question-40",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 40",
    "text": "Question 40\ndf의 데이터 중 choice_description 값에 Vegetables 들어가지 않는 경우의 갯수를 출력하라\n\n\nCode\nAns = len(df[~df['choice_description'].str.contains('Vegetables')])\nAns\n\n\n3900"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-41",
    "href": "Certification/Preprocessing.html#question-41",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 41",
    "text": "Question 41\ndf의 데이터 중 item_name 값이 N으로 시작하는 데이터를 모두 추출하라\n\n\nCode\nAns = df[df['item_name'].str.startswith('N')]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      2\n      1\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n      3.39\n    \n    \n      22\n      11\n      1\n      Nantucket Nectar\n      [Pomegranate Cherry]\n      $3.39\n      3.39\n    \n    \n      105\n      46\n      1\n      Nantucket Nectar\n      [Pineapple Orange Banana]\n      $3.39\n      3.39\n    \n    \n      173\n      77\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n      3.39\n    \n    \n      205\n      91\n      1\n      Nantucket Nectar\n      [Peach Orange]\n      $3.39\n      3.39\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3368\n      1351\n      1\n      Nantucket Nectar\n      [Pineapple Orange Banana]\n      $3.39\n      3.39\n    \n    \n      3570\n      1433\n      1\n      Nantucket Nectar\n      [Pineapple Orange Banana]\n      $3.39\n      3.39\n    \n    \n      3845\n      1541\n      1\n      Nantucket Nectar\n      [Peach Orange]\n      $3.39\n      3.39\n    \n    \n      4019\n      1609\n      1\n      Nantucket Nectar\n      [Pineapple Orange Banana]\n      $3.39\n      3.39\n    \n    \n      4078\n      1632\n      1\n      Nantucket Nectar\n      [Peach Orange]\n      $3.39\n      3.39\n    \n  \n\n27 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-42",
    "href": "Certification/Preprocessing.html#question-42",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 42",
    "text": "Question 42\ndf의 데이터 중 item_name 값의 단어갯수가 15개 이상인 데이터를 인덱싱하라\n\n\nCode\nAns = df[df['item_name'].str.len() >= 15]\nAns\n\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n      new_price\n    \n  \n  \n    \n      0\n      1\n      1\n      Chips and Fresh Tomato Salsa\n      NoData\n      $2.39\n      2.39\n    \n    \n      2\n      1\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n      3.39\n    \n    \n      3\n      1\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      NoData\n      $2.39\n      2.39\n    \n    \n      8\n      4\n      1\n      Steak Soft Tacos\n      [Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n      $9.25\n      9.25\n    \n    \n      10\n      5\n      1\n      Chips and Guacamole\n      NoData\n      $4.45\n      4.45\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4615\n      1832\n      1\n      Chicken Soft Tacos\n      [Fresh Tomato Salsa, [Rice, Cheese, Sour Cream]]\n      $8.75\n      8.75\n    \n    \n      4616\n      1832\n      1\n      Chips and Guacamole\n      NoData\n      $4.45\n      4.45\n    \n    \n      4619\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $11.25\n      11.25\n    \n    \n      4620\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n      $8.75\n      8.75\n    \n    \n      4621\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n      8.75\n    \n  \n\n2373 rows × 6 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-43",
    "href": "Certification/Preprocessing.html#question-43",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 43",
    "text": "Question 43\ndf의 데이터 중 new_price값이 lst에 해당하는 경우의 데이터 프레임을 구하고 그 갯수를 출력하라\nlst = [1.69, 2.39, 3.39, 4.45, 9.25, 10.98, 11.75, 16.98]\n\n\nCode\nlst = [1.69, 2.39, 3.39, 4.45, 9.25, 10.98, 11.75, 16.98]\nAns = len(df[df['new_price'].isin(lst)])\nAns\n\n\n1393"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-44",
    "href": "Certification/Preprocessing.html#question-44",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 44",
    "text": "Question 44\n\nData\n\n뉴욕 AirBnB : https://www.kaggle.com/ptoscano230382/air-bnb-ny-2019\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/AB_NYC_2019.csv’\n\n\n데이터를 로드하고 상위 5개 컬럼을 출력하라\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/AB_NYC_2019.csv')\nAns = df.head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      host_id\n      host_name\n      neighbourhood_group\n      ...\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n    \n  \n  \n    \n      0\n      2539\n      Clean & quiet apt home by the park\n      2787\n      John\n      Brooklyn\n      ...\n      9\n      2018-10-19\n      0.21\n      6\n      365\n    \n    \n      1\n      2595\n      Skylit Midtown Castle\n      2845\n      Jennifer\n      Manhattan\n      ...\n      45\n      2019-05-21\n      0.38\n      2\n      355\n    \n    \n      2\n      3647\n      THE VILLAGE OF HARLEM....NEW YORK !\n      4632\n      Elisabeth\n      Manhattan\n      ...\n      0\n      NaN\n      NaN\n      1\n      365\n    \n    \n      3\n      3831\n      Cozy Entire Floor of Brownstone\n      4869\n      LisaRoxanne\n      Brooklyn\n      ...\n      270\n      2019-07-05\n      4.64\n      1\n      194\n    \n    \n      4\n      5022\n      Entire Apt: Spacious Studio/Loft by central park\n      7192\n      Laura\n      Manhattan\n      ...\n      9\n      2018-11-19\n      0.10\n      1\n      0\n    \n  \n\n5 rows × 16 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-45",
    "href": "Certification/Preprocessing.html#question-45",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 45",
    "text": "Question 45\n데이터의 각 host_name의 빈도수를 구하고 host_name으로 정렬하여 상위 5개를 출력하라\n\n\nCode\n# Ans = df['host_name'].value_counts().sort_index()\nAns = df.groupby('host_name', as_index = False).size().head()\nAns\n\n\n\n\n\n\n  \n    \n      \n      host_name\n      size\n    \n  \n  \n    \n      0\n      'Cil\n      1\n    \n    \n      1\n      (Ari) HENRY LEE\n      1\n    \n    \n      2\n      (Email hidden by Airbnb)\n      6\n    \n    \n      3\n      (Mary) Haiy\n      1\n    \n    \n      4\n      -TheQueensCornerLot\n      1"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-46",
    "href": "Certification/Preprocessing.html#question-46",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 46",
    "text": "Question 46\n데이터의 각 host_name의 빈도수를 구하고 빈도수 기준 내림차순 정렬한 데이터 프레임을 만들어라. 빈도수 컬럼은 counts로 명명하라\n\n\nCode\nAns = df.groupby('host_name', as_index = False).size().sort_values(by = 'size', ascending = False).rename(columns = {'size' : 'counts'})\nAns\n\n\n\n\n\n\n  \n    \n      \n      host_name\n      counts\n    \n  \n  \n    \n      7135\n      Michael\n      417\n    \n    \n      2376\n      David\n      403\n    \n    \n      9781\n      Sonder (NYC)\n      327\n    \n    \n      4989\n      John\n      294\n    \n    \n      314\n      Alex\n      279\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      4804\n      Jerbean\n      1\n    \n    \n      4803\n      Jerald\n      1\n    \n    \n      4802\n      Jeonghoon\n      1\n    \n    \n      4800\n      Jeny\n      1\n    \n    \n      11451\n      현선\n      1\n    \n  \n\n11452 rows × 2 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-47",
    "href": "Certification/Preprocessing.html#question-47",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 47",
    "text": "Question 47\nneighbourhood_group의 값에 따른 neighbourhood컬럼 값의 갯수를 구하여라\n\n\nCode\nAns = df.groupby(['neighbourhood_group', 'neighbourhood'], as_index = False).size()\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood_group\n      neighbourhood\n      size\n    \n  \n  \n    \n      0\n      Bronx\n      Allerton\n      42\n    \n    \n      1\n      Bronx\n      Baychester\n      7\n    \n    \n      2\n      Bronx\n      Belmont\n      24\n    \n    \n      3\n      Bronx\n      Bronxdale\n      19\n    \n    \n      4\n      Bronx\n      Castle Hill\n      9\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      216\n      Staten Island\n      Tottenville\n      7\n    \n    \n      217\n      Staten Island\n      West Brighton\n      18\n    \n    \n      218\n      Staten Island\n      Westerleigh\n      2\n    \n    \n      219\n      Staten Island\n      Willowbrook\n      1\n    \n    \n      220\n      Staten Island\n      Woodrow\n      1\n    \n  \n\n221 rows × 3 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-48",
    "href": "Certification/Preprocessing.html#question-48",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 48",
    "text": "Question 48\nneighbourhood_group의 값에 따른 neighbourhood컬럼 값 중 neighbourhood_group그룹의 최댓값들을 출력하라\n\n\nCode\nAns = df.groupby(['neighbourhood_group', 'neighbourhood'], as_index = False).size().groupby('neighbourhood_group', as_index = False).max()\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood_group\n      neighbourhood\n      size\n    \n  \n  \n    \n      0\n      Bronx\n      Woodlawn\n      70\n    \n    \n      1\n      Brooklyn\n      Windsor Terrace\n      3920\n    \n    \n      2\n      Manhattan\n      West Village\n      2658\n    \n    \n      3\n      Queens\n      Woodside\n      900\n    \n    \n      4\n      Staten Island\n      Woodrow\n      48"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-49",
    "href": "Certification/Preprocessing.html#question-49",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 49",
    "text": "Question 49\nneighbourhood_group 값에 따른 price값의 평균, 분산, 최대, 최소 값을 구하여라\n\n\nCode\nAns = df.groupby('neighbourhood_group', as_index = False)['price'].agg(['mean', 'std', 'min', 'max'])\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood_group\n      mean\n      std\n      min\n      max\n    \n  \n  \n    \n      0\n      Bronx\n      87.496792\n      106.709349\n      0\n      2500\n    \n    \n      1\n      Brooklyn\n      124.383207\n      186.873538\n      0\n      10000\n    \n    \n      2\n      Manhattan\n      196.875814\n      291.383183\n      0\n      10000\n    \n    \n      3\n      Queens\n      99.517649\n      167.102155\n      10\n      10000\n    \n    \n      4\n      Staten Island\n      114.812332\n      277.620403\n      13\n      5000"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-50",
    "href": "Certification/Preprocessing.html#question-50",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 50",
    "text": "Question 50\nneighbourhood_group 값에 따른 reviews_per_month 평균, 분산, 최대, 최소 값을 구하여라\n\n\nCode\nAns = df.groupby('neighbourhood_group', as_index = False)['reviews_per_month'].agg(['mean', 'std', 'min', 'max'])\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood_group\n      mean\n      std\n      min\n      max\n    \n  \n  \n    \n      0\n      Bronx\n      1.837831\n      1.673284\n      0.02\n      10.34\n    \n    \n      1\n      Brooklyn\n      1.283212\n      1.516259\n      0.01\n      14.00\n    \n    \n      2\n      Manhattan\n      1.272131\n      1.628252\n      0.01\n      58.50\n    \n    \n      3\n      Queens\n      1.941200\n      2.213108\n      0.01\n      20.94\n    \n    \n      4\n      Staten Island\n      1.872580\n      1.685495\n      0.02\n      10.12"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-51",
    "href": "Certification/Preprocessing.html#question-51",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 51",
    "text": "Question 51\nneighbourhood 값과 neighbourhood_group 값에 따른 price의 평균을 구하라\n\n\nCode\nAns = df.groupby(['neighbourhood', 'neighbourhood_group'], as_index = False)['price'].agg(['mean'])\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood\n      neighbourhood_group\n      mean\n    \n  \n  \n    \n      0\n      Allerton\n      Bronx\n      87.595238\n    \n    \n      1\n      Arden Heights\n      Staten Island\n      67.250000\n    \n    \n      2\n      Arrochar\n      Staten Island\n      115.000000\n    \n    \n      3\n      Arverne\n      Queens\n      171.779221\n    \n    \n      4\n      Astoria\n      Queens\n      117.187778\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      216\n      Windsor Terrace\n      Brooklyn\n      138.993631\n    \n    \n      217\n      Woodhaven\n      Queens\n      67.170455\n    \n    \n      218\n      Woodlawn\n      Bronx\n      60.090909\n    \n    \n      219\n      Woodrow\n      Staten Island\n      700.000000\n    \n    \n      220\n      Woodside\n      Queens\n      85.097872\n    \n  \n\n221 rows × 3 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-52",
    "href": "Certification/Preprocessing.html#question-52",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 52",
    "text": "Question 52\nneighbourhood 값과 neighbourhood_group 값에 따른 price 의 평균을 계층적 indexing 없이 구하라\n\n\nCode\nAns = df.groupby(['neighbourhood', 'neighbourhood_group'])['price'].mean().unstack()\nAns\n\n\n\n\n\n\n  \n    \n      neighbourhood_group\n      Bronx\n      Brooklyn\n      Manhattan\n      Queens\n      Staten Island\n    \n    \n      neighbourhood\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Allerton\n      87.595238\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      Arden Heights\n      NaN\n      NaN\n      NaN\n      NaN\n      67.25\n    \n    \n      Arrochar\n      NaN\n      NaN\n      NaN\n      NaN\n      115.00\n    \n    \n      Arverne\n      NaN\n      NaN\n      NaN\n      171.779221\n      NaN\n    \n    \n      Astoria\n      NaN\n      NaN\n      NaN\n      117.187778\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Windsor Terrace\n      NaN\n      138.993631\n      NaN\n      NaN\n      NaN\n    \n    \n      Woodhaven\n      NaN\n      NaN\n      NaN\n      67.170455\n      NaN\n    \n    \n      Woodlawn\n      60.090909\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      Woodrow\n      NaN\n      NaN\n      NaN\n      NaN\n      700.00\n    \n    \n      Woodside\n      NaN\n      NaN\n      NaN\n      85.097872\n      NaN\n    \n  \n\n221 rows × 5 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-53",
    "href": "Certification/Preprocessing.html#question-53",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 53",
    "text": "Question 53\nneighbourhood 값과 neighbourhood_group 값에 따른 price 의 평균을 계층적 indexing 없이 구하고 nan 값은 -999값으로 채워라\n\n\nCode\nAns = df.groupby(['neighbourhood', 'neighbourhood_group'])['price'].mean().unstack().fillna(-999)\nAns\n\n\n\n\n\n\n  \n    \n      neighbourhood_group\n      Bronx\n      Brooklyn\n      Manhattan\n      Queens\n      Staten Island\n    \n    \n      neighbourhood\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Allerton\n      87.595238\n      -999.000000\n      -999.0\n      -999.000000\n      -999.00\n    \n    \n      Arden Heights\n      -999.000000\n      -999.000000\n      -999.0\n      -999.000000\n      67.25\n    \n    \n      Arrochar\n      -999.000000\n      -999.000000\n      -999.0\n      -999.000000\n      115.00\n    \n    \n      Arverne\n      -999.000000\n      -999.000000\n      -999.0\n      171.779221\n      -999.00\n    \n    \n      Astoria\n      -999.000000\n      -999.000000\n      -999.0\n      117.187778\n      -999.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Windsor Terrace\n      -999.000000\n      138.993631\n      -999.0\n      -999.000000\n      -999.00\n    \n    \n      Woodhaven\n      -999.000000\n      -999.000000\n      -999.0\n      67.170455\n      -999.00\n    \n    \n      Woodlawn\n      60.090909\n      -999.000000\n      -999.0\n      -999.000000\n      -999.00\n    \n    \n      Woodrow\n      -999.000000\n      -999.000000\n      -999.0\n      -999.000000\n      700.00\n    \n    \n      Woodside\n      -999.000000\n      -999.000000\n      -999.0\n      85.097872\n      -999.00\n    \n  \n\n221 rows × 5 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-54",
    "href": "Certification/Preprocessing.html#question-54",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 54",
    "text": "Question 54\n데이터중 neighbourhood_group 값이 Queens값을 가지는 데이터들 중 neighbourhood 그룹별로 price값의 평균, 분산, 최대, 최소값을 구하라\n\n\nCode\nAns = df[df['neighbourhood_group'] == 'Queens'].groupby('neighbourhood', as_index = False)['price'].agg(['mean', 'std', 'min', 'max'])\nAns\n\n\n\n\n\n\n  \n    \n      \n      neighbourhood\n      mean\n      std\n      min\n      max\n    \n  \n  \n    \n      0\n      Arverne\n      171.779221\n      193.347902\n      35\n      1500\n    \n    \n      1\n      Astoria\n      117.187778\n      349.898287\n      25\n      10000\n    \n    \n      2\n      Bay Terrace\n      142.000000\n      82.561492\n      32\n      258\n    \n    \n      3\n      Bayside\n      157.948718\n      407.561616\n      30\n      2600\n    \n    \n      4\n      Bayswater\n      87.470588\n      48.279289\n      45\n      230\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      46\n      St. Albans\n      100.828947\n      97.416137\n      25\n      600\n    \n    \n      47\n      Sunnyside\n      84.865014\n      52.227837\n      12\n      600\n    \n    \n      48\n      Whitestone\n      107.545455\n      116.756468\n      35\n      400\n    \n    \n      49\n      Woodhaven\n      67.170455\n      40.527335\n      10\n      250\n    \n    \n      50\n      Woodside\n      85.097872\n      70.724116\n      28\n      500\n    \n  \n\n51 rows × 5 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-55",
    "href": "Certification/Preprocessing.html#question-55",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 55",
    "text": "Question 55\n데이터중 neighbourhood_group 값에 따른 room_type 컬럼의 숫자를 구하고 neighbourhood_group 값을 기준으로 각 값의 비율을 구하여라\n\n\nCode\nAns = df.groupby(['neighbourhood_group', 'room_type']).size().unstack()\nAns.loc[:, :] = (Ans.values / Ans.sum(axis = 1).values.reshape(-1, 1))\nAns\n\n\n\n\n\n\n  \n    \n      room_type\n      Entire home/apt\n      Private room\n      Shared room\n    \n    \n      neighbourhood_group\n      \n      \n      \n    \n  \n  \n    \n      Bronx\n      0.347388\n      0.597617\n      0.054995\n    \n    \n      Brooklyn\n      0.475478\n      0.503979\n      0.020543\n    \n    \n      Manhattan\n      0.609344\n      0.368496\n      0.022160\n    \n    \n      Queens\n      0.369926\n      0.595129\n      0.034945\n    \n    \n      Staten Island\n      0.471850\n      0.504021\n      0.024129"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-56",
    "href": "Certification/Preprocessing.html#question-56",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 56",
    "text": "Question 56\n\nData\n\n카드이용데이터 : https://www.kaggle.com/sakshigoyal7/credit-card-customers\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/BankChurnersUp.csv’\n\n\n데이터를 로드하고 데이터 행과 열의 갯수를 출력하라\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/BankChurnersUp.csv', index_col = 0)\ndf.shape\n\n\n(10127, 18)"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-57",
    "href": "Certification/Preprocessing.html#question-57",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 57",
    "text": "Question 57\nIncome_Category의 카테고리를 map 함수를 이용하여 다음과 같이 변경하여 newIncome 컬럼에 매핑하라\n\n\n\nValue\nC\n\n\n\n\nUnknown\nN\n\n\nLess than $40K\nA\n\n\n$40K - $60K\nB\n\n\n$60K - $80K\nC\n\n\n$80K - $120K\nD\n\n\n$120K +\nE\n\n\n\n\n\nCode\ndict = {'Unknown' : 'N', 'Less than $40K' : 'A', '$40K - $60K' : 'B', '$60K - $80K' : 'C', '$80K - $120K' : 'D', '$120K +' : 'E'}\ndf['newIncome'] = df['Income_Category'].map(dict)\nAns = df['newIncome']\nAns\n\n\n0        C\n1        A\n2        D\n3        A\n4        C\n        ..\n10122    B\n10123    B\n10124    A\n10125    B\n10126    A\nName: newIncome, Length: 10127, dtype: object"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-58",
    "href": "Certification/Preprocessing.html#question-58",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 58",
    "text": "Question 58\nIncome_Category의 카테고리를 apply 함수를 이용하여 다음과 같이 변경하여 newIncome 컬럼에 매핑하라\n\n\n\nValue\nC\n\n\n\n\nUnknown\nN\n\n\nLess than $40K\nA\n\n\n$40K - $60K\nB\n\n\n$60K - $80K\nC\n\n\n$80K - $120K\nD\n\n\n$120K +\nE\n\n\n\n\n\nCode\ndef ChangeCategory(x) :\n    if x == 'Unknown' :\n        return 'N'\n    elif x == 'Less than $40K' :\n        return 'A'\n    elif x == '$40K - $60K' :\n        return 'B'\n    elif x == '$60K - $80K' :\n        return 'C'\n    elif x == '$80K - $120K' :\n        return 'D'\n    elif x == '$120K +' :\n        return 'E'\n\ndf['newIncome'] = df['Income_Category'].apply(ChangeCategory)\nAns = df['newIncome']\nAns\n\n\n0        C\n1        A\n2        D\n3        A\n4        C\n        ..\n10122    B\n10123    B\n10124    A\n10125    B\n10126    A\nName: newIncome, Length: 10127, dtype: object"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-59",
    "href": "Certification/Preprocessing.html#question-59",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 59",
    "text": "Question 59\nCustomer_Age의 값을 이용하여 나이 구간을 AgeState 컬럼으로 정의하라. (0~9 : 0 , 10~19 :10 , 20~29 :20 …) 각 구간의 빈도수를 출력하라\n\n\nCode\ndf['AgeState'] = df['Customer_Age'].map(lambda x : x // 10 * 10)\nAns = df['AgeState'].value_counts().sort_index()\nAns\n\n\nAgeState\n20     195\n30    1841\n40    4561\n50    2998\n60     530\n70       2\nName: count, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-60",
    "href": "Certification/Preprocessing.html#question-60",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 60",
    "text": "Question 60\nEducation_Level의 값중 Graduate단어가 포함되는 값은 1 그렇지 않은 경우에는 0으로 변경하여 newEduLevel 컬럼을 정의하고 빈도수를 출력하라\n\n\nCode\ndf['newEduLevel'] = df['Education_Level'].map(lambda x : 1 if 'Graduate' in x else 0)\nAns = df['newEduLevel'].value_counts()\nAns\n\n\nnewEduLevel\n0    6483\n1    3644\nName: count, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-61",
    "href": "Certification/Preprocessing.html#question-61",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 61",
    "text": "Question 61\nCredit_Limit 컬럼값이 4500 이상인 경우 1 그외의 경우에는 모두 0으로 하는 newLimit 정의하라. newLimit 각 값들의 빈도수를 출력하라\n\n\nCode\ndf['newLimit'] = df['Credit_Limit'].map(lambda x : 1 if x >= 4500 else 0)\nAns = df['newLimit'].value_counts().sort_index()\nAns\n\n\nnewLimit\n0    5031\n1    5096\nName: count, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-62",
    "href": "Certification/Preprocessing.html#question-62",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 62",
    "text": "Question 62\nMarital_Status 컬럼값이 Married 이고 Card_Category 컬럼의 값이 Platinum인 경우 1 그외의 경우에는 모두 0으로 하는 newState컬럼을 정의하라. newState의 각 값들의 빈도수를 출력하라\n\n\nCode\ndf['newState'] = df[['Marital_Status', 'Card_Category']].apply(lambda x : 1 if x[0] == 'Married' and x[1] == 'Platinum' else 0, axis = 1)\nAns = df['newState'].value_counts()\nAns\n\n\nnewState\n0    10120\n1        7\nName: count, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-63",
    "href": "Certification/Preprocessing.html#question-63",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 63",
    "text": "Question 63\nGender 컬럼값 M인 경우 Male, F인 경우 Female로 값을 변경하여 Gender 컬럼에 새롭게 정의하라. 각 value의 빈도를 출력하라\n\n\nCode\ndf['Gender'] = df['Gender'].apply(lambda x : 'Male' if x == 'M' else 'Female')\nAns = df['Gender'].value_counts()\nAns\n\n\nGender\nFemale    5358\nMale      4769\nName: count, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-64",
    "href": "Certification/Preprocessing.html#question-64",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 64",
    "text": "Question 64\n\nData\n\n주가 데이터 : https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/06_Stats/Wind_Stats/wind.data\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/timeTest.csv’\n\n\n데이터를 로드하고 각 열의 데이터 타입을 파악하라\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/timeTest.csv')\nAns = df.dtypes\nAns\n\n\nYr_Mo_Dy     object\nRPT         float64\nVAL         float64\nROS         float64\nKIL         float64\n             ...   \nCLA         float64\nMUL         float64\nCLO         float64\nBEL         float64\nMAL         float64\nLength: 13, dtype: object"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-65",
    "href": "Certification/Preprocessing.html#question-65",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 65",
    "text": "Question 65\nYr_Mo_Dy을 판다스에서 인식할 수 있는 datetime64타입으로 변경하라\n\n\nCode\ndf['Yr_Mo_Dy'] = pd.to_datetime(df['Yr_Mo_Dy'])\nAns = df['Yr_Mo_Dy']\nAns\n\n\n0      2061-01-01\n1      2061-01-02\n2      2061-01-03\n3      2061-01-04\n4      2061-01-05\n          ...    \n6569   1978-12-27\n6570   1978-12-28\n6571   1978-12-29\n6572   1978-12-30\n6573   1978-12-31\nName: Yr_Mo_Dy, Length: 6574, dtype: datetime64[ns]"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-66",
    "href": "Certification/Preprocessing.html#question-66",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 66",
    "text": "Question 66\nYr_Mo_Dy에 존재하는 년도의 유일값을 모두 출력하라\n\n\nCode\ndf['Yr_Mo_Dy'].dt.year.unique()\n\n\narray([2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 1971,\n       1972, 1973, 1974, 1975, 1976, 1977, 1978])"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-67",
    "href": "Certification/Preprocessing.html#question-67",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 67",
    "text": "Question 67\nYr_Mo_Dy에 년도가 2061년 이상의 경우에는 모두 잘못된 데이터이다. 해당경우의 값은 100을 빼서 새롭게 날짜를 Yr_Mo_Dy 컬럼에 정의하라\n\n\nCode\ndf.loc[df['Yr_Mo_Dy'].dt.year >= 2061, 'Yr_Mo_Dy'] -= pd.DateOffset(years = 100)\nAns = df['Yr_Mo_Dy']\nAns\n\n\n0      1961-01-01\n1      1961-01-02\n2      1961-01-03\n3      1961-01-04\n4      1961-01-05\n          ...    \n6569   1978-12-27\n6570   1978-12-28\n6571   1978-12-29\n6572   1978-12-30\n6573   1978-12-31\nName: Yr_Mo_Dy, Length: 6574, dtype: datetime64[ns]"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-68",
    "href": "Certification/Preprocessing.html#question-68",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 68",
    "text": "Question 68\n년도별 각컬럼의 평균값을 구하여라\n\n\nCode\nAns = df.groupby(df['Yr_Mo_Dy'].dt.year).mean().drop(columns = ['Yr_Mo_Dy'])\nAns\n\n\n\n\n\n\n  \n    \n      \n      RPT\n      VAL\n      ROS\n      KIL\n      SHA\n      ...\n      CLA\n      MUL\n      CLO\n      BEL\n      MAL\n    \n    \n      Yr_Mo_Dy\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1961\n      12.299583\n      10.351796\n      11.362369\n      6.958227\n      10.881763\n      ...\n      8.858788\n      8.647652\n      9.835577\n      13.502795\n      13.680773\n    \n    \n      1962\n      12.246923\n      10.110438\n      11.732712\n      6.960440\n      10.657918\n      ...\n      8.793753\n      8.316822\n      9.676247\n      12.930685\n      14.323956\n    \n    \n      1963\n      12.813452\n      10.836986\n      12.541151\n      7.330055\n      11.724110\n      ...\n      10.336548\n      8.903589\n      10.224438\n      13.638877\n      14.999014\n    \n    \n      1964\n      12.363661\n      10.920164\n      12.104372\n      6.787787\n      11.454481\n      ...\n      9.467350\n      7.789016\n      10.207951\n      13.740546\n      14.910301\n    \n    \n      1965\n      12.451370\n      11.075534\n      11.848767\n      6.858466\n      11.024795\n      ...\n      8.879918\n      7.907425\n      9.918082\n      12.964247\n      15.591644\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1974\n      13.643096\n      11.811781\n      12.336356\n      6.427041\n      11.110986\n      ...\n      9.896986\n      9.331753\n      8.736356\n      13.252959\n      16.947671\n    \n    \n      1975\n      12.008575\n      10.293836\n      11.564712\n      5.269096\n      9.190082\n      ...\n      7.843836\n      8.797945\n      7.382822\n      12.631671\n      15.307863\n    \n    \n      1976\n      11.737842\n      10.203115\n      10.761230\n      5.109426\n      8.846339\n      ...\n      7.146202\n      8.883716\n      7.883087\n      12.332377\n      15.471448\n    \n    \n      1977\n      13.099616\n      11.144493\n      12.627836\n      6.073945\n      10.003836\n      ...\n      8.378384\n      9.098192\n      8.821616\n      13.459068\n      16.590849\n    \n    \n      1978\n      12.504356\n      11.044274\n      11.380000\n      6.082356\n      10.167233\n      ...\n      8.800466\n      9.089753\n      8.301699\n      12.967397\n      16.771370\n    \n  \n\n18 rows × 12 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-69",
    "href": "Certification/Preprocessing.html#question-69",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 69",
    "text": "Question 69\nweekday컬럼을 만들고 요일별로 매핑하라 (월요일 : 0 ~ 일요일 : 6)\n\n\nCode\ndf['weekday'] = df['Yr_Mo_Dy'].dt.weekday\nAns = df['weekday']\nAns\n\n\n0       6\n1       0\n2       1\n3       2\n4       3\n       ..\n6569    2\n6570    3\n6571    4\n6572    5\n6573    6\nName: weekday, Length: 6574, dtype: int32"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-70",
    "href": "Certification/Preprocessing.html#question-70",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 70",
    "text": "Question 70\nweekday컬럼을 기준으로 주말이면 1 평일이면 0의 값을 가지는 WeekCheck 컬럼을 만들어라\n\n\nCode\ndf['WeekCheck'] = df['weekday'].map(lambda x : 1 if x in [5, 6] else 0)\nAns = df['WeekCheck']\nAns\n\n\n0       1\n1       0\n2       0\n3       0\n4       0\n       ..\n6569    0\n6570    0\n6571    0\n6572    1\n6573    1\nName: WeekCheck, Length: 6574, dtype: int64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-71",
    "href": "Certification/Preprocessing.html#question-71",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 71",
    "text": "Question 71\n년도, 일자 상관없이 모든 컬럼의 각 달의 평균을 구하여라\n\n\nCode\ndf.groupby(df['Yr_Mo_Dy'].dt.month).mean().drop(columns = ['Yr_Mo_Dy'])\n\n\n\n\n\n\n  \n    \n      \n      RPT\n      VAL\n      ROS\n      KIL\n      SHA\n      ...\n      CLA\n      MUL\n      CLO\n      BEL\n      MAL\n    \n    \n      Yr_Mo_Dy\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      14.847325\n      12.914560\n      13.299624\n      7.199498\n      11.667734\n      ...\n      9.512047\n      9.543208\n      10.053566\n      14.550520\n      18.028763\n    \n    \n      2\n      13.710906\n      12.111122\n      12.879132\n      6.942411\n      11.551772\n      ...\n      9.341437\n      9.313169\n      9.518051\n      13.728898\n      17.156142\n    \n    \n      3\n      13.158687\n      11.505842\n      12.648118\n      7.265907\n      11.554516\n      ...\n      9.635896\n      9.700324\n      10.096953\n      13.810609\n      16.909317\n    \n    \n      4\n      12.555648\n      10.429759\n      12.204815\n      6.898037\n      10.677667\n      ...\n      8.909056\n      8.930870\n      9.158019\n      12.664759\n      14.937611\n    \n    \n      5\n      11.724032\n      10.145619\n      11.550394\n      6.307487\n      10.224301\n      ...\n      8.452903\n      8.040806\n      8.524857\n      12.767258\n      13.736039\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8\n      10.213411\n      8.415143\n      9.993441\n      5.270681\n      8.901559\n      ...\n      6.842025\n      7.240573\n      7.002783\n      11.110090\n      12.565943\n    \n    \n      9\n      11.458519\n      9.981002\n      10.756883\n      5.615176\n      9.766315\n      ...\n      7.745677\n      7.610556\n      7.689278\n      12.686389\n      14.761963\n    \n    \n      10\n      12.660610\n      11.010681\n      11.453943\n      6.065215\n      10.550251\n      ...\n      8.726308\n      8.347181\n      8.850376\n      14.155323\n      16.697151\n    \n    \n      11\n      13.200722\n      11.639500\n      12.293407\n      6.247611\n      10.501130\n      ...\n      8.427167\n      8.604000\n      8.943167\n      13.815741\n      18.114185\n    \n    \n      12\n      14.446398\n      12.353602\n      13.212276\n      6.829910\n      11.301254\n      ...\n      9.209355\n      9.447258\n      9.627670\n      14.259516\n      18.697599\n    \n  \n\n12 rows × 12 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-72",
    "href": "Certification/Preprocessing.html#question-72",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 72",
    "text": "Question 72\n모든 결측치는 컬럼기준 직전의 값으로 대체하고 첫번째 행에 결측치가 있을경우 뒤에있는 값으로 대채하라\n\n\nCode\nAns = df.interpolate(method = 'ffill').interpolate(method = 'bfill')\nAns\n\n\n\n\n\n\n  \n    \n      \n      Yr_Mo_Dy\n      RPT\n      VAL\n      ROS\n      KIL\n      ...\n      CLA\n      MUL\n      CLO\n      BEL\n      MAL\n    \n  \n  \n    \n      0\n      1961-01-01\n      15.04\n      14.96\n      13.17\n      9.29\n      ...\n      10.25\n      10.83\n      12.58\n      18.50\n      15.04\n    \n    \n      1\n      1961-01-02\n      14.71\n      14.96\n      10.83\n      6.50\n      ...\n      10.04\n      9.79\n      9.67\n      17.54\n      13.83\n    \n    \n      2\n      1961-01-03\n      18.50\n      16.88\n      12.33\n      10.13\n      ...\n      10.04\n      8.50\n      7.67\n      12.75\n      12.71\n    \n    \n      3\n      1961-01-04\n      10.58\n      6.63\n      11.75\n      4.58\n      ...\n      1.79\n      5.83\n      5.88\n      5.46\n      10.88\n    \n    \n      4\n      1961-01-05\n      13.33\n      13.25\n      11.42\n      6.17\n      ...\n      6.54\n      10.92\n      10.34\n      12.92\n      11.83\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6569\n      1978-12-27\n      17.58\n      16.96\n      17.62\n      8.08\n      ...\n      15.59\n      14.04\n      14.00\n      17.21\n      40.08\n    \n    \n      6570\n      1978-12-28\n      13.21\n      5.46\n      13.46\n      5.00\n      ...\n      16.25\n      15.25\n      18.05\n      21.79\n      41.46\n    \n    \n      6571\n      1978-12-29\n      14.00\n      10.29\n      14.42\n      8.71\n      ...\n      12.46\n      14.50\n      16.42\n      18.88\n      29.58\n    \n    \n      6572\n      1978-12-30\n      18.50\n      14.04\n      21.29\n      9.13\n      ...\n      12.87\n      12.46\n      12.12\n      14.67\n      28.79\n    \n    \n      6573\n      1978-12-31\n      20.33\n      17.41\n      27.29\n      9.59\n      ...\n      11.63\n      11.58\n      11.38\n      12.08\n      22.08\n    \n  \n\n6574 rows × 13 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-73",
    "href": "Certification/Preprocessing.html#question-73",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 73",
    "text": "Question 73\n년도 - 월을 기준으로 모든 컬럼의 평균값을 구하여라\n\n\nCode\nAns = df.groupby(df['Yr_Mo_Dy'].dt.to_period('M')).mean().drop(columns = 'Yr_Mo_Dy')\nAns\n\n\n\n\n\n\n  \n    \n      \n      RPT\n      VAL\n      ROS\n      KIL\n      SHA\n      ...\n      CLA\n      MUL\n      CLO\n      BEL\n      MAL\n    \n    \n      Yr_Mo_Dy\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1961-01\n      14.841333\n      11.988333\n      13.431613\n      7.736774\n      11.072759\n      ...\n      9.245333\n      9.085806\n      10.107419\n      13.880968\n      14.703226\n    \n    \n      1961-02\n      16.269286\n      14.975357\n      14.441481\n      9.230741\n      13.852143\n      ...\n      11.846071\n      11.821429\n      12.714286\n      18.583214\n      15.411786\n    \n    \n      1961-03\n      10.890000\n      11.296452\n      10.752903\n      7.284000\n      10.509355\n      ...\n      9.829677\n      10.294138\n      11.251935\n      16.410968\n      15.720000\n    \n    \n      1961-04\n      10.722667\n      9.427667\n      9.998000\n      5.830667\n      8.435000\n      ...\n      7.094667\n      7.342333\n      7.237000\n      11.147333\n      10.278333\n    \n    \n      1961-05\n      9.860968\n      8.850000\n      10.818065\n      5.905333\n      9.490323\n      ...\n      8.177097\n      8.039355\n      8.499355\n      11.900323\n      12.011613\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1978-08\n      9.645161\n      8.259355\n      9.032258\n      4.502903\n      7.368065\n      ...\n      5.417742\n      7.241290\n      5.536774\n      10.466774\n      12.054194\n    \n    \n      1978-09\n      10.913667\n      10.895000\n      10.635000\n      5.725000\n      10.372000\n      ...\n      9.583000\n      10.069333\n      8.939000\n      15.680333\n      19.391333\n    \n    \n      1978-10\n      9.897742\n      8.670968\n      9.295806\n      4.721290\n      8.525161\n      ...\n      7.337742\n      8.297742\n      8.243871\n      13.776774\n      17.150000\n    \n    \n      1978-11\n      16.151667\n      14.802667\n      13.508000\n      7.317333\n      11.475000\n      ...\n      9.657333\n      10.701333\n      10.676000\n      17.404667\n      20.723000\n    \n    \n      1978-12\n      16.175484\n      13.748065\n      15.635161\n      7.094839\n      11.398710\n      ...\n      10.194839\n      10.616774\n      11.028710\n      13.859677\n      21.371613\n    \n  \n\n216 rows × 12 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-74",
    "href": "Certification/Preprocessing.html#question-74",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 74",
    "text": "Question 74\nRPT 컬럼의 값을 일자별 기준으로 1차차분하라\n\n\nCode\nAns = df['RPT'].diff()\nAns\n\n\n0        NaN\n1      -0.33\n2       3.79\n3      -7.92\n4       2.75\n        ... \n6569    3.75\n6570   -4.37\n6571    0.79\n6572    4.50\n6573    1.83\nName: RPT, Length: 6574, dtype: float64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-75",
    "href": "Certification/Preprocessing.html#question-75",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 75",
    "text": "Question 75\nRPT와 VAL의 컬럼을 일주일 간격으로 각각 이동평균한값을 구하여라\n\n\nCode\nAns = df[['RPT', 'VAL']].rolling(window = 7).mean()\nAns\n\n\n\n\n\n\n  \n    \n      \n      RPT\n      VAL\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      6569\n      11.868571\n      9.362857\n    \n    \n      6570\n      11.904286\n      9.595714\n    \n    \n      6571\n      13.017143\n      10.011429\n    \n    \n      6572\n      13.285714\n      10.118571\n    \n    \n      6573\n      14.951429\n      11.801429\n    \n  \n\n6574 rows × 2 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-76",
    "href": "Certification/Preprocessing.html#question-76",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 76",
    "text": "Question 76\n\nData\n\n서울시 미세먼지 데이터 : https://www.airkorea.or.kr/web/realSearch?pMENU_NO=97\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/seoul_pm.csv’\n\n\n년-월-일:시 컬럼을 pandas에서 인식할 수 있는 datetime 형태로 변경하라. 서울시의 제공데이터의 경우 0시가 24시로 표현된다\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/seoul_pm.csv')\n\ndef Change_Date(x) :\n    import datetime\n    date = x.split(':')[0]\n    hour = x.split(':')[1]\n    \n    if hour == '24' :\n        hour = '00:00:00'\n        dt = pd.to_datetime(date + ' ' + hour) + datetime.timedelta(days = 1)\n    \n    else : \n        hour = hour + ':00:00'\n        dt = pd.to_datetime(date + ' ' + hour)\n    \n    return dt\n\ndf['(년-월-일:시)'] = df['(년-월-일:시)'].apply(Change_Date)\n\nAns = df['(년-월-일:시)']\nAns\n\n\n0      2021-05-15 15:00:00\n1      2021-05-15 14:00:00\n2      2021-05-15 13:00:00\n3      2021-05-15 12:00:00\n4      2021-05-15 11:00:00\n               ...        \n1234   2021-03-25 05:00:00\n1235   2021-03-25 04:00:00\n1236   2021-03-25 03:00:00\n1237   2021-03-25 02:00:00\n1238   2021-03-25 01:00:00\nName: (년-월-일:시), Length: 1239, dtype: datetime64[ns]"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-77",
    "href": "Certification/Preprocessing.html#question-77",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 77",
    "text": "Question 77\n일자별 영어요일 이름을 dayName 컬럼에 저장하라\n\n\nCode\ndf['dayName'] = df['(년-월-일:시)'].dt.day_name()\nAns = df['dayName']\nAns\n\n\n0       Saturday\n1       Saturday\n2       Saturday\n3       Saturday\n4       Saturday\n          ...   \n1234    Thursday\n1235    Thursday\n1236    Thursday\n1237    Thursday\n1238    Thursday\nName: dayName, Length: 1239, dtype: object"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-78",
    "href": "Certification/Preprocessing.html#question-78",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 78",
    "text": "Question 78\n일자별 각 PM10등급의 빈도수를 파악하라\n\n\nCode\ngroup = df.groupby(['dayName', 'PM10등급'], as_index = False).size()\nAns = group.pivot(index = 'dayName', columns = 'PM10등급', values = 'size').fillna(0)\nAns\n\n\n\n\n\n\n  \n    \n      PM10등급\n      나쁨\n      매우나쁨\n      보통\n      좋음\n    \n    \n      dayName\n      \n      \n      \n      \n    \n  \n  \n    \n      Friday\n      31.0\n      17.0\n      120.0\n      21.0\n    \n    \n      Monday\n      1.0\n      21.0\n      83.0\n      63.0\n    \n    \n      Saturday\n      31.0\n      27.0\n      71.0\n      54.0\n    \n    \n      Sunday\n      2.0\n      1.0\n      67.0\n      98.0\n    \n    \n      Thursday\n      41.0\n      0.0\n      144.0\n      5.0\n    \n    \n      Tuesday\n      13.0\n      10.0\n      71.0\n      74.0\n    \n    \n      Wednesday\n      26.0\n      0.0\n      95.0\n      46.0"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-79",
    "href": "Certification/Preprocessing.html#question-79",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 79",
    "text": "Question 79\n시간이 연속적으로 존재하며 결측치가 없는지 확인하라\n\n\nCode\n# 시간을 차분했을 경우 첫 값은 nan, 이후 모든 차분값이 동일하면 연속이라 판단한다.\n\ncheck = len(df['(년-월-일:시)'].diff().unique())\nif check == 2:\n    Ans = True\nelse:\n    Ans = False\nAns\n\n\nTrue"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-80",
    "href": "Certification/Preprocessing.html#question-80",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 80",
    "text": "Question 80\n오전 10시와 오후 10시(22시)의 PM10의 평균값을 각각 구하여라\n\n\nCode\nAns = df.groupby(df['(년-월-일:시)'].dt.hour, as_index = False)['PM10'].mean().loc[[10, 22]]\nAns\n\n\n\n\n\n\n  \n    \n      \n      PM10\n    \n  \n  \n    \n      10\n      70.384615\n    \n    \n      22\n      69.941176"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-81",
    "href": "Certification/Preprocessing.html#question-81",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 81",
    "text": "Question 81\n날짜 컬럼을 index로 만들어라\n\n\nCode\ndf.set_index('(년-월-일:시)', inplace = True, drop = True)\nAns = df\nAns\n\n\n\n\n\n\n  \n    \n      \n      PM10등급\n      PM10\n      PM2.5등급\n      PM2.5\n      오존등급\n      오존\n      이산화질소등급\n      이산화질소\n      일산화탄소등급\n      일산화탄소\n      아황산가스등급\n      아황산가스\n    \n    \n      (년-월-일:시)\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-05-15 15:00:00\n      보통\n      47.0\n      보통\n      19.0\n      좋음\n      0.017\n      좋음\n      0.023\n      좋음\n      0.4\n      좋음\n      0.003\n    \n    \n      2021-05-15 14:00:00\n      보통\n      43.0\n      보통\n      20.0\n      좋음\n      0.024\n      좋음\n      0.019\n      좋음\n      0.3\n      좋음\n      0.003\n    \n    \n      2021-05-15 13:00:00\n      보통\n      34.0\n      보통\n      24.0\n      보통\n      0.035\n      좋음\n      0.017\n      좋음\n      0.4\n      좋음\n      0.004\n    \n    \n      2021-05-15 12:00:00\n      보통\n      41.0\n      보통\n      27.0\n      보통\n      0.037\n      좋음\n      0.020\n      좋음\n      0.4\n      좋음\n      0.004\n    \n    \n      2021-05-15 11:00:00\n      보통\n      51.0\n      보통\n      34.0\n      보통\n      0.033\n      좋음\n      0.023\n      좋음\n      0.4\n      좋음\n      0.005\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-03-25 05:00:00\n      보통\n      39.0\n      보통\n      18.0\n      좋음\n      0.026\n      좋음\n      0.025\n      좋음\n      0.4\n      좋음\n      0.003\n    \n    \n      2021-03-25 04:00:00\n      보통\n      34.0\n      좋음\n      15.0\n      좋음\n      0.017\n      보통\n      0.033\n      좋음\n      0.4\n      좋음\n      0.002\n    \n    \n      2021-03-25 03:00:00\n      보통\n      35.0\n      좋음\n      13.0\n      좋음\n      0.029\n      좋음\n      0.025\n      좋음\n      0.4\n      좋음\n      0.003\n    \n    \n      2021-03-25 02:00:00\n      보통\n      35.0\n      좋음\n      13.0\n      보통\n      0.031\n      좋음\n      0.025\n      좋음\n      0.3\n      좋음\n      0.003\n    \n    \n      2021-03-25 01:00:00\n      보통\n      42.0\n      좋음\n      13.0\n      좋음\n      0.022\n      보통\n      0.037\n      좋음\n      0.4\n      좋음\n      0.003\n    \n  \n\n1239 rows × 12 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-82",
    "href": "Certification/Preprocessing.html#question-82",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 82",
    "text": "Question 82\n데이터를 주단위로 뽑아서 최소,최대 평균, 표준표차를 구하여라\n\n\nCode\ndf.select_dtypes(exclude = ['object']).resample('W').agg(['min', 'max', 'mean', 'std'])\n\n\n\n\n\n\n  \n    \n      \n      PM10\n      PM2.5\n      오존\n      ...\n      이산화질소\n      일산화탄소\n      아황산가스\n    \n    \n      \n      min\n      max\n      mean\n      std\n      min\n      max\n      mean\n      std\n      min\n      max\n      ...\n      mean\n      std\n      min\n      max\n      mean\n      std\n      min\n      max\n      mean\n      std\n    \n    \n      (년-월-일:시)\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-03-28\n      6.0\n      160.0\n      72.747368\n      43.345462\n      4.0\n      113.0\n      44.705263\n      29.551928\n      0.002\n      0.085\n      ...\n      0.044579\n      0.023722\n      0.3\n      1.4\n      0.611579\n      0.232408\n      0.002\n      0.006\n      0.003274\n      0.000961\n    \n    \n      2021-04-04\n      3.0\n      598.0\n      97.148810\n      129.911976\n      1.0\n      120.0\n      23.168675\n      22.399578\n      0.003\n      0.055\n      ...\n      0.027929\n      0.014978\n      0.3\n      0.9\n      0.445833\n      0.135741\n      0.002\n      0.004\n      0.002732\n      0.000541\n    \n    \n      2021-04-11\n      17.0\n      102.0\n      41.059524\n      16.325911\n      7.0\n      70.0\n      21.761905\n      11.479343\n      0.009\n      0.070\n      ...\n      0.022583\n      0.009562\n      0.3\n      0.7\n      0.389286\n      0.087573\n      0.002\n      0.004\n      0.002744\n      0.000569\n    \n    \n      2021-04-18\n      3.0\n      367.0\n      48.180723\n      43.254468\n      2.0\n      38.0\n      17.066265\n      7.867952\n      0.002\n      0.070\n      ...\n      0.023753\n      0.013553\n      0.3\n      0.6\n      0.386747\n      0.084954\n      0.002\n      0.004\n      0.002464\n      0.000579\n    \n    \n      2021-04-25\n      17.0\n      126.0\n      55.119048\n      26.659936\n      7.0\n      61.0\n      26.392857\n      13.094788\n      0.006\n      0.090\n      ...\n      0.028571\n      0.014640\n      0.3\n      0.8\n      0.457143\n      0.122142\n      0.001\n      0.011\n      0.003631\n      0.001763\n    \n    \n      2021-05-02\n      3.0\n      97.0\n      40.612121\n      24.813103\n      1.0\n      43.0\n      16.644578\n      8.850965\n      0.003\n      0.064\n      ...\n      0.020428\n      0.011676\n      0.3\n      0.6\n      0.392771\n      0.092485\n      0.001\n      0.006\n      0.002524\n      0.000768\n    \n    \n      2021-05-09\n      8.0\n      1024.0\n      161.660714\n      239.679148\n      3.0\n      172.0\n      34.738095\n      39.788248\n      0.002\n      0.073\n      ...\n      0.024187\n      0.012371\n      0.3\n      1.0\n      0.419277\n      0.103230\n      0.002\n      0.004\n      0.002771\n      0.000579\n    \n    \n      2021-05-16\n      16.0\n      111.0\n      40.014815\n      21.876855\n      7.0\n      76.0\n      21.577778\n      15.622633\n      0.004\n      0.123\n      ...\n      0.030793\n      0.009503\n      0.3\n      0.8\n      0.440741\n      0.094075\n      0.001\n      0.006\n      0.002459\n      0.001696\n    \n  \n\n8 rows × 24 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-83",
    "href": "Certification/Preprocessing.html#question-83",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 83",
    "text": "Question 83\n\nData\n\n국가별 5세이하 사망비율 통계 : https://www.kaggle.com/utkarshxy/who-worldhealth-statistics-2020-complete\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/under5MortalityRate.csv’\n\n\nIndicator을 삭제하고 First Tooltip 컬럼에서 신뢰구간에 해당하는 표현을 지워라\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/under5MortalityRate.csv')\ndf.drop(columns = 'Indicator', inplace = True)\n\ndf['First Tooltip'] = df['First Tooltip'].apply(lambda x : x.split('[')[0])\ndf['First Tooltip'] = pd.to_numeric(df['First Tooltip'])\nAns = df['First Tooltip']\nAns\n\n\n0         60.27\n1         63.83\n2         56.57\n3         62.54\n4         66.08\n          ...  \n29994     96.97\n29995    102.60\n29996     91.04\n29997    102.50\n29998    108.20\nName: First Tooltip, Length: 29999, dtype: float64"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-84",
    "href": "Certification/Preprocessing.html#question-84",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 84",
    "text": "Question 84\n년도가 2015년 이상, Dim1이 Both sexes인 케이스만 추출하라\n\n\nCode\ndf_new = df[(df['Period'] >= 2015) & (df['Dim1'] == 'Both sexes')]\nAns = df_new\nAns\n\n\n\n\n\n\n  \n    \n      \n      Location\n      Period\n      Dim1\n      First Tooltip\n    \n  \n  \n    \n      0\n      Afghanistan\n      2019\n      Both sexes\n      60.27\n    \n    \n      3\n      Afghanistan\n      2018\n      Both sexes\n      62.54\n    \n    \n      6\n      Afghanistan\n      2017\n      Both sexes\n      64.94\n    \n    \n      9\n      Afghanistan\n      2016\n      Both sexes\n      67.57\n    \n    \n      12\n      Afghanistan\n      2015\n      Both sexes\n      70.44\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      29943\n      Timor-Leste\n      2019\n      Both sexes\n      44.22\n    \n    \n      29946\n      Timor-Leste\n      2018\n      Both sexes\n      45.62\n    \n    \n      29949\n      Timor-Leste\n      2017\n      Both sexes\n      47.27\n    \n    \n      29952\n      Timor-Leste\n      2016\n      Both sexes\n      49.01\n    \n    \n      29955\n      Timor-Leste\n      2015\n      Both sexes\n      50.76\n    \n  \n\n860 rows × 4 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-85",
    "href": "Certification/Preprocessing.html#question-85",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 85",
    "text": "Question 85\n84번 문제에서 추출한 데이터로 아래와 같이 나라에 따른 년도별 사망률을 데이터 프레임화 하라\n\n\nCode\nAns = df_new.pivot(index = 'Location', columns = 'Period', values = 'First Tooltip')\nAns\n\n\n\n\n\n\n  \n    \n      Period\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      70.44\n      67.57\n      64.94\n      62.54\n      60.27\n    \n    \n      Albania\n      9.57\n      9.42\n      9.42\n      9.53\n      9.68\n    \n    \n      Algeria\n      25.18\n      24.79\n      24.32\n      23.81\n      23.26\n    \n    \n      Andorra\n      3.53\n      3.37\n      3.22\n      3.09\n      2.97\n    \n    \n      Angola\n      88.20\n      84.21\n      80.62\n      77.67\n      74.69\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Syrian Arab Republic\n      23.18\n      23.27\n      22.97\n      22.11\n      21.53\n    \n    \n      Tajikistan\n      37.75\n      36.82\n      35.81\n      34.80\n      33.78\n    \n    \n      Thailand\n      10.80\n      10.32\n      9.86\n      9.42\n      9.01\n    \n    \n      The former Yugoslav Republic of Macedonia\n      12.97\n      11.97\n      9.94\n      7.83\n      6.12\n    \n    \n      Timor-Leste\n      50.76\n      49.01\n      47.27\n      45.62\n      44.22\n    \n  \n\n172 rows × 5 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-86",
    "href": "Certification/Preprocessing.html#question-86",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 86",
    "text": "Question 86\nDim1에 따른 년도별 사망비율의 평균을 구하라\n\n\nCode\nAns = df.pivot_table(index = 'Dim1', columns = 'Period', values = 'First Tooltip', aggfunc = 'mean')\nAns\n\n\n\n\n\n\n  \n    \n      Period\n      1950\n      1951\n      1952\n      1953\n      1954\n      1955\n      1956\n      1957\n      1958\n      1959\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Dim1\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Both sexes\n      147.700816\n      155.537544\n      157.811094\n      156.147206\n      154.539857\n      155.797179\n      159.241163\n      156.170114\n      150.813222\n      150.574000\n      ...\n      37.718488\n      35.573663\n      34.290988\n      33.099360\n      32.053314\n      31.012093\n      29.956337\n      29.030465\n      28.083837\n      27.191744\n    \n    \n      Female\n      140.909796\n      149.210175\n      151.516094\n      150.250882\n      148.688286\n      149.843205\n      153.048721\n      149.988295\n      144.719667\n      144.451474\n      ...\n      34.953023\n      32.877616\n      31.654070\n      30.521337\n      29.524302\n      28.544360\n      27.542035\n      26.675291\n      25.782616\n      24.945349\n    \n    \n      Male\n      154.151224\n      161.538246\n      163.760781\n      161.742059\n      160.081000\n      161.456923\n      165.089535\n      162.015000\n      156.573556\n      156.375053\n      ...\n      40.340174\n      38.140291\n      36.793081\n      35.543663\n      34.446105\n      33.354302\n      32.242616\n      31.273198\n      30.283023\n      29.350349\n    \n  \n\n3 rows × 70 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-87",
    "href": "Certification/Preprocessing.html#question-87",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 87",
    "text": "Question 87\n\nData\n\n올림픽 메달리스트 정보 데이터: https://www.kaggle.com/the-guardian/olympic-games\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/winter.csv’\n\n\n데이터에서 한국 KOR 데이터만 추출하라\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/winter.csv')\n\nkor = df[df['Country'] == 'KOR']\nAns = kor\nAns\n\n\n\n\n\n\n  \n    \n      \n      Year\n      City\n      Sport\n      Discipline\n      Athlete\n      Country\n      Gender\n      Event\n      Medal\n    \n  \n  \n    \n      2652\n      1992\n      Albertville\n      Skating\n      Short Track Speed Skating\n      LEE, Jun-Ho\n      KOR\n      Men\n      1000M\n      Bronze\n    \n    \n      2653\n      1992\n      Albertville\n      Skating\n      Short Track Speed Skating\n      KIM, Ki-Hoon\n      KOR\n      Men\n      1000M\n      Gold\n    \n    \n      2671\n      1992\n      Albertville\n      Skating\n      Short Track Speed Skating\n      KIM, Ki-Hoon\n      KOR\n      Men\n      5000M Relay\n      Gold\n    \n    \n      2672\n      1992\n      Albertville\n      Skating\n      Short Track Speed Skating\n      LEE, Jun-Ho\n      KOR\n      Men\n      5000M Relay\n      Gold\n    \n    \n      2673\n      1992\n      Albertville\n      Skating\n      Short Track Speed Skating\n      MO, Ji-Soo\n      KOR\n      Men\n      5000M Relay\n      Gold\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5528\n      2014\n      Sochi\n      Skating\n      Short Track Speed Skating\n      PARK, Seung-Hi\n      KOR\n      Women\n      500M\n      Bronze\n    \n    \n      5544\n      2014\n      Sochi\n      Skating\n      Speed skating\n      JOO, Hyong Jun\n      KOR\n      Men\n      Team Pursuit\n      Silver\n    \n    \n      5545\n      2014\n      Sochi\n      Skating\n      Speed skating\n      KIM, Cheol Min\n      KOR\n      Men\n      Team Pursuit\n      Silver\n    \n    \n      5546\n      2014\n      Sochi\n      Skating\n      Speed skating\n      LEE, Seung Hoon\n      KOR\n      Men\n      Team Pursuit\n      Silver\n    \n    \n      5565\n      2014\n      Sochi\n      Skating\n      Speed skating\n      LEE, Sang Hwa\n      KOR\n      Women\n      500M\n      Gold\n    \n  \n\n87 rows × 9 columns"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-88",
    "href": "Certification/Preprocessing.html#question-88",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 88",
    "text": "Question 88\n한국 올림픽 메달리스트 데이터에서 년도에 따른 Medal 수를 데이터프레임화 하라\n\n\nCode\nAns = kor.pivot_table(index = 'Year', columns = 'Medal', aggfunc = 'size').fillna(0)\nAns\n\n\n\n\n\n\n  \n    \n      Medal\n      Bronze\n      Gold\n      Silver\n    \n    \n      Year\n      \n      \n      \n    \n  \n  \n    \n      1992\n      1.0\n      5.0\n      1.0\n    \n    \n      1994\n      1.0\n      8.0\n      1.0\n    \n    \n      1998\n      2.0\n      6.0\n      4.0\n    \n    \n      2002\n      0.0\n      5.0\n      2.0\n    \n    \n      2006\n      2.0\n      14.0\n      3.0\n    \n    \n      2010\n      2.0\n      6.0\n      10.0\n    \n    \n      2014\n      2.0\n      7.0\n      5.0"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-89",
    "href": "Certification/Preprocessing.html#question-89",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 89",
    "text": "Question 89\n전체 데이터에서 Sport종류에 따른 성별 수를 구하여라\n\n\nCode\nAns = df.pivot_table(index = 'Sport', columns = 'Gender', aggfunc = 'size')\nAns\n\n\n\n\n\n\n  \n    \n      Gender\n      Men\n      Women\n    \n    \n      Sport\n      \n      \n    \n  \n  \n    \n      Biathlon\n      270\n      150\n    \n    \n      Bobsleigh\n      416\n      36\n    \n    \n      Curling\n      97\n      75\n    \n    \n      Ice Hockey\n      1231\n      305\n    \n    \n      Luge\n      135\n      45\n    \n    \n      Skating\n      665\n      564\n    \n    \n      Skiing\n      1130\n      651"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-90",
    "href": "Certification/Preprocessing.html#question-90",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 90",
    "text": "Question 90\n전체 데이터에서 Discipline종류에 따른 따른 Medal 수를 구하여라\n\n\nCode\nAns = df.pivot_table(index = 'Discipline', columns = 'Medal', aggfunc = 'size')\nAns\n\n\n\n\n\n\n  \n    \n      Medal\n      Bronze\n      Gold\n      Silver\n    \n    \n      Discipline\n      \n      \n      \n    \n  \n  \n    \n      Alpine Skiing\n      141\n      143\n      144\n    \n    \n      Biathlon\n      139\n      140\n      141\n    \n    \n      Bobsleigh\n      147\n      134\n      141\n    \n    \n      Cross Country Skiing\n      263\n      264\n      262\n    \n    \n      Curling\n      56\n      58\n      58\n    \n    \n      Figure skating\n      118\n      122\n      119\n    \n    \n      Freestyle Skiing\n      34\n      34\n      34\n    \n    \n      Ice Hockey\n      512\n      510\n      514\n    \n    \n      Luge\n      60\n      62\n      58\n    \n    \n      Nordic Combined\n      55\n      55\n      55\n    \n    \n      Short Track Speed Skating\n      96\n      97\n      97\n    \n    \n      Skeleton\n      10\n      10\n      10\n    \n    \n      Ski Jumping\n      68\n      69\n      70\n    \n    \n      Snowboard\n      30\n      30\n      30\n    \n    \n      Speed skating\n      190\n      193\n      197"
  },
  {
    "objectID": "Certification/Preprocessing.html#merge-concat",
    "href": "Certification/Preprocessing.html#merge-concat",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "7. Merge, Concat",
    "text": "7. Merge, Concat\n\nData\n\n국가별 5세이하 사망비율 통계 : https://www.kaggle.com/utkarshxy/who-worldhealth-statistics-2020-complete\nDataURL = ‘https://raw.githubusercontent.com/Datamanim/pandas/main/mergeTEst.csv’\n\n\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/pandas/main/mergeTEst.csv', index_col = 0)\n\ndf1 = df.iloc[:4, :]\ndf2 = df.iloc[4:, :]\n\ndisplay(df1)\ndisplay(df2)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      64.023\n      61.640\n      59.367\n      57.170\n      55.08\n      53.107\n      51.267\n      49.560\n      47.983\n      46.453\n    \n    \n      Albania\n      11.803\n      10.807\n      9.943\n      9.267\n      8.79\n      8.493\n      8.363\n      8.363\n      8.453\n      8.597\n    \n    \n      Algeria\n      23.540\n      22.907\n      22.450\n      22.117\n      21.85\n      21.587\n      21.257\n      20.850\n      20.407\n      19.930\n    \n    \n      Andorra\n      4.240\n      4.033\n      3.843\n      3.667\n      3.49\n      3.330\n      3.187\n      3.060\n      2.933\n      2.827\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Angola\n      75.713\n      71.280\n      67.233\n      63.570\n      60.430\n      57.757\n      55.510\n      53.460\n      51.757\n      50.093\n    \n    \n      Antigua and Barbuda\n      8.667\n      8.223\n      7.807\n      7.420\n      7.070\n      6.757\n      6.483\n      6.230\n      6.000\n      5.783\n    \n    \n      Argentina\n      12.887\n      12.380\n      11.840\n      11.283\n      10.733\n      10.203\n      9.683\n      9.177\n      8.680\n      8.227\n    \n    \n      Armenia\n      16.497\n      15.677\n      14.897\n      14.170\n      13.477\n      12.817\n      12.183\n      11.583\n      11.007\n      10.497\n    \n    \n      Australia\n      3.993\n      3.803\n      3.623\n      3.467\n      3.343\n      3.253\n      3.183\n      3.137\n      3.090\n      3.047\n    \n    \n      Austria\n      3.573\n      3.463\n      3.333\n      3.210\n      3.113\n      3.043\n      2.987\n      2.943\n      2.897\n      2.843"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-91",
    "href": "Certification/Preprocessing.html#question-91",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 91",
    "text": "Question 91\ndf1과 df2 데이터를 하나의 데이터 프레임으로 합쳐라\n\n\nCode\nAns = pd.concat([df1, df2], axis = 0)\nAns\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      64.023\n      61.640\n      59.367\n      57.170\n      55.080\n      53.107\n      51.267\n      49.560\n      47.983\n      46.453\n    \n    \n      Albania\n      11.803\n      10.807\n      9.943\n      9.267\n      8.790\n      8.493\n      8.363\n      8.363\n      8.453\n      8.597\n    \n    \n      Algeria\n      23.540\n      22.907\n      22.450\n      22.117\n      21.850\n      21.587\n      21.257\n      20.850\n      20.407\n      19.930\n    \n    \n      Andorra\n      4.240\n      4.033\n      3.843\n      3.667\n      3.490\n      3.330\n      3.187\n      3.060\n      2.933\n      2.827\n    \n    \n      Angola\n      75.713\n      71.280\n      67.233\n      63.570\n      60.430\n      57.757\n      55.510\n      53.460\n      51.757\n      50.093\n    \n    \n      Antigua and Barbuda\n      8.667\n      8.223\n      7.807\n      7.420\n      7.070\n      6.757\n      6.483\n      6.230\n      6.000\n      5.783\n    \n    \n      Argentina\n      12.887\n      12.380\n      11.840\n      11.283\n      10.733\n      10.203\n      9.683\n      9.177\n      8.680\n      8.227\n    \n    \n      Armenia\n      16.497\n      15.677\n      14.897\n      14.170\n      13.477\n      12.817\n      12.183\n      11.583\n      11.007\n      10.497\n    \n    \n      Australia\n      3.993\n      3.803\n      3.623\n      3.467\n      3.343\n      3.253\n      3.183\n      3.137\n      3.090\n      3.047\n    \n    \n      Austria\n      3.573\n      3.463\n      3.333\n      3.210\n      3.113\n      3.043\n      2.987\n      2.943\n      2.897\n      2.843"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-92",
    "href": "Certification/Preprocessing.html#question-92",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 92",
    "text": "Question 92\ndf3과 df4 데이터를 하나의 데이터 프레임으로 합쳐라. 둘다 포함하고 있는 년도에 대해서만 고려한다\n\n\nCode\ndf3 = df.iloc[:2, :4]\ndf4 = df.iloc[5:, 3:]\n\ndisplay(df3)\ndisplay(df4)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n    \n    \n      Location\n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      64.023\n      61.640\n      59.367\n      57.170\n    \n    \n      Albania\n      11.803\n      10.807\n      9.943\n      9.267\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Antigua and Barbuda\n      7.420\n      7.070\n      6.757\n      6.483\n      6.230\n      6.000\n      5.783\n    \n    \n      Argentina\n      11.283\n      10.733\n      10.203\n      9.683\n      9.177\n      8.680\n      8.227\n    \n    \n      Armenia\n      14.170\n      13.477\n      12.817\n      12.183\n      11.583\n      11.007\n      10.497\n    \n    \n      Australia\n      3.467\n      3.343\n      3.253\n      3.183\n      3.137\n      3.090\n      3.047\n    \n    \n      Austria\n      3.210\n      3.113\n      3.043\n      2.987\n      2.943\n      2.897\n      2.843\n    \n  \n\n\n\n\n\n\nCode\nAns = pd.concat([df3, df4], join = 'inner')\nAns\n\n\n\n\n\n\n  \n    \n      \n      2013\n    \n    \n      Location\n      \n    \n  \n  \n    \n      Afghanistan\n      57.170\n    \n    \n      Albania\n      9.267\n    \n    \n      Antigua and Barbuda\n      7.420\n    \n    \n      Argentina\n      11.283\n    \n    \n      Armenia\n      14.170\n    \n    \n      Australia\n      3.467\n    \n    \n      Austria\n      3.210"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-93",
    "href": "Certification/Preprocessing.html#question-93",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 93",
    "text": "Question 93\ndf3과 df4 데이터를 하나의 데이터 프레임으로 합쳐라. 모든 컬럼을 포함하고, 결측치는 0으로 대체한다\n\n\nCode\nAns = pd.concat([df3, df4], join = 'outer').fillna(0)\nAns\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Location\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      64.023\n      61.640\n      59.367\n      57.170\n      0.000\n      0.000\n      0.000\n      0.000\n      0.000\n      0.000\n    \n    \n      Albania\n      11.803\n      10.807\n      9.943\n      9.267\n      0.000\n      0.000\n      0.000\n      0.000\n      0.000\n      0.000\n    \n    \n      Antigua and Barbuda\n      0.000\n      0.000\n      0.000\n      7.420\n      7.070\n      6.757\n      6.483\n      6.230\n      6.000\n      5.783\n    \n    \n      Argentina\n      0.000\n      0.000\n      0.000\n      11.283\n      10.733\n      10.203\n      9.683\n      9.177\n      8.680\n      8.227\n    \n    \n      Armenia\n      0.000\n      0.000\n      0.000\n      14.170\n      13.477\n      12.817\n      12.183\n      11.583\n      11.007\n      10.497\n    \n    \n      Australia\n      0.000\n      0.000\n      0.000\n      3.467\n      3.343\n      3.253\n      3.183\n      3.137\n      3.090\n      3.047\n    \n    \n      Austria\n      0.000\n      0.000\n      0.000\n      3.210\n      3.113\n      3.043\n      2.987\n      2.943\n      2.897\n      2.843"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-94",
    "href": "Certification/Preprocessing.html#question-94",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 94",
    "text": "Question 94\ndf5과 df6 데이터를 하나의 데이터 프레임으로 merge함수를 이용하여 합쳐라. Algeria컬럼을 key로 하고 두 데이터 모두 포함하는 데이터만 출력하라\n\n\nCode\ndf5 = df.T.iloc[:7, :3]\ndf6 = df.T.iloc[6:, 2:5]\n\ndisplay(df5)\ndisplay(df6)\n\n\n\n\n\n\n  \n    \n      Location\n      Afghanistan\n      Albania\n      Algeria\n    \n  \n  \n    \n      2010\n      64.023\n      11.803\n      23.540\n    \n    \n      2011\n      61.640\n      10.807\n      22.907\n    \n    \n      2012\n      59.367\n      9.943\n      22.450\n    \n    \n      2013\n      57.170\n      9.267\n      22.117\n    \n    \n      2014\n      55.080\n      8.790\n      21.850\n    \n    \n      2015\n      53.107\n      8.493\n      21.587\n    \n    \n      2016\n      51.267\n      8.363\n      21.257\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      Location\n      Algeria\n      Andorra\n      Angola\n    \n  \n  \n    \n      2016\n      21.257\n      3.187\n      55.510\n    \n    \n      2017\n      20.850\n      3.060\n      53.460\n    \n    \n      2018\n      20.407\n      2.933\n      51.757\n    \n    \n      2019\n      19.930\n      2.827\n      50.093\n    \n  \n\n\n\n\n\n\nCode\nAns = pd.merge(df5, df6, on = 'Algeria', how = 'inner')\nAns\n\n\n\n\n\n\n  \n    \n      Location\n      Afghanistan\n      Albania\n      Algeria\n      Andorra\n      Angola\n    \n  \n  \n    \n      0\n      51.267\n      8.363\n      21.257\n      3.187\n      55.51"
  },
  {
    "objectID": "Certification/Preprocessing.html#question-95",
    "href": "Certification/Preprocessing.html#question-95",
    "title": "빅분기 실기 - 데이터 전처리",
    "section": "Question 95",
    "text": "Question 95\ndf5과 df6 데이터를 하나의 데이터 프레임으로 merge함수를 이용하여 합쳐라. Algeria컬럼을 key로 하고 합집합으로 합쳐라\n\n\nCode\nAns = pd.merge(df5, df6, on = 'Algeria', how = 'outer')\nAns\n\n\n\n\n\n\n  \n    \n      Location\n      Afghanistan\n      Albania\n      Algeria\n      Andorra\n      Angola\n    \n  \n  \n    \n      0\n      64.023\n      11.803\n      23.540\n      NaN\n      NaN\n    \n    \n      1\n      61.640\n      10.807\n      22.907\n      NaN\n      NaN\n    \n    \n      2\n      59.367\n      9.943\n      22.450\n      NaN\n      NaN\n    \n    \n      3\n      57.170\n      9.267\n      22.117\n      NaN\n      NaN\n    \n    \n      4\n      55.080\n      8.790\n      21.850\n      NaN\n      NaN\n    \n    \n      5\n      53.107\n      8.493\n      21.587\n      NaN\n      NaN\n    \n    \n      6\n      51.267\n      8.363\n      21.257\n      3.187\n      55.510\n    \n    \n      7\n      NaN\n      NaN\n      20.850\n      3.060\n      53.460\n    \n    \n      8\n      NaN\n      NaN\n      20.407\n      2.933\n      51.757\n    \n    \n      9\n      NaN\n      NaN\n      19.930\n      2.827\n      50.093"
  },
  {
    "objectID": "Certification/Type1.html",
    "href": "Certification/Type1.html",
    "title": "빅분기 실기 - 작업 1유형",
    "section": "",
    "text": "빅데이터분석기사 실기 - 작업 1유형"
  },
  {
    "objectID": "Certification/Type1.html#유튜브-인기동영상-데이터",
    "href": "Certification/Type1.html#유튜브-인기동영상-데이터",
    "title": "빅분기 실기 - 작업 1유형",
    "section": "유튜브 인기동영상 데이터",
    "text": "유튜브 인기동영상 데이터\n\n데이터 출처 : https://www.kaggle.com/rsrishav/youtube-trending-video-dataset?select=KR_youtube_trending_data.csv\n\n데이터 설명 : 유튜브 데일리 인기동영상 (한국)\n\nDataURL : https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv\n\n\n\nCode\ndf = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv', index_col = 0)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      title\n      channelTitle\n      categoryId\n      view_count\n      likes\n      dislikes\n      comment_count\n      channelId\n      trending_date2\n    \n  \n  \n    \n      0\n      [신병] 물자창고\n      장삐쭈\n      23\n      1893473\n      38249\n      730\n      8595\n      UChbE5OZQ6dRHECsX0tEPEZQ\n      2021-01-01\n    \n    \n      1\n      RAIN(비) - 나로 바꾸자 Switch to me (duet with JYP) MV\n      RAIN's Official Channel\n      10\n      2600864\n      0\n      0\n      20129\n      UCxXgIeE5hxWxHG6dz9Scg2w\n      2021-01-01\n    \n    \n      2\n      2020년 제야의 종 온라인 타종행사 | 보신각 현장 행사는 진행하지 않습니다.\n      서울시 · Seoul\n      29\n      347049\n      3564\n      120\n      178\n      UCZUPZW5idAxYp-Asj__lVAA\n      2021-01-01\n    \n    \n      3\n      고기남자의 칠면조 파티\n      고기남자 MeatMan\n      26\n      528458\n      15372\n      280\n      3470\n      UCT3CumbFIJiW33uq0UI3zlg\n      2021-01-01\n    \n    \n      4\n      골목 3mc를 분노하게 만든 마음고생이 심했을 공릉 백반집 사장님의 푸념?! [예능...\n      스브스밥집\n      24\n      494904\n      3918\n      111\n      3142\n      UCdWgRSfttvDucq4ApcCg5Mw\n      2021-01-01\n    \n  \n\n\n\n\n\nQuestion 1\n인기동영상 제작횟수가 많은 채널 상위 10개명을 출력하라 (날짜기준, 중복포함)\n\n\nQuestion 2\n논란으로 인기동영상이 된 케이스를 확인하고 싶다. dislikes수가 like 수보다 높은 동영상을 제작한 채널을 모두 출력하라\n\n\nQuestion 3\n채널명을 바꾼 케이스가 있는지 확인하고 싶다. channelId의 경우 고유값이므로 이를 통해 채널명을 한번이라도 바꾼 채널의 갯수를 구하여라\n\n\nQuestion 4\n일요일에 인기있었던 영상들중 가장많은 영상 종류(categoryId)는 무엇인가?\n\n\nQuestion 5\n각 요일별 인기 영상들의 categoryId는 각각 몇개 씩인지 하나의 데이터 프레임으로 표현하라\n\n\nQuestion 6\n댓글의 수로 (comment_count) 영상 반응에 대한 판단을 할 수 있다. viewcount대비 댓글수가 가장 높은 영상을 확인하라 (view_count값이 0인 경우는 제외한다)\n\n\nQuestion 7\n댓글의 수로 (comment_count) 영상 반응에 대한 판단을 할 수 있다.viewcount대비 댓글수가 가장 낮은 영상을 확인하라 (view_counts, ratio값이 0인경우는 제외한다.)\n\n\nQuestion 8\nlike 대비 dislike의 수가 가장 적은 영상은 무엇인가? (like, dislike 값이 0인경우는 제외한다)\n\n\nQuestion 9\n가장많은 트렌드 영상을 제작한 채널의 이름은 무엇인가? (날짜기준, 중복포함)\n\n\nQuestion 10\n20회(20일)이상 인기동영상 리스트에 포함된 동영상의 숫자는?"
  },
  {
    "objectID": "Certification.html",
    "href": "Certification.html",
    "title": "Certification",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\n빅분기 실기 - DataQ 제공문제\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅분기 실기 - 데이터 전처리\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅분기 실기 - 작업 1유형\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html",
    "href": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html",
    "title": "Uber Data를 활용한 분석",
    "section": "",
    "text": "h3 라이브러리를 활용한 Uber Data 분석"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#데이터의-기본적인-indexing-및-slicing",
    "href": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#데이터의-기본적인-indexing-및-slicing",
    "title": "Uber Data를 활용한 분석",
    "section": "1. 데이터의 기본적인 indexing 및 slicing",
    "text": "1. 데이터의 기본적인 indexing 및 slicing\n\n\nCode\n# 처음 5개의 행을 출력\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Date/Time\n      Lat\n      Lon\n      Base\n    \n  \n  \n    \n      0\n      4/1/2014 0:11:00\n      40.7690\n      -73.9549\n      B02512\n    \n    \n      1\n      4/1/2014 0:17:00\n      40.7267\n      -74.0345\n      B02512\n    \n    \n      2\n      4/1/2014 0:21:00\n      40.7316\n      -73.9873\n      B02512\n    \n    \n      3\n      4/1/2014 0:28:00\n      40.7588\n      -73.9776\n      B02512\n    \n    \n      4\n      4/1/2014 0:33:00\n      40.7594\n      -73.9722\n      B02512\n    \n  \n\n\n\n\n\n\nCode\n# 'Lat' 열의 값만 출력\ndf['Lat']\n\n\n0         40.7690\n1         40.7267\n2         40.7316\n3         40.7588\n4         40.7594\n           ...   \n564511    40.7640\n564512    40.7629\n564513    40.7443\n564514    40.6756\n564515    40.6880\nName: Lat, Length: 564516, dtype: float64\n\n\n\n\nCode\n# 10행부터 20행까지 출력\ndf.iloc[10:21]\n\n\n\n\n\n\n  \n    \n      \n      Date/Time\n      Lat\n      Lon\n      Base\n    \n  \n  \n    \n      10\n      4/1/2014 1:19:00\n      40.7256\n      -73.9869\n      B02512\n    \n    \n      11\n      4/1/2014 1:48:00\n      40.7591\n      -73.9684\n      B02512\n    \n    \n      12\n      4/1/2014 1:49:00\n      40.7271\n      -73.9803\n      B02512\n    \n    \n      13\n      4/1/2014 2:11:00\n      40.6463\n      -73.7896\n      B02512\n    \n    \n      14\n      4/1/2014 2:25:00\n      40.7564\n      -73.9167\n      B02512\n    \n    \n      15\n      4/1/2014 2:31:00\n      40.7666\n      -73.9531\n      B02512\n    \n    \n      16\n      4/1/2014 2:43:00\n      40.7580\n      -73.9761\n      B02512\n    \n    \n      17\n      4/1/2014 3:22:00\n      40.7238\n      -73.9821\n      B02512\n    \n    \n      18\n      4/1/2014 3:35:00\n      40.7531\n      -74.0039\n      B02512\n    \n    \n      19\n      4/1/2014 3:35:00\n      40.7389\n      -74.0393\n      B02512\n    \n    \n      20\n      4/1/2014 3:41:00\n      40.7619\n      -73.9715\n      B02512\n    \n  \n\n\n\n\n\n\nCode\n# Base 갯수 확인\ndf['Base'].value_counts()\n\n\nBase\nB02682    227808\nB02598    183263\nB02617    108001\nB02512     35536\nB02764      9908\nName: count, dtype: int64"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#결측치에-대한-처리",
    "href": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#결측치에-대한-처리",
    "title": "Uber Data를 활용한 분석",
    "section": "2. 결측치에 대한 처리",
    "text": "2. 결측치에 대한 처리\n\n\nCode\n# 결측치 개수 확인\ndf.isnull().sum()\n\n\nDate/Time    0\nLat          0\nLon          0\nBase         0\ndtype: int64"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#기초적인-통계-추출-및-분석",
    "href": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#기초적인-통계-추출-및-분석",
    "title": "Uber Data를 활용한 분석",
    "section": "3. 기초적인 통계 추출 및 분석",
    "text": "3. 기초적인 통계 추출 및 분석\n\n\nCode\n# 데이터 요약\nprint(df.describe())\n\n# 'Lat' 열의 평균\nprint(df['Lat'].mean())\n\n# 'Lon' 열의 중앙값\nprint(df['Lon'].median())\n\n# 'Base' 열에서 각 값의 빈도수 출력\nprint(df['Base'].value_counts())\n\n\n                 Lat            Lon\ncount  564516.000000  564516.000000\nmean       40.740005     -73.976817\nstd         0.036083       0.050426\nmin        40.072900     -74.773300\n25%        40.722500     -73.997700\n50%        40.742500     -73.984800\n75%        40.760700     -73.970000\nmax        42.116600     -72.066600\n40.74000520746974\n-73.9848\nBase\nB02682    227808\nB02598    183263\nB02617    108001\nB02512     35536\nB02764      9908\nName: count, dtype: int64"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#데이터에-대한-질문을-던져보고-해답을-찾기",
    "href": "Data_Mining/Geospatial_Analysis/data_analysis_uber.html#데이터에-대한-질문을-던져보고-해답을-찾기",
    "title": "Uber Data를 활용한 분석",
    "section": "4. 데이터에 대한 질문을 던져보고 해답을 찾기",
    "text": "4. 데이터에 대한 질문을 던져보고 해답을 찾기\n\n4.1 월별/요일별 Uber 이용량 비교\n\n가장 Uber 이용량이 많은 요일은 언제일까요?\n\n\n\nCode\ndf['Datetime'] = pd.to_datetime(df['Date/Time'])\ndf['Weekday'] = df['Datetime'].dt.day_name()\nweekday_count = df['Weekday'].value_counts().to_frame().reset_index()\ndisplay(weekday_count)\nprint(f\"Uber 이용량이 가장 많은 요일 : {weekday_count.loc[weekday_count['count'].idxmax(), 'Weekday']}\")\n\n\n\n\n\n\n  \n    \n      \n      Weekday\n      count\n    \n  \n  \n    \n      0\n      Wednesday\n      108631\n    \n    \n      1\n      Tuesday\n      91185\n    \n    \n      2\n      Friday\n      90303\n    \n    \n      3\n      Thursday\n      85067\n    \n    \n      4\n      Saturday\n      77218\n    \n    \n      5\n      Monday\n      60861\n    \n    \n      6\n      Sunday\n      51251\n    \n  \n\n\n\n\nUber 이용량이 가장 많은 요일 : Wednesday\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize = (8, 6))\nsns.barplot(x = 'Weekday', y = 'count', ax = ax, data = weekday_count, palette = 'Set2')\n\nplt.title('요일별 Uber 이용량')\nplt.show()\n\n\n\n\n\n\n\n4.2 시간대별 이용량 비교\n\n가장 이용량이 많은 시간대는 언제일까요?\n\n\n\nCode\ndf['Hour'] = df['Datetime'].dt.hour\nhour_count = df['Hour'].value_counts().to_frame().reset_index()\ndisplay(hour_count.head())\nprint(f\"Uber 이용량이 가장 많은 시간대 : {hour_count.loc[hour_count['count'].idxmax(), 'Hour']}\")\n\n\n\n\n\n\n  \n    \n      \n      Hour\n      count\n    \n  \n  \n    \n      0\n      17\n      45475\n    \n    \n      1\n      18\n      43003\n    \n    \n      2\n      16\n      42003\n    \n    \n      3\n      19\n      38923\n    \n    \n      4\n      21\n      36964\n    \n  \n\n\n\n\nUber 이용량이 가장 많은 시간대 : 17\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize = (8, 6))\nsns.barplot(x = 'Hour', y = 'count', ax = ax, data = hour_count, color = '#8BB0F5')# palette = 'viridis')\n\nplt.title('시간대별 Uber 이용량')\nplt.show()\n\n\n\n\n\n\n\n4.3 Uber 수요에 대한 공간적인 시각화\n\n가장 수요가 많은 지역은 어디일까요?\n시간대/지역별로 수요의 변화를 나타내보고, 시사점을 도출해 봅시다\n\n\n\nCode\ndf['hex'] = df.apply(lambda row : h3.geo_to_h3(row['Lat'], row['Lon'], 6), axis = 1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Date/Time\n      Lat\n      Lon\n      Base\n      Datetime\n      Weekday\n      Hour\n      hex\n    \n  \n  \n    \n      0\n      4/1/2014 0:11:00\n      40.7690\n      -73.9549\n      B02512\n      2014-04-01 00:11:00\n      Tuesday\n      0\n      862a100d7ffffff\n    \n    \n      1\n      4/1/2014 0:17:00\n      40.7267\n      -74.0345\n      B02512\n      2014-04-01 00:17:00\n      Tuesday\n      0\n      862a1072fffffff\n    \n    \n      2\n      4/1/2014 0:21:00\n      40.7316\n      -73.9873\n      B02512\n      2014-04-01 00:21:00\n      Tuesday\n      0\n      862a100d7ffffff\n    \n    \n      3\n      4/1/2014 0:28:00\n      40.7588\n      -73.9776\n      B02512\n      2014-04-01 00:28:00\n      Tuesday\n      0\n      862a100d7ffffff\n    \n    \n      4\n      4/1/2014 0:33:00\n      40.7594\n      -73.9722\n      B02512\n      2014-04-01 00:33:00\n      Tuesday\n      0\n      862a100d7ffffff\n    \n  \n\n\n\n\n\n\nCode\nh3_counts = Counter(df['hex']) # 각 셀에 포함된 데이터 개수 계산\ndf_h3_counts = pd.DataFrame.from_dict(h3_counts, orient = 'index').reset_index()\ndf_h3_counts.columns = ['h3', 'count']\n\ndf_h3_counts['lat'] = df_h3_counts['h3'].apply(lambda x : h3.h3_to_geo(x)[0])\ndf_h3_counts['lon'] = df_h3_counts['h3'].apply(lambda x : h3.h3_to_geo(x)[1])\n\ndf_h3_counts.head()\n\n\n\n\n\n\n  \n    \n      \n      h3\n      count\n      lat\n      lon\n    \n  \n  \n    \n      0\n      862a100d7ffffff\n      247294\n      40.742749\n      -73.957628\n    \n    \n      1\n      862a1072fffffff\n      125962\n      40.704168\n      -74.016008\n    \n    \n      2\n      862a10727ffffff\n      50036\n      40.759208\n      -74.033187\n    \n    \n      3\n      862a103b7ffffff\n      11639\n      40.654664\n      -73.789791\n    \n    \n      4\n      862a100f7ffffff\n      10999\n      40.781318\n      -73.899155\n    \n  \n\n\n\n\n\n\nCode\n# Define a layer to display on a map\nlayer = pdk.Layer(\n    'H3HexagonLayer',\n    df_h3_counts,\n    pickable = True,\n    stroked = True,\n    filled = True,\n    extruded = False,\n    get_hexagon = 'h3',\n    get_fill_color = '[255 - count, 255, count]',\n    get_line_color = [255, 255, 255],\n    line_width_min_pixels = 2,\n)\n\n# Set the viewport location\nview_state = pdk.ViewState(latitude = 40.7425, longitude = -73.9848,\n                           zoom = 8, bearing = 0, pitch = 30)\n\n# Render\nr = pdk.Deck(layers = [layer], initial_view_state = view_state, tooltip = {'text' : 'Count: {count}'})\nr.show()"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Exercise1_Your_First_Map.html",
    "href": "Data_Mining/Geospatial_Analysis/Exercise1_Your_First_Map.html",
    "title": "Exercise1 : Your First Map",
    "section": "",
    "text": "Kaggle Geospatial Analysis Exercise 1\n\n이 노트북은 Kaggle Geospatial Analysis의 Exercise 입니다. 이 링크로 튜토리얼을 볼 수 있습니다.\n\n\n소개\nKiva 는 전 세계 빈곤층에게 금융 서비스를 제공하는 온라인 크라우드펀딩 플랫폼입니다. Kiva의 대출자들은 200만명 이상의 사람들에게 10억 달러 이상의 대출을 제공했습니다.\n\n\n\nKiva는 전 세계 ‘현장 파트너’ 네트워크를 통해 세계에서 가장 외진 곳까지 도달합니다. 이러한 파트너는 지역 사회에서 대출자를 심사하고, 서비스를 제공하며, 대출을 관리하는 지역 단체입니다.\n이 Exercise에서는 필리핀의 Kiva 대출을 조사하게 됩니다. 새로운 현장 파트너를 모집할 기회를 찾기 위해 현재 Kiva의 네트워크 밖에 있을 수 있는 지역을 파악할 수 있나요?\n\n\nCode\nimport geopandas as gpd\n\n\n\n1) 데이터 불러오기\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 만듭니다.\n\n\nCode\nloans_filepath = './geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp'\nworld_loans =  gpd.read_file(loans_filepath)\nworld_loans.head()\n\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n  \n\n\n\n\n\n\n2) 데이터 시각화\n다음 셀을 실행하여 국가 경계가 포함된 world GeoDataFrame을 로드합니다.\n\n\nCode\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n  \n\n\n\n\nworld와 world_loans GeoDataFrame을 사용하여 전 세계의 Kiva 대출 위치를 시각화합니다.\n\n\nCode\nax = world.plot(figsize = (10, 10), color = 'lightgreen', linestyle = ':', edgecolor = 'black', zorder = 3)\nworld_loans.plot(color = 'maroon', markersize = 2, ax = ax)\n\n\n<Axes: >\n\n\n\n\n\n\n\n3) 필리핀 대출 선택\n다음으로 필리핀에 기반을 둔 대출에 초점을 맞춥니다. 다음 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다.\n\n\nCode\nPHL_loans = world_loans[world_loans['country'] == 'Philippines']\nPHL_loans.head()\n\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      2859\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.73961 17.64228)\n    \n    \n      2860\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.74169 17.63235)\n    \n    \n      2861\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.46667 16.60000)\n    \n    \n      2862\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      6050\n      POINT (121.73333 17.83333)\n    \n    \n      2863\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      625\n      POINT (121.51800 16.72368)\n    \n  \n\n\n\n\n\n\n4) 필리핀 대출 이해\n다음 셀을 실행하여 필리핀의 모든 섬에 대한 경계가 포함된 PHL GeoDataFrame을 로드합니다.\n\n\nCode\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file('./geospatial-learn-course-data/Philippines_AL258.kml', driver = 'KML')\nPHL.head()\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Autonomous Region in Muslim Mindanao\n      \n      MULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n    \n    \n      1\n      Bicol Region\n      \n      MULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n    \n    \n      2\n      Cagayan Valley\n      \n      MULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n    \n    \n      3\n      Calabarzon\n      \n      MULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n    \n    \n      4\n      Caraga\n      \n      MULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n    \n  \n\n\n\n\nPHL와 PHL_loans GeoDataFrame을 사용하여 필리핀의 대출 위치를 시각화합니다.\n\n\nCode\nax = PHL.plot(figsize = (5, 5), color = 'whitesmoke', linestyle = ':', edgecolor = 'black', zorder = 3)\nPHL_loans.plot(markersize = 4, ax = ax)\n\n\n<Axes: >\n\n\n\n\n\n새로운 현장 파트너를 모집하는 데 도움이 될 만한 섬을 식별할 수 있나요? 현재 Kiva의 손이 닿지 않는 섬이 있나요?\n이 지도가 질문에 답하는 데 유용할 수 있습니다.\n   \n여러 잠재적 섬이 있지만, 현재 데이터 세트에서 대출이 없는 비교적 큰 섬으로 민도로(필리핀 중부) 가 눈에 띕니다.\n이 섬은 새로운 필드 파트너를 모집하기에 좋은 장소가 될 수 있습니다!\n\n\n\n그 다음은?\nLesson 2 : Coordinate Reference Systems에 대해서 알아보세요.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Exercise2_Coordinate_Reference_Systems.html",
    "href": "Data_Mining/Geospatial_Analysis/Exercise2_Coordinate_Reference_Systems.html",
    "title": "Exercise2 : Coordinate Reference Systems",
    "section": "",
    "text": "Kaggle Geospatial Analysis Exercise 2\n\n이 노트북은 Kaggle Geospatial Analysis의 Exercise 입니다. 이 링크로 튜토리얼을 볼 수 있습니다.\n\n\n소개\n당신은 조류 보호 전문가이며 보라색 담비의 이동 패턴을 이해하고자 합니다. 연구를 통해 이 새가 일반적으로 미국 동부에서 여름 번식기를 보낸 후 겨울을 위해 남미로 이동한다는 사실을 알게 되었습니다. 하지만 이 새는 멸종 위기에 처해 있기 때문에 이 새가 방문할 가능성이 높은 장소를 자세히 살펴보고 싶을 것입니다.\n\n\n\n남아메리카에는 여러 보호 지역이 있으며, 이 지역으로 이동하거나 서식하는 종들이 번성할 수 있는 최상의 기회를 갖도록 특별 규정에 따라 운영되고 있습니다. 보라색 담비가 이러한 지역을 방문하는 경향이 있는지 알고 싶을 것입니다. 이 질문에 답하기 위해 최근에 수집한 11가지 새의 연중 위치를 추적하는 데이터를 사용합니다.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\n\n\n1) 데이터 불러오기\n다음 셀을 실행하여 GPS 데이터를 birds_df DataFrame 에 로드합니다.\n\n\nCode\nbirds_df = pd.read_csv('./geospatial-learn-course-data/purple_martin.csv', parse_dates = ['timestamp'])\nprint(f\"There are {birds_df['tag-local-identifier'].nunique()} different birds in the dataset.\")\nbirds_df.head()\n\n\nThere are 11 different birds in the dataset.\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n    \n  \n\n\n\n\nbird_df 데이터셋에는 11마리의 새가 있으며, 각 새는 tag-local-identifier 열의 고유 값으로 식별됩니다. 각 새는 일 년 중 다른 시기에 수집된 여러 측정값을 가지고 있습니다.\n다음 셀을 사용하여 GeoDataFrame birds를 만듭니다.\n- birds에는 (경도, 위도) 위치가 있는 포인트 개체를 포함하는 geometry 열과 함께 birds_df의 모든 열이 있어야 합니다.\n- birds의 CRS를 {'init': 'epsg:4326'}로 설정합니다.\n\n\nCode\nbirds = gpd.GeoDataFrame(birds_df, geometry = gpd.points_from_xy(birds_df['location-long'], birds_df['location-lat']))\nbirds.crs = {'init': 'epsg:4326'}\nbirds.head()\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n      POINT (-88.14601 17.51305)\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n      POINT (-85.24350 13.09578)\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n      POINT (-62.90609 -7.85244)\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n      POINT (-61.77683 -11.72390)\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n      POINT (-61.24154 -11.61224)\n    \n  \n\n\n\n\n\n\n2) 데이터 시각화\n다음 셀을 실행하여 GeoPandas에서 'naturalearth_lowres' 데이터를 로드하고 아메리카 대륙(북미와 남미 모두)의 모든 국가의 경계를 포함하는 GeoDataFrame을 americas로 설정합니다.\n\n\nCode\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      16\n      11263077.0\n      North America\n      Haiti\n      HTI\n      14332\n      POLYGON ((-71.71236 19.71446, -71.62487 19.169...\n    \n  \n\n\n\n\n다음 셀을 사용하여 (1) Americas GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 점을 모두 표시하는 단일 플롯을 만듭니다.\n여기서 특별한 스타일링에 대해 걱정할 필요 없이 모든 데이터가 제대로 로드되었는지 간단히 확인하기 위해 예비 플롯을 만들면 됩니다. 특히, 새를 구분하기 위해 점을 색상으로 구분할 필요가 없으며, 시작점과 끝점을 구분할 필요도 없습니다. 이 작업은 다음 Exercise에서 하도록 하겠습니다.\n\n\nCode\nax = americas.plot(figsize = (8, 8), color = 'white', linestyle = ':', edgecolor = 'black')\nbirds.plot(ax = ax, markersize = 10)\n\n\n<Axes: >\n\n\n\n\n\n\n\n3) 각 새들의 여행의 출발지와 도착지는 어디인가요? (파트 1)\n이제 각 새의 경로를 더 자세히 살펴볼 준비가 되었습니다. 다음 셀을 실행하여 두 개의 GeoDataFrame을 만듭니다. - path_gdf에는 각 새의 경로를 표시하는 LineString 객체가 포함되어 있습니다. 이 셀은 LineString() 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다. - start_gdf에는 각 새의 경로의 시작 지점이 포함됩니다.\n\n\nCode\npath_df = birds.groupby('tag-local-identifier')['geometry'].apply(list).apply(lambda x : LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry = path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\nstart_df = birds.groupby('tag-local-identifier')['geometry'].apply(list).apply(lambda x : x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry = start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\nstart_gdf.head()\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n\n\n다음 셀을 사용하여 각 새의 최종 위치가 포함된 end_gdf GeoDataFrame을 만듭니다.\n- 형식은 두 개의 열(tag-local-identifier 및 geometry)이 있는 start_gdf와 동일해야 하며, 여기서 geometry 열에는 포인트 개체가 포함되어야 합니다. - end_gdf의 CRS를 {'init': 'epsg:4326'}로 설정합니다.\n\n\nCode\nend_df = birds.groupby('tag-local-identifier')['geometry'].apply(list).apply(lambda x : x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry = end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\nend_gdf.head()\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-47.53632 -4.43758)\n    \n    \n      1\n      30054\n      POINT (-62.47914 -5.03840)\n    \n    \n      2\n      30198\n      POINT (-57.46417 -2.77617)\n    \n    \n      3\n      30263\n      POINT (-50.19230 -5.70504)\n    \n    \n      4\n      30275\n      POINT (-57.70404 -16.72336)\n    \n  \n\n\n\n\n\n\n4) 각 새들의 여행의 출발지와 도착지는 어디인가요? (파트 2)\n위의 GeoDataFrame(path_gdf, start_gdf, end_gdf)을 사용하여 모든 새의 경로를 단일 맵에 시각화합니다. americas GeoDataFrame을 사용할 수도 있습니다.\n\n\nCode\nax = americas.plot(figsize = (8, 8), color = 'white', linestyle = ':', edgecolor = 'black')\npath_gdf.plot(ax = ax, cmap = 'tab20b', linewidth = 1, linestyle = '-', zorder = 1)\nstart_gdf.plot(ax = ax, color = 'red', markersize = 10)\nend_gdf.plot(ax = ax, color = 'blue', markersize = 10)\n\n\n<Axes: >\n\n\n\n\n\n\n\n5) 남아메리카의 보호 지역은 어디인가요? (파트 1)\n모든 새들은 남미 어딘가에 있는 것 같습니다. 하지만 새들이 보호 지역으로 갈까요?\n다음 셀에서는 남아메리카의 모든 보호 지역의 위치가 포함된 protected_areas GeoDataFrame을 만듭니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다.\n\n\nCode\nprotected_filepath = './geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp'\nprotected_areas = gpd.read_file(protected_filepath)\nprotected_areas.head()\n\n\n\n\n\n\n  \n    \n      \n      WDPAID\n      WDPA_PID\n      PA_DEF\n      NAME\n      ORIG_NAME\n      DESIG\n      DESIG_ENG\n      DESIG_TYPE\n      IUCN_CAT\n      INT_CRIT\n      ...\n      GOV_TYPE\n      OWN_TYPE\n      MANG_AUTH\n      MANG_PLAN\n      VERIF\n      METADATAID\n      SUB_LOC\n      PARENT_ISO\n      ISO3\n      geometry\n    \n  \n  \n    \n      0\n      14067.0\n      14067\n      1\n      Het Spaans Lagoen\n      Het Spaans Lagoen\n      Ramsar Site, Wetland of International Importance\n      Ramsar Site, Wetland of International Importance\n      International\n      Not Reported\n      Not Reported\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Management plan is not implemented and not ava...\n      State Verified\n      1856\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-69.97523 12.47379, -69.97523 12.473...\n    \n    \n      1\n      14003.0\n      14003\n      1\n      Bubali Pond Bird Sanctuary\n      Bubali Pond Bird Sanctuary\n      Bird Sanctuary\n      Bird Sanctuary\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-70.04734 12.56329, -70.04615 12.563...\n    \n    \n      2\n      555624439.0\n      555624439\n      1\n      Arikok National Park\n      Arikok National Park\n      National Park\n      National Park\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Non-profit organisations\n      Non-profit organisations\n      Fundacion Parke Nacional Arikok\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      MULTIPOLYGON (((-69.96302 12.48384, -69.96295 ...\n    \n    \n      3\n      303894.0\n      303894\n      1\n      Madidi\n      Madidi\n      Area Natural de Manejo Integrado\n      Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-68.59060 -14.43388, -68.59062 -14.4...\n    \n    \n      4\n      303893.0\n      303893\n      1\n      Apolobamba\n      Apolobamba\n      Area Natural de Manejo Integado Nacional\n      National Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-69.20949 -14.73334, -69.20130 -14.7...\n    \n  \n\n5 rows × 29 columns\n\n\n\n\n\n6) 남아메리카의 보호 지역은 어디인가요? (파트 2)\nprotected_areas GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 지도를 생성합니다. (일부 보호 지역은 육지에 있는 반면 다른 보호 지역은 해역에 있음을 알 수 있습니다.)\n\n\nCode\nsouth_america = americas.loc[americas['continent'] == 'South America']\n\nax = south_america.plot(figsize = (5, 5), color = 'white', edgecolor = 'gray')\nprotected_areas.plot(ax = ax, alpha = 0.4)\n\n\n<Axes: >\n\n\n\n\n\n\n\n7) 남아메리카의 몇 퍼센트가 보호되고 있나요?\n남아메리카의 몇 퍼센트가 보호되고 있는지 확인하여 새들이 살기에 적합한 남아메리카의 면적을 파악하고 싶으신가요?\n첫번째 단계로 남아메리카의 모든 보호 지역(해양 지역 제외)의 총 면적을 계산합니다. 이를 위해 각각 총 면적과 총 해양 면적을 km² 단위로 포함하는 REP_AREA 및 REP_M_AREA 열을 사용합니다.\n\n\nCode\nP_Area = sum(protected_areas['REP_AREA'] - protected_areas['REP_M_AREA'])\nprint(f'South America has {P_Area} km² of protected areas.')\n\n\nSouth America has 5396761.9116883585 km² of protected areas.\n\n\n그리고, south_america GeoDataFrame을 사용하여 계산을 완료합니다.\n\n\nCode\nsouth_america.head()\n\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      20\n      3398.0\n      South America\n      Falkland Is.\n      FLK\n      282\n      POLYGON ((-61.20000 -51.85000, -60.00000 -51.2...\n    \n    \n      28\n      3461734.0\n      South America\n      Uruguay\n      URY\n      56045\n      POLYGON ((-57.62513 -30.21629, -56.97603 -30.1...\n    \n    \n      29\n      211049527.0\n      South America\n      Brazil\n      BRA\n      1839758\n      POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n    \n  \n\n\n\n\n다음 단계에 따라 남아메리카의 총 면적을 계산합니다. - 각 Polygon의 area 속성을 사용하여 각 국가의 면적을 계산하고 (CRS로 EPSG:3035를 사용) 결과를 합산합니다. 계산된 면적은 m² 단위로 표시됩니다. - 계산된 값을 km² 단위로 변환합니다.\n\n\nCode\ntotalArea = sum(south_america['geometry'].to_crs(epsg = 3035).area) / 10**6\ntotalArea\n\n\n17759005.81506123\n\n\n다음 셀을 실행하여 남아메리카의 보호 구역의 비율을 계산합니다.\n\n\nCode\npercentage_protected = P_Area / totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected * 100, 2)))\n\n\nApproximately 30.39% of South America is protected.\n\n\n\n\n8) 남아메리카의 새들은 어디에 있나요?\n새들이 보호 구역에 있을까요?\n모든 새에 대해 남아메리카에서 발견된 모든 위치를 보여주는 지도를 생성합니다. 또한, 남아메리카의 모든 보호 지역의 위치도 시각화합니다.\n육지 구성 요소가 없는 순수 해양 지역인 보호 지역을 제외하려면 MARINE 열을 사용하고 protected_areas GeoDataFrame의 모든 행 대신 protected_areas[protected_areas['MARINE'] != '2']의 행만 시각화하면 됩니다.\n\n\nCode\nax = south_america.plot(figsize = (5, 5), color = 'white', edgecolor = 'gray')\nprotected_areas[protected_areas['MARINE'] != '2'].plot(ax = ax, alpha = 0.4, zorder = 1)\nbirds[birds['geometry'].y < 0].plot(ax = ax, color = 'red', alpha = 0.6, markersize = 10, zorder = 2)\n\n\n<Axes: >\n\n\n\n\n\n\n\n\n그 다음은?\nLesson 3 : Interactive Maps 를 통해 지리공간 데이터로 대화형 지도를 만들어 보세요.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Exercise3_Interactive_Maps.html",
    "href": "Data_Mining/Geospatial_Analysis/Exercise3_Interactive_Maps.html",
    "title": "Exercise3 : Interactive Maps",
    "section": "",
    "text": "Kaggle Geospatial Analysis Exercise 3\n\n이 노트북은 Kaggle Geospatial Analysis의 Exercise 입니다. 이 링크로 튜토리얼을 볼 수 있습니다.\n\n\n소개\n일본의 도시 안전 계획가가 일본의 어느 지역에 추가적인 지진 보강이 필요한지 분석하고 있습니다. 인구 밀도가 높고 지진이 발생하기 쉬운 지역은 어디일까요 ?\n\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\n\nembed_map() 함수를 정의해 대화형 지도를 표시해봅시다. 이 함수는 지도가 포함된 벼수와 지도가 저장될 HTML 파일의 이름이라는 두가지 인수를 가집니다.\n이 함수는 지도가 모든 웹 브라우저에서 표시되도록 합니다.\n\n\nCode\ndef embed_map(m, file_name) :\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width = '100%', height = '500px')\n\n\n\n1) 지진이 판의 경계와 일치하나요?\n전 세계 판의 경계를 표시하는 plate_boundaries DataFrame을 생성합니다. coordinates 열은 경계를 따라 (위도, 경도) 위치의 목록입니다.\n\n\nCode\nplate_boundaries = gpd.read_file('./geospatial-learn-course-data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp')\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x : [(b, a) for (a, b) in list(x['geometry'].coords)], axis = 'columns')\nplate_boundaries.drop('geometry', axis = 1, inplace = True)\n\nplate_boundaries.head()\n\n\n\n\n\n\n  \n    \n      \n      HAZ_PLATES\n      HAZ_PLAT_1\n      HAZ_PLAT_2\n      Shape_Leng\n      coordinates\n    \n  \n  \n    \n      0\n      TRENCH\n      SERAM TROUGH (ACTIVE)\n      6722\n      5.843467\n      [(-5.444200361999947, 133.6808931800001), (-5....\n    \n    \n      1\n      TRENCH\n      WETAR THRUST\n      6722\n      1.829013\n      [(-7.760600482999962, 125.47879802900002), (-7...\n    \n    \n      2\n      TRENCH\n      TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ...\n      6621\n      6.743604\n      [(19.817899819000047, 120.09999798800004), (19...\n    \n    \n      3\n      TRENCH\n      BONIN TRENCH\n      9821\n      8.329381\n      [(26.175899215000072, 143.20620700100005), (26...\n    \n    \n      4\n      TRENCH\n      NEW GUINEA TRENCH\n      8001\n      11.998145\n      [(0.41880004000006466, 132.8273013480001), (0....\n    \n  \n\n\n\n\n과거 지진 데이터를 earthquakes DataFrame으로 로드합니다.\n\n\nCode\nearthquakes = pd.read_csv('./geospatial-learn-course-data/earthquakes1970-2014.csv', parse_dates = ['DateTime'])\nearthquakes.head()\n\n\n\n\n\n\n  \n    \n      \n      DateTime\n      Latitude\n      Longitude\n      Depth\n      Magnitude\n      MagType\n      NbStations\n      Gap\n      Distance\n      RMS\n      Source\n      EventID\n    \n  \n  \n    \n      0\n      1970-01-04 17:00:40.200\n      24.139\n      102.503\n      31.0\n      7.5\n      Ms\n      90.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970010e+09\n    \n    \n      1\n      1970-01-06 05:35:51.800\n      -9.628\n      151.458\n      8.0\n      6.2\n      Ms\n      85.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      2\n      1970-01-08 17:12:39.100\n      -34.741\n      178.568\n      179.0\n      6.1\n      Mb\n      59.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      3\n      1970-01-10 12:07:08.600\n      6.825\n      126.737\n      73.0\n      6.1\n      Mb\n      91.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      4\n      1970-01-16 08:05:39.000\n      60.280\n      -152.660\n      85.0\n      6.0\n      ML\n      0.0\n      NaN\n      NaN\n      NaN\n      AK\n      NaN\n    \n  \n\n\n\n\n다음 셀로 판의 경계를 지도에 시각화합니다. 모든 지진 데이터를 사용하여 동일한 맵에 히트맵을 추가하여 지진이 판의 경계와 일치하는지 여부를 확인하세요.\n\n\nCode\nm_1 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\nfor i in range(len(plate_boundaries)) :\n    folium.PolyLine(locations = plate_boundaries['coordinates'].iloc[i], weight = 2, color = 'black').add_to(m_1)\n\nHeatMap(data = earthquakes[['Latitude', 'Longitude']], radius = 15).add_to(m_1)\nm_1 # embed_map(m_1, 'q_1.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n위의 지도를 봤을 때, 지진이 판의 경계와 일치하나요? 네, 일치합니다.\n\n\n2) 일본의 지진 깊이와 판 경계와의 근접성 사이에 관계가 있나요?\n최근 지진의 깊이가 지구의 구조에 대한 중요한 정보를 알려준다는 글이 있습니다. 전 세계적으로 흥미로운 패턴이 있는지 궁금하고, 일본에서는 깊이가 어떻게 달라지는지도 알고 싶습니다.\n\n\nCode\nm_2 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\n\ndef color_producer(val) :\n    if val < 50 :\n        return 'forestgreen'\n    elif val < 100 : \n        return 'darkorange'\n    else :\n        return 'darkred'\n\nfor i in range(0, len(earthquakes)) :\n    folium.Circle(location = [earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n                  radius = 2000, color = color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n\nm_2 # embed_map(m_2, 'q_2.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n판 경계에 대한 근정성과 지진 깊이 사이의 관계를 알 수 있나요? 이 패턴이 전 세계적으로도 유지되나요? 일본에서는 이 패턴이 나타나나요?\n일본 북부에서는 판 경계에 가까운 지진의 깊이가 더 얕은 경향을 띄고, 판 경계에서 멀리 떨어진 지진의 깊이가 더 깊은 경향을 띄고 있습니다.\n이 패턴은 남미 서부 해안과 같은 다른 지역에서도 볼 수 있습니다. 그러나 중국, 몽골, 러시아 등 모든 곳에서 이 패턴이 적용되는 것은 아닙니다.\n\n\n3) 인구 밀도가 높은 도도부현(Prefecture)은 어디일까요?\n일본 도도부현의 지리적 경계를 포함하는 prefectures GeoDataFrame을 생성합니다.\n도도부현(Prefecture) : 일본의 광역 자치 단체인 도, 도, 부, 현을 묶어 이르는 말\n\n\nCode\nprefectures = gpd.read_file('./geospatial-learn-course-data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp')\nprefectures.set_index('prefecture', inplace = True)\nprefectures.head()\n\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      prefecture\n      \n    \n  \n  \n    \n      Aichi\n      MULTIPOLYGON (((137.09523 34.65330, 137.09546 ...\n    \n    \n      Akita\n      MULTIPOLYGON (((139.55725 39.20330, 139.55765 ...\n    \n    \n      Aomori\n      MULTIPOLYGON (((141.39860 40.92472, 141.39806 ...\n    \n    \n      Chiba\n      MULTIPOLYGON (((139.82488 34.98967, 139.82434 ...\n    \n    \n      Ehime\n      MULTIPOLYGON (((132.55859 32.91224, 132.55904 ...\n    \n  \n\n\n\n\n일본 각 도도부현의 인구, 면적(km²) 및 인구 밀도(km² 당)를 포함하는 stats DataFrame을 생성합니다.\n\n\nCode\npopulation = pd.read_csv('./geospatial-learn-course-data/japan-prefecture-population.csv')\npopulation.set_index('prefecture', inplace = True)\n\narea_sqkm = pd.Series(prefectures['geometry'].to_crs(epsg = 32654).area / 10**6, name = 'area_sqkm')\nstats = population.join(area_sqkm)\n\nstats['density'] = stats['population'] / stats['area_sqkm']\nstats.head()\n\n\n\n\n\n\n  \n    \n      \n      population\n      area_sqkm\n      density\n    \n    \n      prefecture\n      \n      \n      \n    \n  \n  \n    \n      Tokyo\n      12868000\n      1800.614782\n      7146.448049\n    \n    \n      Kanagawa\n      8943000\n      2383.038975\n      3752.771186\n    \n    \n      Osaka\n      8801000\n      1923.151529\n      4576.342460\n    \n    \n      Aichi\n      7418000\n      5164.400005\n      1436.372085\n    \n    \n      Saitama\n      7130000\n      3794.036890\n      1879.264806\n    \n  \n\n\n\n\n다음 셀로 인구 밀도를 시각화하는 단계구분도를 시각화하세요.\n\n\nCode\nm_3 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\n\nChoropleth(geo_data = prefectures.__geo_interface__, \n           data = stats['density'], \n           key_on = 'feature.id', \n           fill_color = 'YlGnBu', \n           legend_name = 'Population Density (per km²)'\n          ).add_to(m_3)\n\nm_3 # embed_map(m_3, 'q_3.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n다른 지역보다 상대적으로 인구 밀도가 높은 3개의 도도부현은 어디인가요? 일본 전역에 퍼져 있나요, 아니면 모두 거의 같은 지역에 위치해있나요?\n(일본 지리에 익숙하지 않다면 밑의 지도가 질문에 답하는데 유용할 수 있습니다.)\n\n\n\n\n도쿄, 가와나가, 오사카 의 인구 밀도가 다른 지역보다 상대적으로 인구 밀도가 높습니다. 이 현들은 모두 일본 중부에 위치하고 있으며, 도쿄와 가와나가는 인접해있습니다.\n\n\n4) 인구 밀도가 높은 현 중 강진이 발생하기 쉬운 현은 어디일까요?\n내진 보강을 해야하는 1개의 현을 제안하는 지도를 만드세요. 지도는 인구 밀도와 지진 규모를 모두 시각화해야합니다.\n\n\nCode\nm_4 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\n\ndef color_producer(magnitude) :\n    if magnitude > 6.5 :\n        return 'red'\n    else :\n        return 'green'\n\nfor i in range(0, len(earthquakes)) :\n    folium.Circle(location = [earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n                  popup = (f\"{earthquakes.iloc[i]['Magnitude']} ({earthquakes.iloc[i]['DateTime'].year})\"),\n                  radius = earthquakes.iloc[i]['Magnitude']**5.5,\n                  color = color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n\nChoropleth(geo_data = prefectures.__geo_interface__, \n           data = stats['density'], \n           key_on = 'feature.id', \n           fill_color = 'YlGnBu', \n           legend_name = 'Population Density (per km²)'\n          ).add_to(m_4)\n\nm_4 # embed_map(m_4, 'q_4.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n추가적으로 내진 보강이 필요한 현은 어디인가요?\n이 질문에 대한 명확한 정답은 없지만 몇가지 합리적인 옵션이 있습니다.\n\n도쿄 는 인구밀도가 가장 높은 현이며 지진이 많이 발생한 곳이기도 합니다.\n오사카 는 상대적으로 인구밀도가 낮지만 도쿄 인근 지역보다 상대적으로 강한 지진을 경험했습니다.\n가나가와 는 긴 해안으로 인해 인구 밀도가 높고 역사적으로 강한 지진이 자주 발생했기 때문에 잠재적인 쓰나미 위험에 대해 걱정할 수 있습니다.\n\n\n\n\n그 다음은?\nLesson 4 : Manipulating Geospatial Data 를 통해 지오코딩 을 사용하여 장소 이름을 지리적 좌표로 변환하는 방법을 알아보세요. 또한 여러 GeoDataFrame의 정보를 조인하는 방법에 대해서도 살펴봅시다.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Exercise4_Manipulating_Geospatial_Data.html",
    "href": "Data_Mining/Geospatial_Analysis/Exercise4_Manipulating_Geospatial_Data.html",
    "title": "Exercise4 : Manipulating Geospatial Data",
    "section": "",
    "text": "Kaggle Geospatial Analysis Exercise 4\n\n이 노트북은 Kaggle Geospatial Analysis의 Exercise 입니다. 이 링크로 튜토리얼을 볼 수 있습니다.\n\n\n소개\n당신은 스타벅스 리저브 로스터리로 전환할 다음 매장을 찾고 있는 스타벅스 빅데이터 전문가(실존하는 직업)입니다.\n스타벅스 리저브 로스터리는 일반 스타벅스 매장보다 훨씬 더 크고 다양한 음식과 와인, 고급 라운지 공간 등 여러가지 추가 기능을 갖추고 있습니다.\n캘리포니아 주 내 여러 지방의 인구 통계를 조사하여 잠재적으로 적합한 위치를 결정합니다.\n\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom geopy.geocoders import Nominatim\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\n\nembed_map() 함수를 사용해 지도를 시각화해봅시다.\n\n\nCode\ndef embed_map(m, file_name) :\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width = '100%', height = '500px')\n\n\n\n1) 누락된 위치 지오코딩\n캘리포니아 주의 스타벅스의 위치를 포함하는 starbucks라는 DataFrame을 생성합니다.\n\n\nCode\nstarbucks = pd.read_csv('./geospatial-learn-course-data/starbucks_locations.csv')\nstarbucks.head()\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n    \n  \n\n\n\n\n대부분의 매장의 위도와 경도는 알고 있습니다. 하지만, 버클리 시의 모든 위치가 누락되어 있습니다.\n\n\nCode\nprint(starbucks[['Longitude', 'Latitude']].isnull().sum(), '\\n')\n\nrows_with_missing = starbucks[starbucks['City'] == 'Berkeley']\nrows_with_missing\n\n\nLongitude    5\nLatitude     5\ndtype: int64 \n\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      153\n      5406-945\n      2224 Shattuck - Berkeley\n      2224 Shattuck Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      154\n      570-512\n      Solano Ave\n      1799 Solano Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      155\n      17877-164526\n      Safeway - Berkeley #691\n      1444 Shattuck Place Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      156\n      19864-202264\n      Telegraph & Ashby\n      3001 Telegraph Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      157\n      9217-9253\n      2128 Oxford St.\n      2128 Oxford Street Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n  \n\n\n\n\n아래 셀을 사용하여 Nominatim geocoder로 누락된 값을 채우세요.\n\n\n\n\n\n\n튜토리얼에서는 geopy.geocoders의 Nominatim()을 사용하여 값을 지오코딩했으며, 이 강좌 외의 프로젝트에서도 이 함수를 사용할 수 있습니다.\n\n\n이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools 내장 함수)을 사용합니다. 이 함수는 GeoPandas의 함수와 동일하게 작동합니다.\n\n\n즉, import 문을 변경하지 않는 한 노트북 상단의 가져오기 문을 변경하지 않고, 아래 코드 셀에서 지오코딩 함수를 geocode()로 호출하기만 하면 됩니다.\n\n\n\n\n\nCode\ngeolocator = Nominatim(user_agent = 'kaggle_learn')\n\ndef my_geocoder(row) :\n    try :\n        point = geolocator.geocode(row).point\n        return pd.Series({'Latitude' : point.latitude, 'Longitude' : point.longitude})\n    except :\n        return None\n\nrows_with_missing[['Latitude', 'Longitude']] = rows_with_missing.apply(lambda x : my_geocoder(x['Address']), axis = 1)\n\nstarbucks.update(rows_with_missing[['Latitude', 'Longitude']])\n\nprint('{}% of addresses were geocoded!'.format(\n    (1 - sum(np.isnan(starbucks['Latitude'])) / len(starbucks)) * 100))\n\n\n100.0% of addresses were geocoded!\n\n\n\n\n2) 버클리 지역 시각화\n방금 찾은 위치를 살펴봅시다. 버클리의 위도, 경도를 OpenStreetMap 스타일로 시각화해보세요.\n\n\nCode\nm_2 = folium.Map(location = [37.88, -122.26], tiles = 'openstreetmap', zoom_start = 13)\n\nfor idx, row in starbucks[starbucks['City'] == 'Berkeley'].iterrows() :\n    Marker([row['Latitude'], row['Longitude']], popup = row['Store Name']).add_to(m_2)\n    \nm_2 # embed_map(m_2, 'q_2.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n버클리의 5개의 매장만 고려했을 때, 몇 개의 매장이 잠재적으로 정확(정확히 도시에 위치)해 보이나요?\n5개 매장 모두 정확 해 보입니다.\n\n\n3) 데이터 통합\n아래 셀을 사용하여 캘리포니아 주의 각 지방에 대한 이름, 면적(km²) 및 고유ID(GEOID 열)가 포함된 CA_counties GeoDataFrame을 로드합니다.\ngeometry 열에는 각 지방의 경계가 있는 Polygon이 포함되어 있습니다.\n\n\nCode\nCA_counties = gpd.read_file('./geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp')\nCA_counties.head()\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n    \n  \n\n\n\n\n다음으로, 우리는 3개의 DataFrame을 생성합니다.\n\nCA_pop에는 각 지방의 인구 추정치가 포함됩니다.\nCA_high_earners에는 연간 소득이 $150,000 이상인 가구 수가 포함됩니다.\nCA_median_age에는 각 지방의 평균 연령을 포함합니다.\n\n\n\nCode\nCA_pop = pd.read_csv('./geospatial-learn-course-data/CA_county_population.csv', index_col = 'GEOID')\nCA_high_earners = pd.read_csv('./geospatial-learn-course-data/CA_county_high_earners.csv', index_col = 'GEOID')\nCA_median_age = pd.read_csv('./geospatial-learn-course-data/CA_county_median_age.csv', index_col = 'GEOID')\n\n\n아래 셀을 사용하여 CA_counties GeoDataFrame과 CA_pop, CA_high_earners, CA_median_age과 조인하세요.\n조인한 GeoDataFrame의 이름을 CA_stats로 지정하고, GEOID, name, area_sqkm, geometry, population, high_earners, median_age 8개의 열을 가지도록 하세요.\n또한 CRS가 {'init' : 'epsg:4326'}으로 설정되어 있는지 확인하세요.\n\n\nCode\nCA_all = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(CA_all, on = 'GEOID')\nCA_stats.crs = {'init' : 'epsg:4326'}\nCA_stats.head()\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n      2987\n      111\n      55.0\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n      1540975\n      65768\n      35.9\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n      446527\n      25231\n      33.7\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n      45602\n      2046\n      51.6\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n      850967\n      57121\n      37.5\n    \n  \n\n\n\n\n이제 모든 데이터가 한 곳에 있으므로 열의 조합으로 통계량을 계산하는 것이 훨씬 쉬워졌습니다.\n다음 셀을 실행하여 인구 밀도가 포함된 density 열을 만들어보세요.\n\n\nCode\nCA_stats['density'] = CA_stats['population'] / CA_stats['area_sqkm']\nCA_stats.head()\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n      density\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n      2987\n      111\n      55.0\n      1.198638\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n      1540975\n      65768\n      35.9\n      598.376878\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n      446527\n      25231\n      33.7\n      45.499825\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n      45602\n      2046\n      51.6\n      16.980022\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n      850967\n      57121\n      37.5\n      148.788107\n    \n  \n\n\n\n\n\n\n4) 어떤 지방이 유망해보이나요?\n모든 정보를 하나의 GeoDataFrame으로 축소하면 특정 기준을 충족하는 지방을 훨씬 쉽게 선택할 수 있습니다.\n다음 셀을 사용하여 CA_stats GeoDataFrame에서 행의 하위 집합 (및 모든 열)을 포함하는 sel_counties GeoDataFrame을 만듭니다.\n특히, 최소 10만 가구가 있는 지방을 선택해야합니다.\n\n연간 소득 $150,000를 버는 가구가 최소 10만 가구 이상\n연령 중앙값이 38.5세 미만\n주민 밀도가 최소 285명(km² 당) 이상\n\n추가적으로, 선정된 지방은 다음 기준 중 하나 이상을 충족해야합니다.\n\n연간 소득이 $150,000 이상인 가구가 최소 50만 가구 이상\n연령 중앙값이 35.5세 미만\n주민 밀도가 최소 1400명(km² 당) 이상\n\n\n\nCode\nsel_counties = CA_stats[(CA_stats['high_earners'] >= 100000) & (CA_stats['median_age'] < 38.5) & (CA_stats['density'] >= 285) &\n                        ((CA_stats['high_earners'] >= 500000) | (CA_stats['median_age'] < 35.5) | (CA_stats['density'] >= 1400))]\nsel_counties\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n      density\n    \n  \n  \n    \n      5\n      6037\n      Los Angeles County\n      12305.376879\n      MULTIPOLYGON (((-118.66761 33.47749, -118.6682...\n      10105518\n      501413\n      36.0\n      821.227834\n    \n    \n      8\n      6073\n      San Diego County\n      11721.342229\n      POLYGON ((-117.43744 33.17953, -117.44955 33.1...\n      3343364\n      194676\n      35.4\n      285.237299\n    \n    \n      10\n      6075\n      San Francisco County\n      600.588247\n      MULTIPOLYGON (((-122.60025 37.80249, -122.6123...\n      883305\n      114989\n      38.3\n      1470.733077\n    \n  \n\n\n\n\n\n\n5) 몇 개의 매장이 식별되었나요?\n다음 스타벅스 리저브 로스터리 매장을 찾을 때 선택한 지방 내의 모든 매장을 고려하고 싶을 것입니다.\n그렇다면, 선택한 지방 내에 몇 개의 매장이 있을까요?\n다음 셀을 실행하여 이 질문에 답을 할 수 있도록 모든 스타벅스 위치가 포함된 starbucks_gdf GeoDataFrame을 생성하세요.\n\n\nCode\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry = gpd.points_from_xy(starbucks['Longitude'], starbucks['Latitude']))\nstarbucks_gdf.crs = {'init' : 'epsg:4326'}\nstarbucks_gdf.head()\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n      POINT (-117.40000 34.51000)\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n      POINT (-118.76000 34.16000)\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n      POINT (-118.76000 34.15000)\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n      POINT (-117.75000 33.87000)\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n      POINT (-122.28000 37.79000)\n    \n  \n\n\n\n\n선택한 지방에 몇 개의 매장이 있나요?\n\n\nCode\ncounty_stores = gpd.sjoin(sel_counties, starbucks_gdf)\nnum_stores = len(county_stores)\nprint(num_stores)\n\n\n1043\n\n\n\n\n6) 매장 위치 시각화\n위에서 식별한 매장의 위치를 표시하는 지도를 만들어보세요.\n\n\nCode\nm_6 = folium.Map(location = [37, -120], zoom_start = 6)\n\nmc = MarkerCluster()\nfor idx, row in county_stores.iterrows() :\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(Marker([row['Latitude'], row['Longitude']]))\nm_6.add_child(mc)\n\nm_6 # embed_map(m_6, 'q_6.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\n그 다음은?\nLesson 5 : Proximity Analysis 를 통해 지도 상의 Point 간의 관계를 이해하는 것을 배워보세요.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Exercise5_Proximity_Analysis.html",
    "href": "Data_Mining/Geospatial_Analysis/Exercise5_Proximity_Analysis.html",
    "title": "Exercise5 : Proximity Analysis",
    "section": "",
    "text": "Kaggle Geospatial Analysis Exercise 5\n\n이 노트북은 Kaggle Geospatial Analysis의 Exercise 입니다. 이 링크로 튜토리얼을 볼 수 있습니다.\n\n\n소개\n위기 대응팀의 일원으로 뉴욕 시에서 발생한 충돌 사고에 대해 병원들이 어떻게 대응하고 있는지 파악해봅시다.\n\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n\nembed_map() 함수를 사용해 지도를 시각화해봅시다.\n\n\nCode\ndef embed_map(m, file_name) :\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width = '100%', height = '500px')\n\n\n\n1) 충돌 데이터 시각화\n다음 셀을 실행하여 2013~2018년 주요 자동차 충돌을 추적하는 collisions GeoDataFrame을 로드해보세요.\n\n\nCode\ncollisions = gpd.read_file('./geospatial-learn-course-data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp')\ncollisions.head()\n\n\n\n\n\n\n  \n    \n      \n      DATE\n      TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET\n      CROSS STRE\n      OFF STREET\n      ...\n      CONTRIBU_2\n      CONTRIBU_3\n      CONTRIBU_4\n      UNIQUE KEY\n      VEHICLE TY\n      VEHICLE _1\n      VEHICLE _2\n      VEHICLE _3\n      VEHICLE _4\n      geometry\n    \n  \n  \n    \n      0\n      07/30/2019\n      0:00\n      BRONX\n      10464\n      40.841100\n      -73.784960\n      (40.8411, -73.78496)\n      None\n      None\n      121       PILOT STREET\n      ...\n      Unspecified\n      None\n      None\n      4180045\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      Station Wagon/Sport Utility Vehicle\n      None\n      None\n      POINT (1043750.211 245785.815)\n    \n    \n      1\n      07/30/2019\n      0:10\n      QUEENS\n      11423\n      40.710827\n      -73.770660\n      (40.710827, -73.77066)\n      JAMAICA AVENUE\n      188 STREET\n      None\n      ...\n      None\n      None\n      None\n      4180007\n      Sedan\n      Sedan\n      None\n      None\n      None\n      POINT (1047831.185 198333.171)\n    \n    \n      2\n      07/30/2019\n      0:25\n      None\n      None\n      40.880318\n      -73.841286\n      (40.880318, -73.841286)\n      BOSTON ROAD\n      None\n      None\n      ...\n      None\n      None\n      None\n      4179575\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      None\n      None\n      None\n      POINT (1028139.293 260041.178)\n    \n    \n      3\n      07/30/2019\n      0:35\n      MANHATTAN\n      10036\n      40.756744\n      -73.984590\n      (40.756744, -73.98459)\n      None\n      None\n      155       WEST 44 STREET\n      ...\n      None\n      None\n      None\n      4179544\n      Box Truck\n      Station Wagon/Sport Utility Vehicle\n      None\n      None\n      None\n      POINT (988519.261 214979.320)\n    \n    \n      4\n      07/30/2019\n      10:00\n      BROOKLYN\n      11223\n      40.600090\n      -73.965910\n      (40.60009, -73.96591)\n      AVENUE T\n      OCEAN PARKWAY\n      None\n      ...\n      None\n      None\n      None\n      4180660\n      Station Wagon/Sport Utility Vehicle\n      Bike\n      None\n      None\n      None\n      POINT (993716.669 157907.212)\n    \n  \n\n5 rows × 30 columns\n\n\n\nLATITUDE와 LONGITUDE 열을 사용하여 대화형 지도를 만들어 충돌 데이터를 시각화해보세요.\n어떤 유형의 지도가 가장 효과적이라고 생각하시나요?\n\n\nCode\nm_1 = folium.Map(location = [40.7, -74], zoom_start = 11) \n\nHeatMap(data = collisions[['LATITUDE', 'LONGITUDE']], radius = 9).add_to(m_1)\n    \nm_1 # embed_map(m_1, 'q_1.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n2) 병원의 서비스 범위를 이해\n다음 셀을 실행하여 병원 데이터를 로드해보세요.\n\n\nCode\nhospitals = gpd.read_file('./geospatial-learn-course-data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp')\nhospitals.head()\n\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      address\n      zip\n      factype\n      facname\n      capacity\n      capname\n      bcode\n      xcoord\n      ycoord\n      latitude\n      longitude\n      geometry\n    \n  \n  \n    \n      0\n      317000001H1178\n      BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI...\n      1650 Grand Concourse\n      10457\n      3102\n      Hospital\n      415\n      Beds\n      36005\n      1008872.0\n      246596.0\n      40.843490\n      -73.911010\n      POINT (1008872.000 246596.000)\n    \n    \n      1\n      317000001H1164\n      BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION\n      1276 Fulton Ave\n      10456\n      3102\n      Hospital\n      164\n      Beds\n      36005\n      1011044.0\n      242204.0\n      40.831429\n      -73.903178\n      POINT (1011044.000 242204.000)\n    \n    \n      2\n      317000011H1175\n      CALVARY HOSPITAL INC\n      1740-70 Eastchester Rd\n      10461\n      3102\n      Hospital\n      225\n      Beds\n      36005\n      1027505.0\n      248287.0\n      40.848060\n      -73.843656\n      POINT (1027505.000 248287.000)\n    \n    \n      3\n      317000002H1165\n      JACOBI MEDICAL CENTER\n      1400 Pelham Pkwy\n      10461\n      3102\n      Hospital\n      457\n      Beds\n      36005\n      1027042.0\n      251065.0\n      40.855687\n      -73.845311\n      POINT (1027042.000 251065.000)\n    \n    \n      4\n      317000008H1172\n      LINCOLN MEDICAL & MENTAL HEALTH CENTER\n      234 E 149 St\n      10451\n      3102\n      Hospital\n      362\n      Beds\n      36005\n      1005154.0\n      236853.0\n      40.816758\n      -73.924478\n      POINT (1005154.000 236853.000)\n    \n  \n\n\n\n\nLATITUDE와 LONGITUDE 열을 사용하여 병원 위치를 시각화해보세요.\n\n\nCode\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup = row['name']).add_to(m_2)\n    \nm_2 # embed_map(m_2, 'q_2.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n3) 가장 가까운 병원이 10km 이상 떨어진 충돌 발생 시점은 언제였나요?\n가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌이 있는 collisions의 모든 행을 포함하는 outside_range DataFrame을 생성합니다.\nhospitals와 collisions DataFrame 모두 좌표 참조 시스템 (CRS)로 EPSG 2263을 사용하며, EPSG 2263는 미터 단위를 사용합니다.\n\n\nCode\nbuffer = gpd.GeoDataFrame(geometry = hospitals['geometry']).buffer(10000)\nmy_union = buffer.geometry.unary_union\noutside_range = collisions.loc[~collisions['geometry'].apply(lambda x : my_union.contains(x))]\noutside_range.head()\n\n\n\n\n\n\n  \n    \n      \n      DATE\n      TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET\n      CROSS STRE\n      OFF STREET\n      ...\n      CONTRIBU_2\n      CONTRIBU_3\n      CONTRIBU_4\n      UNIQUE KEY\n      VEHICLE TY\n      VEHICLE _1\n      VEHICLE _2\n      VEHICLE _3\n      VEHICLE _4\n      geometry\n    \n  \n  \n    \n      0\n      07/30/2019\n      0:00\n      BRONX\n      10464\n      40.841100\n      -73.784960\n      (40.8411, -73.78496)\n      None\n      None\n      121       PILOT STREET\n      ...\n      Unspecified\n      None\n      None\n      4180045\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      Station Wagon/Sport Utility Vehicle\n      None\n      None\n      POINT (1043750.211 245785.815)\n    \n    \n      1\n      07/30/2019\n      0:10\n      QUEENS\n      11423\n      40.710827\n      -73.770660\n      (40.710827, -73.77066)\n      JAMAICA AVENUE\n      188 STREET\n      None\n      ...\n      None\n      None\n      None\n      4180007\n      Sedan\n      Sedan\n      None\n      None\n      None\n      POINT (1047831.185 198333.171)\n    \n    \n      5\n      07/30/2019\n      10:50\n      QUEENS\n      11423\n      40.721060\n      -73.759450\n      (40.72106, -73.75945)\n      FRANCIS LEWIS BOULEVARD\n      HILLSIDE AVENUE\n      None\n      ...\n      None\n      None\n      None\n      4179812\n      Sedan\n      Box Truck\n      None\n      None\n      None\n      POINT (1050928.749 202069.687)\n    \n    \n      6\n      07/30/2019\n      10:55\n      QUEENS\n      11434\n      40.676228\n      -73.761120\n      (40.676228, -73.76112)\n      CRANDALL AVENUE\n      CHENEY STREET\n      None\n      ...\n      None\n      None\n      None\n      4180464\n      Station Wagon/Sport Utility Vehicle\n      Station Wagon/Sport Utility Vehicle\n      None\n      None\n      None\n      POINT (1050510.380 185734.852)\n    \n    \n      15\n      07/30/2019\n      13:05\n      None\n      None\n      40.588413\n      -74.166725\n      (40.588413, -74.166725)\n      None\n      None\n      26        RICHMOND HILL ROAD\n      ...\n      None\n      None\n      None\n      4180091\n      Station Wagon/Sport Utility Vehicle\n      Box Truck\n      None\n      None\n      None\n      POINT (937943.004 153695.210)\n    \n  \n\n5 rows × 30 columns\n\n\n\n다음 셀은 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌의 비율을 계산합니다.\n\n\nCode\npercentage = round(len(outside_range) / len(collisions) * 100, 2)\nprint('Percentage of collisions more than 10km away from the closest hospital : {}%'.format(percentage))\n\n\nPercentage of collisions more than 10km away from the closest hospital : 15.12%\n\n\n\n\n4) 추천하기\n먼 곳에서 충돌 사고가 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다.\n이를 염두에 두고, 다음과 같은 추천을 만들기로 결정합니다.\n\n충돌 위치(EPSG 2263)를 입력으로 받습니다.\n가장 가까운 병원을 찾습니다(거리 계산은 EPSG 2263으로 수행).\n가장 가까운 병원의 이름을 반환합니다.\n\n\n\nCode\ndef best_hospital(collision_location) :\n    idx_min = hospitals['geometry'].distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    name = my_hospital['name']\n    return name\n\nprint(best_hospital(outside_range['geometry'].iloc[0]))\n\n\nCALVARY HOSPITAL INC\n\n\n\n\n5) 가장 수요가 많은 병원은 어디일까요?\noutside_range DataFrame의 충돌만 고려할 때, 가장 추천하는 병원은 어디일까요?\n정답은 4) 에서 생성한 함수가 반환한 병원 이름과 정확히 일치해야합니다.\n\n\nCode\nhighest_demand = outside_range['geometry'].apply(best_hospital).value_counts().idxmax()\nhighest_demand\n\n\n'JAMAICA HOSPITAL MEDICAL CENTER'\n\n\n\n\n6) 시에서 새로운 병원을 어디에 건설해야 할까요?\n다음 셀을 실행하여 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌과 병원의 위치를 시각화해보세요.\n\n\nCode\nm_6 = folium.Map(location = [40.7, -74], zoom_start = 11) \n\ncoverage = gpd.GeoDataFrame(geometry = hospitals['geometry']).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg = 4326)).add_to(m_6)\nHeatMap(data = outside_range[['LATITUDE', 'LONGITUDE']], radius = 9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nm_6 # embed_map(m_6, 'm_6.html')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n지도의 아무 곳이나 클릭하면 해당 위치가 위도와 경도로 표시된 팝업이 표시됩니다.\n뉴욕 시에서 새로운 병원 두 곳의 입지를 결정하는데 도움을 요청하는 연락이 왔습니다. 특히 3) 단계에서 계산된 비율을 10% 미만으로 낮추기 위해 위치를 식별하는데 도움을 요청합니다.\n지도를 사용하여(그리고 구역법이나 병원을 짓기 위해 특정 건물을 철거해야 하는지에 대해 걱정하지 않고) 이 목표를 달성하는데 도움이 될 두 곳의 위치를 찾을 수 있나요?\n병원 1의 제안된 위도와 경도를 각각 lat_1, lon_1으로 할당하세요. (병원 2도 마찬가지입니다.)\n그리고, 다음 셀을 실행하여 새로운 병원의 효과를 확인합니다. 두 개의 새 병원을 통해 백분율이 10% 미만이 되면 정답입니다.\n\n\nCode\nlat_1, lon_1 = 40.6714, -73.8492\nlat_2, lon_2 = 40.6702, -73.7612\n\nnew_df = pd.DataFrame(\n    {'Latitude' : [lat_1, lat_2],\n     'Longitude' : [lon_1, lon_2]})\nnew_gdf = gpd.GeoDataFrame(new_df, geometry = gpd.points_from_xy(new_df['Longitude'], new_df['Latitude']))\nnew_gdf.crs = {'init' : 'epsg:4326'}\nnew_gdf = new_gdf.to_crs(epsg = 2263)\nnew_coverage = gpd.GeoDataFrame(geometry = new_gdf.geometry).buffer(10000)\nnew_my_union = new_coverage.geometry.unary_union\nnew_outside_range = outside_range.loc[~outside_range['geometry'].apply(lambda x : new_my_union.contains(x))]\nnew_percentage = round(len(new_outside_range) / len(collisions) * 100, 2)\nprint('(NEW) Percentage of collisions more than 10km away from the closest hospital : {}%'.format(new_percentage))\nm = folium.Map(location = [40.7, -74], zoom_start = 11) \nfolium.GeoJson(coverage.geometry.to_crs(epsg = 4326)).add_to(m)\nfolium.GeoJson(new_coverage.geometry.to_crs(epsg = 4326)).add_to(m)\nfor idx, row in new_gdf.iterrows() :\n    Marker([row['Latitude'], row['Longitude']]).add_to(m)\nHeatMap(data = new_outside_range[['LATITUDE', 'LONGITUDE']], radius = 9).add_to(m)\nfolium.LatLngPopup().add_to(m)\ndisplay(m) # embed_map(m, 'q_6.html')\n\n\n(NEW) Percentage of collisions more than 10km away from the closest hospital : 9.12%\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\n축하합니다!\nGeospatial Analysis 강좌를 완료하였습니다! 고생하셨습니다!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Kaggle Geospatial Analysis Course"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개",
    "title": "Geospatial Analysis",
    "section": "소개",
    "text": "소개\n이 과정에서는 지리 공간 데이터 또는 지리적 위치가 있는 데이터를 다루고 시각화하는 다양한 방법에 대해 알아봅니다.\n\n\n\n\n그 과정에서 다음과 같은 몇 가지 실제 문제에 대한 솔루션을 제공하게 됩니다.\n\n글로벌 비영리 단체가 필리핀의 외딴 지역에서 활동 범위를 넓히려면 어디로 가야할까요?\n멸종 위기 조류인 보라색 담비는 북미와 남미를 어떻게 이동하나요? 그 새들은 보호 지역으로 이동하나요?\n일본의 어느 지역을 추가적으로 내진 설계를 해야할까요?\n캘리포니아의 어느 스타벅스 매장이 다음 스타벅스 리저브 로스터리 매장으로 유력한 후보지인가요?\n뉴욕에는 자동차 충돌 사고에 대응할 수 있는 충분한 병원이 있나요? 뉴욕에서 의료 서비스 제공에 공백이 있는 지역은 어디일까요?\n\n또한, 보스턴의 범죄를 시각화하고, 가나의 의료 시설을 조사하고, 유럽의 최고 대학을 탐색하고, 미국의 독성 화학물질 방출을 추적할 수 있습니다.\n이 첫번째 튜토리얼에서는 이 강좌를 완료하는데 필요한 전제 조건을 빠르게 다룹니다. 더 깊이 잇는 복습을 원하신다면, Pandas 강좌를 추천합니다.\n이제, 첫번째 지리공간 데이터 집합을 시각화해보겠습니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#데이터-불러오기",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#데이터-불러오기",
    "title": "Geospatial Analysis",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n첫번째 단계는 지리공간 데이터를 불러오는 것입니다. 이를 위해 GeoPandas 라이브러리를 사용하겠습니다.\n지리공간 파일 형식에는 shapefile, GeoJSON, KML, GPKG 등의 다양한 형식이 있습니다. 이 강좌에서는 그 차이점에 대해서는 다루지 않겠지만, 중요한 것들은 다음과 같습니다. - shapefile이 가장 흔한 파일 형식입니다. - 지리공간 파일의 모든 형식은 gpd.read_file() 함수를 이용하여 빠르게 불러올 수 있습니다.\n다음 셀은 뉴욕 주 환경보전국에서 관리하는 숲, 야생지대 및 기타 토지에 대한 정보가 포함된 shapefile을 불러옵니다.\n\n\nCode\n# Read in the data\nfull_data = gpd.read_file(\"./geospatial-learn-course-data/DEC_lands/DEC_lands/DEC_lands.shp\")\n\n# View the first five rows of the data\nfull_data.head()\n\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      CATEGORY\n      UNIT\n      FACILITY\n      CLASS\n      UMP\n      DESCRIPTIO\n      REGION\n      COUNTY\n      URL\n      SOURCE\n      UPDATE_\n      OFFICE\n      ACRES\n      LANDS_UID\n      GREENCERT\n      SHAPE_AREA\n      SHAPE_LEN\n      geometry\n    \n  \n  \n    \n      0\n      1\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      738.620192\n      103\n      N\n      2.990365e+06\n      7927.662385\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      2\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      282.553140\n      1218\n      N\n      1.143940e+06\n      4776.375600\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      3\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      234.291262\n      1780\n      N\n      9.485476e+05\n      5783.070364\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      4\n      FOR PRES DET PAR\n      CFP\n      GREENE COUNTY FP DETACHED PARCEL\n      WILD FOREST\n      None\n      None\n      4\n      GREENE\n      http://www.dec.ny.gov/\n      GREENE RPP\n      5/12\n      STAMFORD\n      450.106464\n      2060\n      N\n      1.822293e+06\n      7021.644833\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      6\n      FOREST PRESERVE\n      AFP\n      SARANAC LAKES WILD FOREST\n      WILD FOREST\n      SARANAC LAKES\n      None\n      5\n      ESSEX\n      http://www.dec.ny.gov/lands/22593.html\n      DECRP, ESSEX RPP\n      12/96\n      RAY BROOK\n      69.702387\n      1517\n      N\n      2.821959e+05\n      2663.909932\n      POLYGON ((583896.043 4909643.187, 583891.200 4...\n    \n  \n\n\n\n\nCLASS 열에서 볼 수 있듯이, 처음 5개 행은 각각 다른 숲에 해당합니다.\n이 튜토리얼의 나머지 부분에서는 이 데이터를 사용하여 주말 캠핑 여행을 계획하는 시나리오를 고려해보겠습니다.\n온라인에서 크라우드 소싱된 리뷰에 의존하는 대신 자신만의 지도를 만들어봅시다.\n이렇게 하면 특정 관심사에 맞게 여행을 조정할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#전제-조건",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#전제-조건",
    "title": "Geospatial Analysis",
    "section": "전제 조건",
    "text": "전제 조건\n데이터의 처음 5개의 행을 보기 위해 head() 함수를 사용했습니다. 이 함수는 Pandas DataFrame에서도 사용된다는 것을 알고 있을 것입니다.\n사실, 모든 명령어는 Pandas DataFrame에서도 함께 작동합니다.\n이는, 데이터가 (Pandas) DataFrame의 모든 기능을 갖춘 (GeoPandas) GeoDataFrame 객체로 로드되었기 때문입니다.\n\n\nCode\ntype(full_data)\n\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n예를 들어, 모든 열을 사용하지 않으려는 경우 열의 하위 집합을 선택할 수 있습니다. (데이터를 선택하는 다른 방법을 복습하려면, Pandas 강좌의 튜토리얼을 확인하세요.)\n\n\nCode\ndata = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n\n\nvalue_counts() 함수를 사용하여 다양한 토지 유형 목록과 함께 데이터 집합에 나타나는 횟수를 확인합니다. (이 함수 (관련 함수)를 복습하려면, Pandas 강좌의 튜토리얼을 확인하세요.)\n\n\nCode\n# 각 유형별로 토지의 개수는 몇개인가요?\ndata['CLASS'].value_counts()\n\n\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: CLASS, dtype: int64\n\n\nloc(iloc)를 사용하여 데이터의 하위 집합을 선택할 수도 있습니다. (이 함수를 복습하려면, Pandas 강좌의 튜토리얼을 확인하세요.)\n\n\nCode\n# \"WILD FOREST\" 또는 \"WILDERNESS\"에 해당하는 토지를 선택합니다.\nwild_lands = data.loc[data['CLASS'].isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()\n\n\n\n\n\n\n  \n    \n      \n      CLASS\n      COUNTY\n      geometry\n    \n  \n  \n    \n      0\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      WILD FOREST\n      DELAWARE\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      WILD FOREST\n      GREENE\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      WILD FOREST\n      ESSEX\n      POLYGON ((583896.043 4909643.187, 583891.200 4..."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#첫번째-지도를-만들어보세요",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#첫번째-지도를-만들어보세요",
    "title": "Geospatial Analysis",
    "section": "첫번째 지도를 만들어보세요!",
    "text": "첫번째 지도를 만들어보세요!\nplot() 함수를 사용하여 데이터를 빠르게 시각화할 수 있습니다.\n\n\nCode\nwild_lands.plot()\n\n\n<Axes: >\n\n\n\n\n\n모든 GeoDataFrame에는 특별한 geometry 열이 포함되어 있습니다. 이 열에는 plot() 메서드를 호출할 때 표시되는 모든 기하학적 개체가 포함됩니다.\n\n\nCode\nwild_lands['geometry'].head()\n\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\n\n\n이 열에는 다양한 데이터 유형이 포함될 수 있지만, 일반적으로 Point, LineString, or Polygon 입니다.\n\n\n\n이 데이터의 geometry 열에는 2,983개의 서로 다른 Polygon 객체가 포함되어 있고, 각 객체는 plot에서 서로 다른 모양에 해당합니다.\n아래 셀을 통해 우리는 캠프장 위치 (Point), 도보 경로 (LineString), 지역 경계 (Polygon)를 포함한 3개의 GeoDataFrame을 더 생성합니다.\n\n\nCode\n# 뉴욕 주의 캠프장 위치 (Point)\nPOI_data = gpd.read_file('./geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp')\ncampsites = POI_data.loc[POI_data['ASSET'] == 'PRIMITIVE CAMPSITE'].copy()\n\n# 뉴욕 주의 도보 경로 (LineString)\nroads_trails = gpd.read_file('./geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp')\ntrails = roads_trails.loc[roads_trails['ASSET'] == 'FOOT TRAIL'].copy()\n\n# 뉴욕 주의 지역 경계 (Polygon)\ncounties = gpd.read_file('./geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp')\n\n\n다음으로, 4개의 GeoDataFrame을 모두 사용하여 지도를 만들어봅시다.\nplot() 함수는 지도를 커스터마이징하는데 필요로 하는 데 사용할 수 있는 몇가지 파라미터(선택사항)를 입력으로 받습니다.\n가장 중요한 것은 ax에 값을 설정하면, 모든 정보가 동일한 지도에 그려진다는 것입니다.\n\n\nCode\n# Base Map을 지역 경계로 설정\nax = counties.plot(figsize = (10, 10), color = 'none', edgecolor = 'gainsboro', zorder = 3)\n\n# 토지와 캠프장 위치, 도보 경로를 Base Map에 추가\nwild_lands.plot(color = 'lightgreen', ax = ax)\ncampsites.plot(color = 'maroon', markersize = 2, ax = ax)\ntrails.plot(color = 'black', markersize = 1, ax = ax)\n\n\n<Axes: >\n\n\n\n\n\n북동부 지역 이 캠핑 여행지에 좋은 선택지가 될 것 같습니다!"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습",
    "title": "Geospatial Analysis",
    "section": "실습",
    "text": "실습\n처음엔 복잡하게 느껴지겠지만, 이미 중요한 분석을 수행할 수 있을만큼 충분히 배웠을 것입니다.\nExercise1 : Your First Map를 통해 비영리 단체가 사업을 확장할 수 있는 필리핀의 외딴 지역을 직접 찾아보세요."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-1",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-1",
    "title": "Geospatial Analysis",
    "section": "소개",
    "text": "소개\n이 강좌에서 만드는 지도는 지구 표면을 2차원으로 묘사합니다. 하지만 아시다시피 지구는 실제로 3차원입니다. 따라서, 지구를 평면으로 렌더링하려면 Map Projection(지도 투영) 이라는 방법을 사용해야합니다.\nMap Projection은 100% 정확할 수 없습니다. 각 투영법은 지구 표면을 중요한 속성은 유지하지만, 어떤 식으로든 왜곡합니다.\n예를 들어,\n\nequal-area projections(등면적 투영, ‘Lambert Cylindrical Equal Area’, or ‘Africa Albers Equal Area Conic’ 등)은 면적을 보존합니다.\n\n국가나 도시의 면적을 계산하려는 경우 이 투영법을 선택하는 것이 좋습니다.\n\nequidstant projections(등거리 투영, ‘Azimuthal Equidistant projection’)은 거리를 보존합니다.\n\n비행 거리를 계산할 때 좋은 투영법입니다.\n\n\n\n\n\n\n투영된 점이 지구상의 실제 위치와 어떻게 일치하는지 보여주기 위해 좌표 참조 체계 (CRS) 를 사용합니다. 이 튜토리얼에서는 좌표 참조계에 대해 자세히 알아보고 GeoPandas에서 좌표 참조계를 사용하는 방법을 알아봅시다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#crs-설정",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#crs-설정",
    "title": "Geospatial Analysis",
    "section": "CRS 설정",
    "text": "CRS 설정\nshapefile에서 GeoDataFrame을 만들 때, CRS는 이미 로드되어 있습니다.\n\n\nCode\n# 가나의 지역이 포함된 GeoDataFrame 불러오기\nregions = gpd.read_file('./geospatial-learn-course-data/ghana/ghana/Regions/Map_of_Regions_in_Ghana.shp')\nprint(regions.crs)\n\n\nepsg:32630\n\n\n이를 어떻게 해석해야하나요?\n좌표 참조 시스템은 유럽 석유 측량 그룹 (EPSG) 코드에 의해 참조됩니다.\n이 GeoDataFrame은 일반적으로 ‘Mercator (메르카토르)’ 투영법이라고 더 많이 불리는 EPSG 32630을 사용합니다. 이 투영법은 각도를 보존하고 (해상 항해에 유용) 면적을 약간 왜곡합니다.\n그러나, CSV 파일에서 GeoDataFrame을 만들 때, CRS를 설정해야합니다. EPSG 4326은 위도 및 경도 좌표에 해당합니다.\n\n\nCode\n# 가나의 의료 시설로 DataFrame을 만들기\nfacilities_df = pd.read_csv('./geospatial-learn-course-data/ghana/ghana/health_facilities.csv')\n\n# DataFrame을 GeoDataFrame으로 변환하기\nfacilities = gpd.GeoDataFrame(facilities_df, geometry = gpd.points_from_xy(facilities_df['Longitude'], facilities_df['Latitude']))\n\n# 좌표 참조 시스템 (CRS) EPSG 4326으로 설정\nfacilities.crs = {'init': 'epsg:4326'}\n\n# GeoDataFrame의 첫 5개 행 출력\nfacilities.head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      District\n      FacilityName\n      Type\n      Town\n      Ownership\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      Offinso North\n      A.M.E Zion Clinic\n      Clinic\n      Afrancho\n      CHAG\n      7.40801\n      -1.96317\n      POINT (-1.96317 7.40801)\n    \n    \n      1\n      Ashanti\n      Bekwai Municipal\n      Abenkyiman Clinic\n      Clinic\n      Anwiankwanta\n      Private\n      6.46312\n      -1.58592\n      POINT (-1.58592 6.46312)\n    \n    \n      2\n      Ashanti\n      Adansi North\n      Aboabo Health Centre\n      Health Centre\n      Aboabo No 2\n      Government\n      6.22393\n      -1.34982\n      POINT (-1.34982 6.22393)\n    \n    \n      3\n      Ashanti\n      Afigya-Kwabre\n      Aboabogya Health Centre\n      Health Centre\n      Aboabogya\n      Government\n      6.84177\n      -1.61098\n      POINT (-1.61098 6.84177)\n    \n    \n      4\n      Ashanti\n      Kwabre\n      Aboaso Health Centre\n      Health Centre\n      Aboaso\n      Government\n      6.84177\n      -1.61098\n      POINT (-1.61098 6.84177)\n    \n  \n\n\n\n\n위의 셀 CSV 파일에서 GeoDataFrame을 만들려면 Pandas와 GeoPandas를 모두 사용해야했습니다.\n\n먼저 위도 및 경도 좌표가 포함된 열을 포함하는 DataFrame 만듭니다.\n이를 GeoDataFrame으로 변환하기 위해 gpd.GeoDataFrame()을 사용합니다.\ngpd.points_from_xy() 함수는 위도 및 경도 열에서 Point 개체를 생성합니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#재투영-re-projecting",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#재투영-re-projecting",
    "title": "Geospatial Analysis",
    "section": "재투영 (Re-projecting)",
    "text": "재투영 (Re-projecting)\n재투영은 CRS를 변경하는 과정을 말합니다. 이 작업은 GeoPandas에서 to_crs() 함수를 사용하여 수행됩니다.\n여러 개의 GeoDataFrame을 plot할 때 모두 동일한 CRS를 사용하는 것이 중요합니다. 아래 셀에서는 facilities GeoDataFrame의 CRS를 regions의 CRS와 일치하도록 변경한 후 plot합니다.\n\n\nCode\n# 지도 만들기\nax = regions.plot(figsize = (8, 8), color = 'whitesmoke', linestyle = ':', edgecolor = 'black')\nfacilities.to_crs(epsg = 32630).plot(markersize = 1, ax = ax)\n\n\n<Axes: >\n\n\n\n\n\nto_crs() 함수는 geometry 열만 수정하고 다른 모든 열은 그대로 유지합니다.\n\n\nCode\n# `Latitude`와 `Longitude` 열은 변경 X\nfacilities.to_crs(epsg = 32630).head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      District\n      FacilityName\n      Type\n      Town\n      Ownership\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      Offinso North\n      A.M.E Zion Clinic\n      Clinic\n      Afrancho\n      CHAG\n      7.40801\n      -1.96317\n      POINT (614422.662 818986.851)\n    \n    \n      1\n      Ashanti\n      Bekwai Municipal\n      Abenkyiman Clinic\n      Clinic\n      Anwiankwanta\n      Private\n      6.46312\n      -1.58592\n      POINT (656373.863 714616.547)\n    \n    \n      2\n      Ashanti\n      Adansi North\n      Aboabo Health Centre\n      Health Centre\n      Aboabo No 2\n      Government\n      6.22393\n      -1.34982\n      POINT (682573.395 688243.477)\n    \n    \n      3\n      Ashanti\n      Afigya-Kwabre\n      Aboabogya Health Centre\n      Health Centre\n      Aboabogya\n      Government\n      6.84177\n      -1.61098\n      POINT (653484.490 756478.812)\n    \n    \n      4\n      Ashanti\n      Kwabre\n      Aboaso Health Centre\n      Health Centre\n      Aboaso\n      Government\n      6.84177\n      -1.61098\n      POINT (653484.490 756478.812)\n    \n  \n\n\n\n\nGeoPandas에서 EPSG 코드를 사용할 수 없는 경우, CRS의 ’proj4 string’을 사용하여 CRS를 변경할 수 있습니다.\n예를 들어, 위도/경도 좌표로 변환하는 proj4 string은 다음과 같습니다.\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nCode\n# CRS를 EPSG 4326로 변경\nregions.to_crs('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs').head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      POLYGON ((-1.30985 7.62302, -1.30786 7.62198, ...\n    \n    \n      1\n      Brong Ahafo\n      POLYGON ((-2.54567 8.76089, -2.54473 8.76071, ...\n    \n    \n      2\n      Central\n      POLYGON ((-2.06723 6.29473, -2.06658 6.29420, ...\n    \n    \n      3\n      Eastern\n      POLYGON ((-0.21751 7.21009, -0.21747 7.20993, ...\n    \n    \n      4\n      Greater Accra\n      POLYGON ((0.23456 6.10986, 0.23484 6.10974, 0...."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#기하학적-개체의-속성",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#기하학적-개체의-속성",
    "title": "Geospatial Analysis",
    "section": "기하학적 개체의 속성",
    "text": "기하학적 개체의 속성\n첫번째 튜토리얼에서 배운 것처럼, 임의의 GeoDataFrame의 경우 geometry 열의 유형은 표시하려는 대상에 따라 달라집니다.\n\n지진의 진원지를 나타내는 Point\n거리의 경우 Street\n국가 경계를 표시하는 Polygon\n\n세 가지 유형의 기하학적 객체 모두 데이터 집합을 빠르게 분석하는데 사용할 수 있는 기본 속성이 있습니다.\n예를 들어, x와 y 속성에서 각 Point의 x좌표와 y좌표를 얻을 수 있습니다.\n\n\nCode\n# 각 Point에서 x좌표\nfacilities['geometry'].head().x\n\n\n0   -1.96317\n1   -1.58592\n2   -1.34982\n3   -1.61098\n4   -1.61098\ndtype: float64\n\n\n그리고, length 속성에서 LineString의 길이를 얻을 수 있습니다.\n또는, area 속성에서 Polygon의 면적을 구할 수 있습니다.\n\n\nCode\n# GeoDataFrame에서 각 Polygon의 면적(km²) 계산\nregions.loc[:, 'AREA'] = regions['geometry'].area / 10**6\n\nprint('Area of Ghana : {} km²'.format(regions['AREA'].sum()))\nprint('CRS : ', regions.crs)\nregions.head()\n\n\nArea of Ghana : 239584.5760055668 km²\nCRS :  epsg:32630\n\n\n\n\n\n\n  \n    \n      \n      Region\n      geometry\n      AREA\n    \n  \n  \n    \n      0\n      Ashanti\n      POLYGON ((686446.075 842986.894, 686666.193 84...\n      24379.017777\n    \n    \n      1\n      Brong Ahafo\n      POLYGON ((549970.457 968447.094, 550073.003 96...\n      40098.168231\n    \n    \n      2\n      Central\n      POLYGON ((603176.584 695877.238, 603248.424 69...\n      9665.626760\n    \n    \n      3\n      Eastern\n      POLYGON ((807307.254 797910.553, 807311.908 79...\n      18987.625847\n    \n    \n      4\n      Greater Accra\n      POLYGON ((858081.638 676424.913, 858113.115 67...\n      3706.511145\n    \n  \n\n\n\n\n위의 셀에서, regions GeoDataFrame의 CRS가 EPSG 32630(메르카토르 투영법)으로 설정되어 있기 때문에 면적 계산은 ’Africa Albers Equal Area Conic’과 같은 등면적 투영법을 사용한 경우보다 약간 덜 정확합니다.\n하지만, 이렇게 하면 가나의 면적은 약 239,584km²로 계산되며, 이는 정답과 크게 다르지 않습니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-1",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-1",
    "title": "Geospatial Analysis",
    "section": "실습",
    "text": "실습\nExercise2 : Coordinate Reference Systems를 배운 내용을 사용하여 남미로 이주하는 조류를 추적해보세요."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-2",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-2",
    "title": "Geospatial Analysis",
    "section": "소개",
    "text": "소개\n이 튜토리얼에서는 folium 패키지로 대화형 지도 (Interactive Maps) 를 만드는 방법을 배웁니다. 이 과정에서, 새로운 기술을 적용하여 보스턴 범죄 데이터를 시각화합니다.\n\n\nCode\n# MapBox API를 활용하여 지도 만들기\nmap = folium.Map(location=[36.3504119, 127.3845475], zoom_start=14,\n                 tiles='https://api.mapbox.com/styles/v1/jw0112/cl1yi61ue002z14ojne3qrry8/tiles/256/{z}/{x}/{y}@2x?access_token=pk.eyJ1IjoiancwMTEyIiwiYSI6ImNsaDhzcDhkMTAwcGwzY2xnenhscjZxaTMifQ.kyL3owKwnt5Ewy58TbmlGQ',\n                 attr='Mapbox Attribution')\nmap\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#첫번째-대화형-지도",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#첫번째-대화형-지도",
    "title": "Geospatial Analysis",
    "section": "첫번째 대화형 지도",
    "text": "첫번째 대화형 지도\nfolium.Map() 함수로 비교적 간단한 지도를 만들어보겠습니다.\n\n\nCode\n# 지도 생성\nm_1 = folium.Map(\n    location = [42.32, -71.0589],\n    tiles = 'openstreetmap',\n    zoom_start = 10)\n\n# 지도 표현\nm_1\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n 여러 인수들을 활용하여 지도의 모양을 커스터마이징할 수 있습니다.\n\nlocation은 지도의 초기 중심을 설정합니다. 여기서는 보스턴 시의 위도(42.32° N)와 경도(-71.0589° E)를 사용합니다.\ntiles은 지도의 스타일을 변경합니다. 여기서는 OpenStreetMap스타일을 사용합니다. 다른 옵션이 궁금하다면 이 링크에서 나열된 다른 옵션을 찾을 수 있습니다.\nzoom_start은 지도의 초기 줌 레벨을 설정하며, 값이 클 수록 지도가 더 가까이 확대됩니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#데이터",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#데이터",
    "title": "Geospatial Analysis",
    "section": "데이터",
    "text": "데이터\n이제, 지도에 범죄 데이터를 추가하겠습니다!\n여기서는 데이터 불러오는 것에 집중하지 않고 이미 crimes라는 Pandas DataFrame을 이미 가지고 있다고 생각하고, DataFrame의 처음 5개의 행을 보겠습니다.\n\n\nCode\n# 데이터 불러오기\ncrimes = pd.read_csv('./geospatial-learn-course-data/crimes-in-boston/crimes-in-boston/crime.csv', encoding = 'latin-1')\n\n# 결측치가 존재하는 행 제거\ncrimes.dropna(subset = ['Lat', 'Long', 'DISTRICT'], inplace = True)\n\n# 2018년의 주요 범죄만 추출\ncrimes = crimes[crimes['OFFENSE_CODE_GROUP'].isin([\n    'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle',\n    'Residential Burglary', 'Simple Assault', 'Harassment', 'Ballistics',\n    'Aggravated Assault', 'Other Burglary', 'Arson', 'Commercial Burglary',\n    'HOME INVASION', 'Homicide', 'Criminal Harassment', 'Manslaughter'])]\ncrimes = crimes[crimes['YEAR'] >= 2018]\n\n# 처음 5개 행 출력\ncrimes.head()\n\n\n\n\n\n\n  \n    \n      \n      INCIDENT_NUMBER\n      OFFENSE_CODE\n      OFFENSE_CODE_GROUP\n      OFFENSE_DESCRIPTION\n      DISTRICT\n      REPORTING_AREA\n      SHOOTING\n      OCCURRED_ON_DATE\n      YEAR\n      MONTH\n      DAY_OF_WEEK\n      HOUR\n      UCR_PART\n      STREET\n      Lat\n      Long\n      Location\n    \n  \n  \n    \n      0\n      I182070945\n      619\n      Larceny\n      LARCENY ALL OTHERS\n      D14\n      808\n      NaN\n      2018-09-02 13:00:00\n      2018\n      9\n      Sunday\n      13\n      Part One\n      LINCOLN ST\n      42.357791\n      -71.139371\n      (42.35779134, -71.13937053)\n    \n    \n      6\n      I182070933\n      724\n      Auto Theft\n      AUTO THEFT\n      B2\n      330\n      NaN\n      2018-09-03 21:25:00\n      2018\n      9\n      Monday\n      21\n      Part One\n      NORMANDY ST\n      42.306072\n      -71.082733\n      (42.30607218, -71.08273260)\n    \n    \n      8\n      I182070931\n      301\n      Robbery\n      ROBBERY - STREET\n      C6\n      177\n      NaN\n      2018-09-03 20:48:00\n      2018\n      9\n      Monday\n      20\n      Part One\n      MASSACHUSETTS AVE\n      42.331521\n      -71.070853\n      (42.33152148, -71.07085307)\n    \n    \n      19\n      I182070915\n      614\n      Larceny From Motor Vehicle\n      LARCENY THEFT FROM MV - NON-ACCESSORY\n      B2\n      181\n      NaN\n      2018-09-02 18:00:00\n      2018\n      9\n      Sunday\n      18\n      Part One\n      SHIRLEY ST\n      42.325695\n      -71.068168\n      (42.32569490, -71.06816778)\n    \n    \n      24\n      I182070908\n      522\n      Residential Burglary\n      BURGLARY - RESIDENTIAL - NO FORCE\n      B2\n      911\n      NaN\n      2018-09-03 18:38:00\n      2018\n      9\n      Monday\n      18\n      Part One\n      ANNUNCIATION RD\n      42.335062\n      -71.093168\n      (42.33506218, -71.09316781)"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#plotting-points-설정",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#plotting-points-설정",
    "title": "Geospatial Analysis",
    "section": "Plotting Points 설정",
    "text": "Plotting Points 설정\n지도에 넣어야 하는 데이터의 양을 줄이기 위해 (일시적으로) 낮 시간대 강도 사건으로만 한정하겠습니다.\n\n\nCode\ndaytime_robberies = crimes[((crimes['OFFENSE_CODE_GROUP'] == 'Robbery') & (crimes['HOUR'].isin(range(9, 18))))]\ndaytime_robberies.head(2)\n\n\n\n\n\n\n  \n    \n      \n      INCIDENT_NUMBER\n      OFFENSE_CODE\n      OFFENSE_CODE_GROUP\n      OFFENSE_DESCRIPTION\n      DISTRICT\n      REPORTING_AREA\n      SHOOTING\n      OCCURRED_ON_DATE\n      YEAR\n      MONTH\n      DAY_OF_WEEK\n      HOUR\n      UCR_PART\n      STREET\n      Lat\n      Long\n      Location\n    \n  \n  \n    \n      299\n      I182070598\n      311\n      Robbery\n      ROBBERY - COMMERCIAL\n      A15\n      60\n      NaN\n      2018-09-02 17:15:00\n      2018\n      9\n      Sunday\n      17\n      Part One\n      RUTHERFORD AVE\n      42.371673\n      -71.063264\n      (42.37167264, -71.06326413)\n    \n    \n      527\n      I182070342\n      381\n      Robbery\n      ROBBERY - CAR JACKING\n      E18\n      490\n      NaN\n      2018-09-01 17:05:00\n      2018\n      9\n      Saturday\n      17\n      Part One\n      CUMMINS HWY\n      42.276453\n      -71.112980\n      (42.27645319, -71.11297971)\n    \n  \n\n\n\n\n\n\nfolium.Marker()\nfolium.Marker() 함수로 지도에 Marker를 추가합니다. 아래의 각 Marker는 서로 다른 강도에 해당합니다.\n\n\nCode\n# 지도 생성\nm_2 = folium.Map(\n    location = [42.32, -71.0589], \n    tiles = 'cartodbpositron', \n    zoom_start = 13)\n\n# 지도 + Marker\nfor idx, row in daytime_robberies.iterrows() :\n    Marker([row['Lat'], row['Long']]).add_to(m_2)\n\n# 지도 표현\nm_2\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\nfolium.plugins.MarkerCluster()\nfolium.plugins.MarkerCluster()를 사용하면 맵을 깔끔하게 정리할 수 있습니다. 각 Marker는 MarkerCluster 객체에 추가됩니다.\n\n\nCode\n# 지도 생성\nm_3 = folium.Map(\n    location = [42.32, -71.0589], \n    tiles = 'cartodbpositron', \n    zoom_start = 13)\n\n# 지도 + MarkerCluster\nmc = MarkerCluster()\nfor idx, row in daytime_robberies.iterrows() :\n    if not math.isnan(row['Long']) and not math.isnan(row['Lat']):\n        mc.add_child(Marker([row['Lat'], row['Long']]))\nm_3.add_child(mc)\n\n# Display the map\nm_3\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#버블맵-bubble-maps",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#버블맵-bubble-maps",
    "title": "Geospatial Analysis",
    "section": "버블맵 (Bubble Maps)",
    "text": "버블맵 (Bubble Maps)\n버블 맵는 Marker 대신 원을 사용합니다. 각 원의 크기와 색상을 변경하여 위치와 다른 두 변수 사이의 관계를 표시할 수도 있습니다.\nfolium.Circle()을 사용하여 원을 반복적으로 추가하여 버블 지도를 만듭니다.\n아래 셀에서 9~12시에 발생한 강도는 녹색으로 표시되고, 13~17시에 발생한 강도는 빨간색으로 표시됩니다.\n\n\nCode\n# 지도 생성\nm_4 = folium.Map(location = [42.32, -71.0589], tiles = 'cartodbpositron', zoom_start = 13)\n\ndef color_producer(val) :\n    if val <= 12 :\n        return 'forestgreen'\n    else :\n        return 'darkred'\n\n# 지도 + 버블맵\nfor i in range(0, len(daytime_robberies)) :\n    Circle(\n        location = [daytime_robberies.iloc[i]['Lat'],\n                    daytime_robberies.iloc[i]['Long']],\n        radius = 20,\n        color = color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)\n\n# 지도 표현\nm_4\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfolium.Circle()은 여러 인수를 받습니다.\n\nlocation은 원의 중심을 위도와 경도로 포함하는 목록입니다.\nradius는 원의 반지름을 설정합니다.\n\n기존 버블맵에서는 각 원의 반지름이 달라질 수 있습니다. 각 원의 색상을 변경하는데 사용되는 color_producer() 함수와 유사한 함수를 정의하여 이를 구현할 수 있습니다.\n\ncolor는 각 원의 색상을 설정합니다.\n\ncolor_producer() 함수는 강도의 위치에 대한 시간의 효과를 시각화하는데 사용됩니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#히트맵-heatmaps",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#히트맵-heatmaps",
    "title": "Geospatial Analysis",
    "section": "히트맵 (Heatmaps)",
    "text": "히트맵 (Heatmaps)\n히트맵을 만드려면 folium.plugins.HeatMap()을 사용합니다. 이는 도시 내 여러 지역의 범죄 밀도를 보여주며, 빨간색 영역은 상대적으로 범죄 발생이 더 많습니다.\n대도시에서 예상할 수 있듯이, 대부분의 범죄는 도심 근처에서 발생합니다.\n\n\nCode\n# 지도 생성\nm_5 = folium.Map(location = [42.32, -71.0589], tiles = 'cartodbpositron', zoom_start = 12)\n\n# 지도 + 히트맵\nHeatMap(data = crimes[['Lat', 'Long']], radius = 10).add_to(m_5)\n\n# 지도 표현\nm_5\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfolium.plugins.HeatMap()은 여러 인수를 받습니다.\n\ndata는 시각화하고자 하는 위치를 포함하는 DataFrame입니다.\nradius는 히트맵의 부드러움 정도를 조정합니다. 값이 클수록 히트맵의 부드러움 정도가 커집니다.(즉, 간격이 줄어듭니다.)"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#단계구분도-choropleth-maps",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#단계구분도-choropleth-maps",
    "title": "Geospatial Analysis",
    "section": "단계구분도 (Choropleth Maps)",
    "text": "단계구분도 (Choropleth Maps)\n경찰 관할 구역별로 범죄가 어떻게 다른지 이해하기 위해 단계구분도를 만들어 보겠습니다.\n첫번째 단계로, 각 구역에서 서로 다른 행이 할당되고 geometry 열에 지리적 경계가 포함된 GeoDataFrame을 만듭니다.\n\n\nCode\n# 보스턴 경찰 관할 구역 지리적 경계가 포함된 GeoDataFrame\ndistricts_full = gpd.read_file('./geospatial-learn-course-data/Police_Districts/Police_Districts/Police_Districts.shp')\ndistricts = districts_full[['DISTRICT', 'geometry']].set_index('DISTRICT')\ndistricts.head()\n\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      DISTRICT\n      \n    \n  \n  \n    \n      A15\n      MULTIPOLYGON (((-71.07416 42.39051, -71.07415 ...\n    \n    \n      A7\n      MULTIPOLYGON (((-70.99644 42.39557, -70.99644 ...\n    \n    \n      A1\n      POLYGON ((-71.05200 42.36884, -71.05169 42.368...\n    \n    \n      C6\n      POLYGON ((-71.04406 42.35403, -71.04412 42.353...\n    \n    \n      D4\n      POLYGON ((-71.07416 42.35724, -71.07359 42.357...\n    \n  \n\n\n\n\n각 구역별 범죄 발생 건수를 보여주는 plot_dict라는 Pandas Series를 만듭니다.\n\n\nCode\n# 각 구역별 범죄 발생 건수\nplot_dict = crimes['DISTRICT'].value_counts()\nplot_dict.head()\n\n\nD4     2885\nB2     2231\nA1     2130\nC11    1899\nB3     1421\nName: DISTRICT, dtype: int64\n\n\n지리적 경계를 적절한 색상과 일치시키는 방법을 알기 때문에, plot_dict과 districts가 동일한 DISTRICT라는 인덱스를 갖는 것이 매우 중요합니다.\nfolium.Choropleth() 함수를 사용하여 단계구분도를 만들 수 있습니다. 아래 지도가 렌더링되지 않는 경우 다른 웹 브라우저에서 페이지를 확인해보세요.\n\n\nCode\n# 지도 생성\nm_6 = folium.Map(location = [42.32, -71.0589], tiles = 'cartodbpositron', zoom_start = 12)\n\n# 지도 + 단계구분도\nChoropleth(geo_data = districts.__geo_interface__, \n           data = plot_dict, \n           key_on = 'feature.id', \n           fill_color = 'YlGnBu', \n           legend_name = 'Major criminal incidents (Jan-Aug 2018)'\n          ).add_to(m_6)\n\n# 지도 표현\nm_6\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfolium.Choropleth()는 여러 인수를 받습니다.\n\ngeo_data는 각 지리적 영역의 경계를 포함하는 GeoJSON FeatureCollection입니다.\n\n위 코드에서 districts GeoDataFrame을 __geo_interface__ 속성을 사용하여 GeoJSON FeatureCollection으로 변환합니다.\n\ndata는 각 지리적 영역의 색상을 지정하는데 사용할 값이 포함된 Pandas Series입니다.\nkey_on은 항상 feature.id로 설정됩니다.\n\n이는 geo_data에 사용되는 GeoDataFrame과 data에 사용되는 Pandas Series가 동일한 인덱스를 가지고 있다는 사실을 나타냅니다. 자세한 내용을 이해하려면 GeoJSON Feature Collection의 구조를 좀 더 자세히 살펴봐야합니다. (‘feature’ 키에 해당하는 값은 list이고, 각 항목은 ‘id’ 키가 포함된 dictonary입니다.)\n\nfill_color는 색상 스케일을 설정합니다.\nlegend_name은 맵 오른쪽 상단에 있는 범례에 레이블을 지정합니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-2",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-2",
    "title": "Geospatial Analysis",
    "section": "실습",
    "text": "실습\nExercise3 : Interactive Maps을 통해 일본에서 추가적으로 내진 보강이 필요한 지역을 확인해보세요."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-3",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-3",
    "title": "Geospatial Analysis",
    "section": "소개",
    "text": "소개\n이 튜토리얼에서는 지리공간 데이터에 대한 (지오코딩 (Geocoding) 과 테이블 조인 (Table Joins)) 두가지 조작에 대해 알아봅시다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#지오코딩-geocoding",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#지오코딩-geocoding",
    "title": "Geospatial Analysis",
    "section": "지오코딩 (Geocoding)",
    "text": "지오코딩 (Geocoding)\n지오코딩은 장소 이름이나 주소를 지도 상의 위치로 변환하는 과정입니다. 예를 들어, Google Maps, Bing Maps, Baidu Maps에서 랜드마크 설명을 기반으로 지리적 위치를 찾아본 적이 있다면 지오코딩을 사용해 본 적이 있을 것입니다!\n\n\n\n\n모든 지오코딩에는 geopy가 사용됩니다.\n\n\nCode\nfrom geopy.geocoders import Nominatim\n\n\n위의 셀에서 Nominatim은 위치를 생성하는데 사용되는 지오코딩 소프트웨어를 나타냅니다.\n지오코더를 인스턴트화하는 것으로 시작합니다. 그런 다음, 이름이나 주소를 Python 문자열로 적용하기만 하면 됩니다. (이 경우, 기자의 대피라미드라고 알려진 \"Pyramid of Khufu\"를 제공합니다.)\n지오코딩이 성공하면 두 가지 중요한 속성을 가진 geopy.location.Location 객체를 반환합니다.\n\npoint 속성에는 (위도, 경도) 위치가 포함합니다.\naddress 속성에는 전체 주소를 포함합니다.\n\n\n\nCode\ngeolocator = Nominatim(user_agent = 'kaggle_learn')\nlocation = geolocator.geocode('Pyramid of Khufu')\n\nprint(location.point)\nprint(location.address)\n\n\n29 58m 44.976s N, 31 8m 3.17625s E\nهرم خوفو, شارع ابو الهول السياحي, نزلة البطران, الجيزة, 12125, مصر\n\n\npoint 속성의 값은 geopy.point.Point 객체이며 각각 latitude와 longitude 속성에서 위도와 경도를 가져올 수 있습니다.\n\n\nCode\npoint = location.point\nprint('Latitude : ', point.latitude)\nprint('Longitude : ', point.longitude)\n\n\nLatitude :  29.97916\nLongitude :  31.134215625236113\n\n\n종종 다양한 주소를 지오코딩해야 하는 경우도 있습니다. 예를 들어, 유럽에 있는 상위 100개 대학의 위치를 얻고 싶다고 가정해 보겠습니다.\n\n\nCode\nuniversities = pd.read_csv('./geospatial-learn-course-data/top_universities.csv')\nuniversities.head()\n\n\n\n\n\n\n  \n    \n      \n      Name\n    \n  \n  \n    \n      0\n      University of Oxford\n    \n    \n      1\n      University of Cambridge\n    \n    \n      2\n      Imperial College London\n    \n    \n      3\n      ETH Zurich\n    \n    \n      4\n      UCL\n    \n  \n\n\n\n\n그런 다음 lambda 함수를 사용해서 DataFrame의 모든 행에 지오코딩 적용할 수 있습니다. (지오코딩이 실패할 경우를 대비하여 try / except 문을 사용합니다.)\n\n\nCode\ndef my_geocoder(row) :\n    try :\n        point = geolocator.geocode(row).point\n        return pd.Series({'Latitude' : point.latitude, 'Longitude' : point.longitude})\n    except :\n        return None\n\nuniversities[['Latitude', 'Longitude']] = universities.apply(lambda x : my_geocoder(x['Name']), axis = 1)\n\nprint('{}% of addresses were geocoded!'.format(\n    (1 - sum(np.isnan(universities['Latitude'])) / len(universities)) * 100))\n\n# 지오코딩 실패한 행 제거\nuniversities = universities.loc[~np.isnan(universities['Latitude'])]\nuniversities = gpd.GeoDataFrame(\n    universities, geometry = gpd.points_from_xy(universities.Longitude, universities.Latitude))\nuniversities.crs = {'init' : 'epsg:4326'}\nuniversities.head()\n\n\n91.0% of addresses were geocoded!\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      University of Oxford\n      51.759037\n      -1.252430\n      POINT (-1.25243 51.75904)\n    \n    \n      1\n      University of Cambridge\n      52.200623\n      0.110474\n      POINT (0.11047 52.20062)\n    \n    \n      2\n      Imperial College London\n      51.498959\n      -0.175641\n      POINT (-0.17564 51.49896)\n    \n    \n      3\n      ETH Zurich\n      47.562772\n      7.580947\n      POINT (7.58095 47.56277)\n    \n    \n      4\n      UCL\n      51.521785\n      -0.135151\n      POINT (-0.13515 51.52179)\n    \n  \n\n\n\n\n다음으로, 지오코더가 반환한 모든 위치를 시각화합니다. 몇몇 위치는 유럽에 있지 않기 때문에 확실히 부정확하다는 것을 알 수 있습니다!\n\n\nCode\n# 지도 생성\nm = folium.Map(location = [54, 15], tiles = 'openstreetmap', zoom_start = 2)\n\n# 지도 + Marker\nfor idx, row in universities.iterrows() :\n    Marker([row['Latitude'], row['Longitude']], popup = row['Name']).add_to(m)\n\n# 지도 표현\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#테이블-조인-table-joins",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#테이블-조인-table-joins",
    "title": "Geospatial Analysis",
    "section": "테이블 조인 (Table Joins)",
    "text": "테이블 조인 (Table Joins)\n이제 주제를 전환하여 서로 다른 원본의 데이터를 결합하는 방법에 대해 알아보겠습니다. \n\n특성 조인 (Attribute Join)\n이미 pd.DataFrame.join()을 사용하여 여러 DataFrame의 정보를 동일한 인덱스로 결합하는 방법을 알고 있습니다. 인덱스에서 일치하는 값을 단순화하여 데이터를 조인하는 방법을 특성 조인 (Attribute Join)이라고 합니다.\nGeoDataFrame의 특성 조인을 수행할 때는 gpd.GeoDataFrame.merge()을 사용하는 것이 가장 좋습니다. 이를 설명하기 위해, 유럽의 모든 국가에 대한 경계가 포함된 europe_boundaries GeoDataFrame으로 작업해보겠습니다.\n\n\nCode\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\neurope = world.loc[world.continent == 'Europe'].reset_index(drop = True)\n\neurope_stats = europe[['name', 'pop_est', 'gdp_md_est']]\neurope_boundaries = europe[['name', 'geometry']]\n\neurope_boundaries.head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      geometry\n    \n  \n  \n    \n      0\n      Russia\n      MULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n    \n    \n      1\n      Norway\n      MULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n    \n    \n      2\n      France\n      MULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n    \n    \n      3\n      Sweden\n      POLYGON ((11.02737 58.85615, 11.46827 59.43239...\n    \n    \n      4\n      Belarus\n      POLYGON ((28.17671 56.16913, 29.22951 55.91834...\n    \n  \n\n\n\n\n각 국가의 예상 인구와 국내총생산(GDP)을 포함하는 DataFrame europe_stats와 결합하겠습니다.\n\n\nCode\neurope_stats.head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      Russia\n      144373535.0\n      1699876\n    \n    \n      1\n      Norway\n      5347896.0\n      403336\n    \n    \n      2\n      France\n      67059887.0\n      2715518\n    \n    \n      3\n      Sweden\n      10285453.0\n      530883\n    \n    \n      4\n      Belarus\n      9466856.0\n      63080\n    \n  \n\n\n\n\n아래 셀에서 특성 조인을 수행합니다. on 인수는 europe_boundaries의 행을 europe_stats의 행과 일치시키는데 사용되는 열 이름으로 설정됩니다.\n\n\nCode\n# 특성 조인을 사용하여 유럽 국가에 대한 데이터 병합\neurope = europe_boundaries.merge(europe_stats, on = 'name')\neurope.head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      geometry\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      Russia\n      MULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n      144373535.0\n      1699876\n    \n    \n      1\n      Norway\n      MULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n      5347896.0\n      403336\n    \n    \n      2\n      France\n      MULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n      67059887.0\n      2715518\n    \n    \n      3\n      Sweden\n      POLYGON ((11.02737 58.85615, 11.46827 59.43239...\n      10285453.0\n      530883\n    \n    \n      4\n      Belarus\n      POLYGON ((28.17671 56.16913, 29.22951 55.91834...\n      9466856.0\n      63080\n    \n  \n\n\n\n\n\n\n공간 조인 (Spatial Join)\n또 다른 조인 유형은 공간 조인(Spatial Join)입니다. 공간 조인을 사용하면 geometry 열에 있는 개체 간의 공간 관계를 기반으로 GeoDataFrame을 결합합니다. 예를 들어, 유럽 대학의 지오코딩된 주소가 포함된 GeoDataFrame universities가 이미 있습니다.\n그런 다음 공간 조인을 사용하여 각 대학을 해당 국가에 일치시킬 수 있습니다. 이 작업은 gpd.sjoin()을 사용하여 수행합니다.\n\n\nCode\n# 공간 조인을 사용하여 대학을 유럽의 국가와 일치\neuropean_universities = gpd.sjoin(universities, europe)\n\n# 결과 검토\nprint('We located {} universities.'.format(len(universities)))\nprint('Only {} of the universities were located in Europe (in {} different countries).'.format(\n    len(european_universities), len(european_universities.name.unique())))\n\neuropean_universities.head()\n\n\nWe located 91 universities.\nOnly 86 of the universities were located in Europe (in 14 different countries).\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Latitude\n      Longitude\n      geometry\n      index_right\n      name\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      University of Oxford\n      51.759037\n      -1.252430\n      POINT (-1.25243 51.75904)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      1\n      University of Cambridge\n      52.200623\n      0.110474\n      POINT (0.11047 52.20062)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      2\n      Imperial College London\n      51.498959\n      -0.175641\n      POINT (-0.17564 51.49896)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      4\n      UCL\n      51.521785\n      -0.135151\n      POINT (-0.13515 51.52179)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      5\n      London School of Economics and Political Science\n      51.514211\n      -0.116808\n      POINT (-0.11681 51.51421)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n  \n\n\n\n\n 위의 공간 조인은 두 GeoDataFrame의 geometry 열을 살펴봅니다. universities GeoDataFrame의 Point 객체가 europe DataFrame의 Polygon 객체와 교차하는 경우, 해당 행이 결합되어 european_universities DataFrame의 단일 행으로 추가됩니다. 그렇지 않으면, 일치하는 대학이 없는 국가(및 일치하는 국가가 없는 대학)는 결과에서 제거됩니다.\ngpd.sjoin() 함수는 how 및 op 인수를 통해 다양한 조인 유형에 맞게 지정할 수 있습니다. 예를 들어, how = left(또는 how = right)를 설정하여 SQL Left (또는 Right) 조인과 동등한 작업을 수행할 수 있습니다. 이 강좌에서는 자세히 설명하지 않겠지만, 이 문서에서 자세한 내용을 확인할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-3",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-3",
    "title": "Geospatial Analysis",
    "section": "실습",
    "text": "실습\nExercise 4 : Manipulating Geospatial Data 에서 지오코딩 및 테이블 조인 사용을 사용하여 스타벅스 리저브 로스터리에 적합한 위치를 파악해봅시다."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-4",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#소개-4",
    "title": "Geospatial Analysis",
    "section": "소개",
    "text": "소개\n이 튜토리얼에서는 근접성 분석 을 위한 몇가지 기법을 살펴봅니다. 특히 다음과 같은 작업을 수행하는 방법을 배우게 됩니다.\n\n지도에서 포인트 사이의 거리를 측정\n특정 Feature의 반경 내에 있는 모든 포인트 선택\n\n미국 펜실베니아주 필라델피아의 독성 화학물질 배출을 추적하는 미국 환경보호청 (EPA)의 데이터를 사용하여 작업하겠습니다.\n\n\nCode\nreleases = gpd.read_file('./geospatial-learn-course-data/toxic_release_pennsylvania/toxic_release_pennsylvania/toxic_release_pennsylvania.shp') \nreleases.head()\n\n\n\n\n\n\n  \n    \n      \n      YEAR\n      CITY\n      COUNTY\n      ST\n      LATITUDE\n      LONGITUDE\n      CHEMICAL\n      UNIT_OF_ME\n      TOTAL_RELE\n      geometry\n    \n  \n  \n    \n      0\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      40.005901\n      -75.072103\n      FORMIC ACID\n      Pounds\n      0.160\n      POINT (2718560.227 256380.179)\n    \n    \n      1\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.920120\n      -75.146410\n      ETHYLENE GLYCOL\n      Pounds\n      13353.480\n      POINT (2698674.606 224522.905)\n    \n    \n      2\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      40.023880\n      -75.220450\n      CERTAIN GLYCOL ETHERS\n      Pounds\n      104.135\n      POINT (2676833.394 261701.856)\n    \n    \n      3\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.913540\n      -75.198890\n      LEAD COMPOUNDS\n      Pounds\n      1730.280\n      POINT (2684030.004 221697.388)\n    \n    \n      4\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.913540\n      -75.198890\n      BENZENE\n      Pounds\n      39863.290\n      POINT (2684030.004 221697.388)\n    \n  \n\n\n\n\n 또한, 같은 도시의 대기질 모니터링 스테이션에서 측정한 수치가 포함된 데이터로 작업하겠습니다.\n\n\nCode\nstations = gpd.read_file('./geospatial-learn-course-data/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations.shp')\nstations.head()\n\n\n\n\n\n\n  \n    \n      \n      SITE_NAME\n      ADDRESS\n      BLACK_CARB\n      ULTRAFINE_\n      CO\n      SO2\n      OZONE\n      NO2\n      NOY_NO\n      PM10\n      ...\n      PAMS_VOC\n      TSP_11101\n      TSP_METALS\n      TSP_LEAD\n      TOXICS_TO1\n      MET\n      COMMUNITY_\n      LATITUDE\n      LONGITUDE\n      geometry\n    \n  \n  \n    \n      0\n      LAB\n      1501 East Lycoming Avenue\n      N\n      N\n      Y\n      N\n      Y\n      Y\n      Y\n      N\n      ...\n      Y\n      N\n      Y\n      N\n      y\n      N\n      N\n      40.008606\n      -75.097624\n      POINT (2711384.641 257149.310)\n    \n    \n      1\n      ROX\n      Eva and Dearnley Streets\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      ...\n      N\n      N\n      Y\n      N\n      Y\n      N\n      N\n      40.050461\n      -75.236966\n      POINT (2671934.290 271248.900)\n    \n    \n      2\n      NEA\n      Grant Avenue and Ashton Street\n      N\n      N\n      N\n      N\n      Y\n      N\n      N\n      N\n      ...\n      N\n      N\n      N\n      N\n      N\n      Y\n      N\n      40.072073\n      -75.013128\n      POINT (2734326.638 280980.247)\n    \n    \n      3\n      CHS\n      500 South Broad Street\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      ...\n      N\n      N\n      Y\n      N\n      Y\n      N\n      N\n      39.944510\n      -75.165442\n      POINT (2693078.580 233247.101)\n    \n    \n      4\n      NEW\n      2861 Lewis Street\n      N\n      N\n      Y\n      Y\n      Y\n      N\n      Y\n      Y\n      ...\n      N\n      Y\n      N\n      Y\n      N\n      Y\n      N\n      39.991688\n      -75.080378\n      POINT (2716399.773 251134.976)\n    \n  \n\n5 rows × 24 columns"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#거리-측정",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#거리-측정",
    "title": "Geospatial Analysis",
    "section": "거리 측정",
    "text": "거리 측정\n서로 다른 두 GeoDataFrame의 Point 간의 거리를 측정하려면, 먼저 두 GeoDataFrame이 동일한 좌표 참조 시스템(CRS)을 사용하는지 확인해야 합니다.\n다행히도 여기서는 둘 다 EPSG:2272를 사용합니다.\n\n\nCode\nprint(stations.crs)\nprint(releases.crs)\n\n\nepsg:2272\nepsg:2272\n\n\n또한, CRS가 어떤 단위(meter, feet 또는 다른 단위)를 사용하는지 확인합니다. 이 경우 EPSG:2272는 feet 단위를 사용합니다. (원한다면 여기에서 확인할 수 있습니다.)\nGeoPandas에서 거리를 계산하는 것은 비교적 간단합니다. 아래 셀은 recent_release의 비교적 최근 릴리즈 사건과 stations GeoDataFrame의 모든 스테이션 사이의 거리(피트)를 계산합니다.\n\n\nCode\n# 특정 릴리즈 사건 1개 선택\nrecent_release = releases.iloc[360]\n\n# 릴리즈에서 각 스테이션까지의 거리 측정\ndistances = stations['geometry'].distance(recent_release['geometry'])\ndistances\n\n\n0     44778.509761\n1     51006.456589\n2     77744.509207\n3     14672.170878\n4     43753.554393\n5      4711.658655\n6     23197.430858\n7     12072.823097\n8     79081.825506\n9      3780.623591\n10    27577.474903\n11    19818.381002\ndtype: float64\n\n\n계산된 거리를 사용하여, 각 스테이션까지의 평균 거리와 같은 통계를 얻을 수 있습니다.\n\n\nCode\nprint('Mean distance to monitoring stations: {} feet'.format(distances.mean()))\n\n\nMean distance to monitoring stations: 33516.28487007786 feet\n\n\n또한, 가장 가까운 모니터링 스테이션을 찾을 수도 있습니다.\n\n\nCode\nprint('Closest monitoring station ({} feet):'.format(distances.min()))\nprint(stations.iloc[distances.idxmin()][['ADDRESS', 'LATITUDE', 'LONGITUDE']])\n\n\nClosest monitoring station (3780.623590556444 feet):\nADDRESS      3100 Penrose Ferry Road\nLATITUDE                    39.91279\nLONGITUDE                 -75.185448\nName: 9, dtype: object"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#버퍼-생성",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#버퍼-생성",
    "title": "Geospatial Analysis",
    "section": "버퍼 생성",
    "text": "버퍼 생성\n지도에서 한 점으로부터 일정 반경 떨어진 모든 점을 이해하려면 버퍼를 만드는 것이 가장 간단한 방법입니다.\n아레 셀은 12개의 서로 다른 Polygon 객체를 포함하는 GeoSeries two_mile_buffer를 만듭니다. 각 다각형은 서로 다른 대기 모니터링 스테이션 주변의 2 miles(또는, 2*5280 feet)의 버퍼입니다.\n\n\nCode\ntwo_mile_buffer = stations['geometry'].buffer(2*5280)\ntwo_mile_buffer.head()\n\n\n0    POLYGON ((2721944.641 257149.310, 2721893.792 ...\n1    POLYGON ((2682494.290 271248.900, 2682443.441 ...\n2    POLYGON ((2744886.638 280980.247, 2744835.789 ...\n3    POLYGON ((2703638.580 233247.101, 2703587.731 ...\n4    POLYGON ((2726959.773 251134.976, 2726908.924 ...\ndtype: geometry\n\n\n각 Polygon을 지도에 그리기 위해 folium.GeoJson()을 사용합니다. folium에는 위도와 경도의 좌표가 필요하므로 시각화 전에 CRS를 EPSG:4326으로 변환해야합니다.\n\n\nCode\n# 최근 릴리즈 사건과 모니터링 스테이션을 포함하여 지도를 생성\nm = folium.Map(location = [39.9526, -75.1652], zoom_start = 11)\nHeatMap(data = releases[['LATITUDE', 'LONGITUDE']], radius = 15).add_to(m)\nfor idx, row in stations.iterrows() :\n    Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)\n    \n# 지도 + 버퍼\nGeoJson(two_mile_buffer.to_crs(epsg = 4326)).add_to(m)\n\n# 지도 표현\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n이제 모든 모니터링 스테이션에서 2마일 이내에 독성 물질 방출이 발생했는지 테스트하려면 각 Polygon에 대해 12가지 테스트를 실행하여 해당 지점이 포함되어 있는지 개별적으로 확인할 수 있습니다.\n하지만 더 효율적인 방법은 먼저 모든 Polygon을 MultiPolygon 객체로 축소하는 것입니다. 이 작업은 unary_union 속성을 사용하여 수행합니다.\n\n\nCode\n# Polygon 그룹을 단일 MultiPolygon 객체로 축소\nmy_union = two_mile_buffer.geometry.unary_union\nprint('Type : ', type(my_union))\n\n# MultiPolygon 객체 시각화\nmy_union\n\n\nType :  <class 'shapely.geometry.multipolygon.MultiPolygon'>\n\n\n\n\n\nMultiPolygon에 Point가 포함되어 있는지 확인하기 위해 contains() 함수를 사용합니다. 튜토리얼 앞부분의 릴리즈 사건을 사용하겠습니다.\n가장 가까운 모니터링 스테이션까지의 거리가 약 3,781피트라는 것을 알고 있습니다.\n하지만 모든 릴리즈가 대기 모니터링 스테이션에서 2마일 이내에 발생한 것은 아닙니다!\n\n\nCode\n# 가장 가까운 스테이션이 2마일 미만인 경우\nprint(my_union.contains(releases.iloc[360].geometry))\n\n# 가장 가까운 스테이션이 2마일 이상 떨어진 경우\nprint(my_union.contains(releases.iloc[358].geometry))\n\n\nTrue\nFalse"
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-4",
    "href": "Data_Mining/Geospatial_Analysis/Geospatial_Analysis.html#실습-4",
    "title": "Geospatial Analysis",
    "section": "실습",
    "text": "실습\nExercise 5 : Proximity Analysis 에서 뉴욕 시의 병원 서비스 적용 범위를 알아보세요."
  },
  {
    "objectID": "Data_Mining/Geospatial_Analysis/Pydeck.html",
    "href": "Data_Mining/Geospatial_Analysis/Pydeck.html",
    "title": "Pydeck 기본",
    "section": "",
    "text": "Pydeck 실습\n\n\nInstall\n\npydeck 설치 가이드 - https://deckgl.readthedocs.io/en/latest/installation.html\n\n강의자료 출처 - https://zzsza.github.io/data/2019/11/24/pydeck/\n현재 pydeck은 3.7-3.9 까지만 호환\n\n\n\nCode\n# %pip install pydeck\n# %pip install pydeck[jupyter]\n\n\n\n\nCode\n!jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck\n!jupyter nbextension enable --sys-prefix --py pydeck\n\n\nInstalling C:\\Users\\user\\anaconda3\\envs\\quartoenv\\lib\\site-packages\\pydeck\\nbextension/static -> pydeck\nRemoving: C:\\Users\\user\\anaconda3\\envs\\quartoenv\\share\\jupyter\\nbextensions\\pydeck\nSymlinking: C:\\Users\\user\\anaconda3\\envs\\quartoenv\\share\\jupyter\\nbextensions\\pydeck -> C:\\Users\\user\\anaconda3\\envs\\quartoenv\\lib\\site-packages\\pydeck\\nbextension\\static\n- Validating: ok\n\n    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n    \n          jupyter nbextension enable pydeck --py --sys-prefix\n    \nEnabling notebook extension pydeck/extensionRequires...\n      - Validating: ok\n\n\n\n\nCode\n# MAPBOX_API_KEY=\"pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g\"\n\n\n\n\n공식 홈페이지 예시\n\n\nCode\nimport pandas as pd\nimport pydeck\n\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\npd.read_csv(UK_ACCIDENTS_DATA).head()\n\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\n\nCode\nlayer = pydeck.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng,lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,                 \n    coverage=1,\n    radius=1000)\n\n# Set the viewport location\nview_state = pydeck.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pydeck.Deck(layers=[layer], initial_view_state=view_state)\nr.show()\n# r.to_html('demo.html')\n\n\n\n\n\n\n\nCode\nlayer.elevation_range = [0, 500]\n\nr.update()\n\n\n\n\nCode\nimport pydeck as pdk\npdk.Deck?\n\n\n\n\nScatter Plots\n\n\nCode\nimport pandas as pd\nfrom pydeck import (\n    data_utils,\n    Deck,\n    Layer\n)\n\n# First, let's use Pandas to download our data\nURL = 'https://raw.githubusercontent.com/ajduberstein/data_sets/master/beijing_subway_station.csv'\ndf = pd.read_csv(URL)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\n\nCode\nfrom ast import literal_eval\n# We have to re-code position to be one field in a list, so we'll do that here:\n# The CSV encodes the [R, G, B, A] color values listed in it as a string\ndf['color'] = df.apply(lambda x: literal_eval(x['color']), axis=1)\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\n\nCode\n# Use pydeck's data_utils module to fit a viewport to the central 90% of the data\nviewport = data_utils.compute_view(points=df[['lng', 'lat']], view_proportion=0.9)\nauto_zoom_map = Deck(layers=None, initial_view_state=viewport)\nauto_zoom_map.show()\n\n# auto_zoom_map.to_html('demo.html')\n\n\n\n\n\n\n\nCode\nfrom IPython.core.display import display\nimport ipywidgets\n\nyear = 2019\n\nscatterplot = Layer(\n    'ScatterplotLayer',\n    df,\n    id='scatterplot-layer',\n    get_radius=500,\n    get_fill_color='color',\n    get_position='[lng, lat]')\nr = Deck(layers=[scatterplot], initial_view_state=viewport)\n\n# Create an HTML header to display the year\ndisplay_el = ipywidgets.HTML('<h1>{}</h1>'.format(year))\ndisplay(display_el)\n# Show the current visualization\nr.show()\n# r.to_html('demo.html')\n\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17440\\1358459236.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display\n\n\n\n\n\n\n\n\n\n\nCode\nimport time\nfor y in range(1971, 2020):\n    scatterplot.data = df[df['opening_date'] <= str(y)]\n    year = y\n    # Reset the header to display the year\n    display_el.value = '<h1>{}</h1>'.format(year)\n    r.update()\n    time.sleep(0.1)\n\n\n\n\nUsing pydeck to manipulate data\n\n\nCode\nimport pydeck as pdk\n\nDATA_URL = 'https://api.data.gov.sg/v1/transport/taxi-availability'\nCOLOR_RANGE = [\n  [255, 255, 178, 25],\n  [254, 217, 118, 85],\n  [254, 178, 76, 127],\n  [253, 141, 60, 170],\n  [240, 59, 32, 212],\n  [189, 0, 38, 255]\n]\n\n\n\n\nCode\nimport pandas as pd\nimport requests\n\njson = requests.get(DATA_URL).json()\ndf = pd.DataFrame(json[\"features\"][0][\"geometry\"][\"coordinates\"])\ndf.columns = ['lng', 'lat']\n\nviewport = pdk.data_utils.compute_view(df[['lng', 'lat']])\nlayer = pdk.Layer(\n    'ScreenGridLayer',\n    df,\n    cell_size_pixels=20,\n    color_range=COLOR_RANGE,\n    get_position='[lng, lat]',\n    pickable=True,\n    auto_highlight=True)\nr = pdk.Deck(layers=[layer], initial_view_state=viewport)\n\n\n\n\nCode\nr.show()\n\n\n\n\n\n\n\nCode\npd.DataFrame([r.deck_widget.selected_data])\n\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n    \n      0\n    \n  \n\n\n\n\n\n\nPlotting massive data sets.ipynb\n\n\nCode\nimport pandas as pd\nall_lidar = pd.concat([\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_1.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_2.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_3.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_4.csv'),\n])\n\n# Filter to one frame of data\nlidar = all_lidar[all_lidar['source'] == 136]\nlidar.loc[: , ['x', 'y']] = lidar[['x', 'y']] / 10000\n\n\n\n\nCode\nimport pydeck as pdk\n\n\npoint_cloud = pdk.Layer(\n    'PointCloudLayer',\n    lidar[['x', 'y', 'z']],\n    get_position='[x, y, z * 10]',\n    get_normal=[0, 0, 1],\n    get_color=[255, 0, 100, 200],\n    pickable=True,  \n    auto_highlight=True,\n    point_size=1)\n\n\nview_state = pdk.data_utils.compute_view(lidar[['x', 'y']], 0.9)\nview_state.max_pitch = 360\nview_state.pitch = 80\nview_state.bearing = 120\n\nr = pdk.Deck(\n    point_cloud,\n    initial_view_state=view_state,\n    map_style='')\nr.show()\n\n\n\n\n\n\n\nCode\nimport time\nfrom collections import deque\n\n# Choose a handful of frames to loop through\nframe_buffer = deque([42, 56, 81, 95])\nprint('Press the stop icon to exit')\nwhile True:\n    current_frame = frame_buffer[0]\n    lidar = all_lidar[all_lidar['source'] == current_frame]\n    r.layers[0].get_position = '[x / 10000, y / 10000, z * 10]'\n    r.layers[0].data = lidar.to_dict(orient='records')\n    frame_buffer.rotate()\n    r.update()\n    time.sleep(0.5)\n\n\nPress the stop icon to exit\n\n\n\n\nInteracting with other Jupyter widgets.ipynb\n\n\nCode\nimport pandas as pd\nimport pydeck as pdk\n\nLIGHTS_URL = 'https://raw.githubusercontent.com/ajduberstein/lights_at_night/master/chengdu_lights_at_night.csv'\ndf = pd.read_csv(LIGHTS_URL)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n    \n  \n  \n    \n      0\n      1993\n      104.575\n      31.808\n      4\n    \n    \n      1\n      1993\n      104.583\n      31.808\n      4\n    \n    \n      2\n      1993\n      104.592\n      31.808\n      4\n    \n    \n      3\n      1993\n      104.600\n      31.808\n      4\n    \n    \n      4\n      1993\n      104.675\n      31.808\n      4\n    \n  \n\n\n\n\n\n\nCode\ndf['color'] = df['brightness'].apply(lambda val: [255, val * 4,  255, 255])\ndf.sample(10)\n\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n      color\n    \n  \n  \n    \n      35303\n      1997\n      103.883\n      29.550\n      4\n      [255, 16, 255, 255]\n    \n    \n      75226\n      2009\n      103.992\n      30.367\n      6\n      [255, 24, 255, 255]\n    \n    \n      24019\n      1997\n      103.775\n      30.800\n      4\n      [255, 16, 255, 255]\n    \n    \n      60366\n      2009\n      104.017\n      31.267\n      5\n      [255, 20, 255, 255]\n    \n    \n      84381\n      2009\n      104.983\n      29.575\n      8\n      [255, 32, 255, 255]\n    \n    \n      222765\n      2011\n      104.450\n      31.375\n      6\n      [255, 24, 255, 255]\n    \n    \n      64172\n      2009\n      104.650\n      31.000\n      7\n      [255, 28, 255, 255]\n    \n    \n      94879\n      2001\n      104.858\n      31.025\n      5\n      [255, 20, 255, 255]\n    \n    \n      64132\n      2009\n      104.133\n      31.000\n      8\n      [255, 32, 255, 255]\n    \n    \n      9067\n      1993\n      103.633\n      30.600\n      4\n      [255, 16, 255, 255]\n    \n  \n\n\n\n\n\n\nCode\nplottable = df[df['year'] == 1993].to_dict(orient='records')\n\nview_state = pdk.ViewState(\n    latitude=31.0,\n    longitude=104.5,\n    zoom=8,\n    max_zoom=8,\n    min_zoom=8)\nscatterplot = pdk.Layer(\n    'HexagonLayer', # HeatmapLayer\n    data=plottable,\n    get_position='[lng, lat]',\n    get_weight='brightness',\n    opacity=0.5,\n    pickable=False,\n    get_radius=800)\nr = pdk.Deck(\n    layers=[scatterplot],\n    initial_view_state=view_state,\n    views=[pdk.View(type='MapView', controller=None)])\nr.show()\n\n\n\n\n\n\n\nCode\nimport ipywidgets as widgets\nfrom IPython.display import display\nslider = widgets.IntSlider(1992, min=1993, max=2013, step=2)\ndef on_change(v):\n    results = df[df['year'] == slider.value].to_dict(orient='records')\n    scatterplot.data = results\n    r.update()\n    \nslider.observe(on_change, names='value')\ndisplay(slider)\n\n\n\n\n\n\n\nCode\ntooltip = {\n   \"html\": \"<b>Elevation Value:</b> {elevationValue}\",\n   \"style\": {\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\"\n   }\n}\n\n\n\n\nTooltip\n\n\nCode\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip={\n        'html': '<b>Elevation Value:</b> {elevationValue}',\n        'style': {\n            'color': 'white'\n        }\n    }\n)\nr.show()\n\n\n\n\n\n\n그냥 텍스트로 하기\n\n\n\nCode\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip = {\n    \"text\": \"Elevation: {elevationValue}\"\n    }\n)\nr.show()\n\n\n\n\n\n\n\nTooltip을 그냥 True값만 주기\n\n\n\nCode\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip=True\n)\nr.show()\n\n\n\n\n\n\n\nCode\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\nuk_data = pd.read_csv(UK_ACCIDENTS_DATA)\n\n\n\n\nCode\nuk_data.head()\n\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\n\n미국 택시 데이터 시각화\n\n\nCode\nimport pandas as pd\nimport pydata_google_auth\n\nSCOPES = [\n  'https://www.googleapis.com/auth/cloud-platform',\n  'https://www.googleapis.com/auth/drive',\n  'https://www.googleapis.com/auth/bigquery'\n]\n\ncredentials = pydata_google_auth.get_user_credentials(\nSCOPES, auth_local_webserver=True)\n\n\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=262006177488-3425ks60hkk80fssi9vpohv88g6q1iqd.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=pH3LUMkjRirs4N7GWMTcomXq5ircWi&access_type=offline\n\n\n\n\nCode\nquery = \"\"\"\nSELECT \n    *\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` \nWHERE EXTRACT(MONTH from pickup_datetime) = 1\nLIMIT 10000\n\"\"\"\n\n\n\n\nCode\n# %%time\ntaxi_df = pd.read_gbq(query=query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\n\nGenericGBQException: Reason: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/persuasive-zoo-147513/jobs?prettyPrint=false: Access Denied: Project persuasive-zoo-147513: User does not have bigquery.jobs.create permission in project persuasive-zoo-147513.\n\nLocation: None\nJob ID: 9c7a150b-0c58-4f25-ae01-5d317dabc780\n\n\n\n\nCode\ntaxi_df\n\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      dropoff_latitude\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n    \n  \n  \n    \n      0\n      1\n      2015-01-02 16:26:22\n      2015-01-02 16:51:10\n      2\n      2.50\n      -73.993172\n      40.762901\n      <NA>\n      N\n      -73.962097\n      40.763584\n      1\n      15.7\n      1.0\n      0.5\n      3.50\n      0.0\n      0.0\n      21.00\n    \n    \n      1\n      1\n      2015-01-16 17:13:00\n      2015-01-16 17:16:10\n      1\n      0.40\n      -73.961601\n      40.771229\n      <NA>\n      N\n      -73.959419\n      40.775253\n      1\n      4.0\n      1.0\n      0.5\n      1.15\n      0.0\n      0.3\n      6.95\n    \n    \n      2\n      2\n      2015-01-24 04:25:01\n      2015-01-24 04:41:43\n      2\n      4.64\n      -74.000595\n      40.737167\n      <NA>\n      N\n      -73.995499\n      40.680763\n      1\n      16.0\n      0.5\n      0.5\n      19.50\n      0.0\n      0.3\n      36.80\n    \n    \n      3\n      2\n      2015-01-30 14:29:58\n      2015-01-30 15:27:13\n      1\n      18.39\n      -73.989914\n      40.729706\n      <NA>\n      N\n      -73.782310\n      40.644180\n      1\n      52.0\n      0.0\n      0.5\n      5.50\n      0.0\n      0.3\n      58.30\n    \n    \n      4\n      1\n      2015-01-14 21:24:13\n      2015-01-14 21:25:55\n      1\n      0.50\n      -73.954849\n      40.773220\n      <NA>\n      N\n      -73.959801\n      40.769432\n      1\n      3.5\n      0.5\n      0.5\n      0.96\n      0.0\n      0.3\n      5.76\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      1\n      2015-01-24 22:49:17\n      2015-01-24 22:56:20\n      2\n      0.70\n      -74.004578\n      40.724056\n      <NA>\n      N\n      -74.006958\n      40.732971\n      1\n      6.0\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      9.10\n    \n    \n      9996\n      2\n      2015-01-21 18:20:27\n      2015-01-21 18:30:16\n      6\n      1.14\n      -74.000038\n      40.748291\n      <NA>\n      N\n      -73.990608\n      40.738071\n      1\n      8.0\n      1.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9997\n      2\n      2015-01-09 20:35:23\n      2015-01-09 20:43:55\n      1\n      2.00\n      -73.974876\n      40.748661\n      <NA>\n      N\n      -73.980530\n      40.768021\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9998\n      2\n      2015-01-31 22:01:33\n      2015-01-31 22:11:22\n      1\n      1.42\n      -73.984718\n      40.728447\n      <NA>\n      N\n      -73.975380\n      40.745564\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9999\n      2\n      2015-01-13 13:40:14\n      2015-01-13 13:51:28\n      5\n      1.57\n      -73.981232\n      40.747498\n      <NA>\n      N\n      -73.998116\n      40.733883\n      1\n      9.0\n      0.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n  \n\n10000 rows × 19 columns\n\n\n\n\n\nGridLayer\n\n10만개 데이터\n\n\n\nCode\narc_layer = pdk.Layer(\n    'GridLayer',\n    taxi_df,\n    get_position='[pickup_longitude, pickup_latitude]',\n    pickable=True, \n    auto_highlight=True,\n    tooltip=True\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\n\nArc Layer\n\n\nCode\nzip_code_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  *\nFROM base_data \nlimit 10000\n\"\"\"\n\n\n\n\nCode\n%%time\ntaxi_df_by_zipcode = pd.read_gbq(query=zip_code_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\n\nWall time: 36.5 s\n\n\n\n\nCode\ntaxi_df_by_zipcode.head(3)\n\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      ...\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n    \n  \n  \n    \n      0\n      2\n      2015-01-20 09:57:47\n      2015-01-20 10:14:24\n      1\n      2.54\n      -74.014793\n      40.714111\n      <NA>\n      N\n      -73.991997\n      ...\n      2.5\n      0.0\n      0.3\n      15.8\n      10280\n      40.709073\n      -74.016423\n      10003\n      40.731829\n      -73.989181\n    \n    \n      1\n      1\n      2015-01-15 04:50:46\n      2015-01-15 05:01:29\n      1\n      2.30\n      -74.015938\n      40.710976\n      <NA>\n      N\n      -73.996552\n      ...\n      0.0\n      0.0\n      0.3\n      11.3\n      10280\n      40.709073\n      -74.016423\n      10011\n      40.742043\n      -74.000620\n    \n    \n      2\n      2\n      2015-01-22 09:31:00\n      2015-01-22 09:50:42\n      1\n      4.13\n      -73.989738\n      40.701981\n      <NA>\n      N\n      -74.007828\n      ...\n      2.0\n      0.0\n      0.3\n      19.8\n      11201\n      40.693700\n      -73.989859\n      10011\n      40.742043\n      -74.000620\n    \n  \n\n3 rows × 25 columns\n\n\n\n\n\nCode\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    taxi_df_by_zipcode,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    get_widht='elevationValue',\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\n\nAggregate\n\n\nCode\nagg_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6\nlimit 10000\n\"\"\"\n\n\n\n\nCode\n%%time\nagg_df = pd.read_gbq(query=agg_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\n\nWall time: 15.9 s\n\n\n\n\nCode\nagg_df.head()\n\n\n\n\n\n\n  \n    \n      \n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      11693\n      40.590916\n      -73.809715\n      11414\n      40.657604\n      -73.844804\n      1\n    \n    \n      1\n      10040\n      40.858314\n      -73.930494\n      10040\n      40.858314\n      -73.930494\n      139\n    \n    \n      2\n      10473\n      40.818690\n      -73.858474\n      10030\n      40.818267\n      -73.942856\n      1\n    \n    \n      3\n      10451\n      40.820454\n      -73.925066\n      10031\n      40.825288\n      -73.950045\n      93\n    \n    \n      4\n      11209\n      40.621993\n      -74.030134\n      11228\n      40.616698\n      -74.013066\n      28\n    \n  \n\n\n\n\n\n\nCode\nagg_df = agg_df.sort_values('cnt', ascending=False)\n\n\n\n\nCode\nagg_df = agg_df[:100]\n\n\n\n\nCode\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    agg_df,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n\n\n요일별 위젯\n\n\nCode\nagg_query2 = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    LIMIT 100000\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  CAST(format_datetime('%u', pickup_datetime) AS INT64) -1 AS weekday,\n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6,7\n\"\"\"\n\n\n\n\nCode\n%%time\nagg_df2 = pd.read_gbq(query=agg_query2, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\n\nWall time: 7.41 s\n\n\n\n\nCode\nagg_df2.head()\n\n\n\n\n\n\n  \n    \n      \n      weekday\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      4\n      11214\n      40.599148\n      -73.996090\n      10035\n      40.795458\n      -73.929570\n      1\n    \n    \n      1\n      6\n      10171\n      40.755899\n      -73.973858\n      11430\n      40.646809\n      -73.786169\n      2\n    \n    \n      2\n      5\n      10461\n      40.847394\n      -73.840583\n      10475\n      40.874375\n      -73.823656\n      1\n    \n    \n      3\n      0\n      10172\n      40.755273\n      -73.974315\n      10065\n      40.764628\n      -73.963144\n      1\n    \n    \n      4\n      6\n      10162\n      40.769308\n      -73.949924\n      11430\n      40.646809\n      -73.786169\n      1\n    \n  \n\n\n\n\n\n\nCode\ndefault_data = agg_df2[agg_df2['weekday'] == 0].to_dict(orient='records')\n\n\n\n\nCode\narc_layer = pdk.Layer(\n    'ArcLayer',\n    default_data,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n\n\nCode\n# Widget 슬라이더 생성\nimport ipywidgets as widgets\nfrom IPython.display import display\nslider = widgets.IntSlider(0, min=0, max=6, step=1)\n\n# Widget에서 사용할 함수 정의 \ndef on_change(v):\n    results = agg_df2[agg_df2['weekday'] == slider.value].to_dict(orient='records')\n    arc_layer.data = results\n    r.update()\n\n# Deck과 슬라이더 연결\nslider.observe(on_change, names='value')\ndisplay(slider)"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html",
    "title": "Increase Loop Speed",
    "section": "",
    "text": "Python For loop\n\n\n자료 출처\n\nhttps://blog.fearcat.in/a?ID=00900-6997c6fb-2680-4531-af1d-73eeccce74ef\nhttps://aldente0630.github.io/data-science/2018/08/05/a-beginners-guide-to-optimizing-pandas-code-for-speed.html\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom math import *\n\n\n\n\n익스피디아 개발자 사이트에서 제공한 뉴욕 주 내 모든 호텔 좌표가 들어있는 데이터셋\n\n\nCode\ndf = pd.read_csv('./new_york_hotels.csv', encoding='cp1252')\ndf.head(3)\n\n\n\n\n\n\n  \n    \n      \n      ean_hotel_id\n      name\n      address1\n      city\n      state_province\n      postal_code\n      latitude\n      longitude\n      star_rating\n      high_rate\n      low_rate\n    \n  \n  \n    \n      0\n      269955\n      Hilton Garden Inn Albany/SUNY Area\n      1389 Washington Ave\n      Albany\n      NY\n      12206\n      42.68751\n      -73.81643\n      3.0\n      154.0272\n      124.0216\n    \n    \n      1\n      113431\n      Courtyard by Marriott Albany Thruway\n      1455 Washington Avenue\n      Albany\n      NY\n      12206\n      42.68971\n      -73.82021\n      3.0\n      179.0100\n      134.0000\n    \n    \n      2\n      108151\n      Radisson Hotel Albany\n      205 Wolf Rd\n      Albany\n      NY\n      12205\n      42.72410\n      -73.79822\n      3.0\n      134.1700\n      84.1600\n    \n  \n\n\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n  \n    \n      \n      ean_hotel_id\n      latitude\n      longitude\n      star_rating\n      high_rate\n      low_rate\n    \n  \n  \n    \n      count\n      1631.000000\n      1631.000000\n      1631.000000\n      1630.000000\n      1631.000000\n      1631.000000\n    \n    \n      mean\n      302845.515021\n      41.851026\n      -75.015019\n      2.894785\n      273.268624\n      169.408866\n    \n    \n      std\n      163497.215910\n      1.131960\n      1.774482\n      0.777486\n      504.191880\n      205.914287\n    \n    \n      min\n      6295.000000\n      40.583990\n      -79.742010\n      1.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      163765.500000\n      40.755540\n      -76.142530\n      2.500000\n      121.400000\n      97.318000\n    \n    \n      50%\n      252457.000000\n      41.558420\n      -73.988710\n      3.000000\n      170.000000\n      134.370000\n    \n    \n      75%\n      437138.000000\n      42.949455\n      -73.905340\n      3.500000\n      279.930000\n      195.260000\n    \n    \n      max\n      685047.000000\n      44.967850\n      -71.933340\n      5.000000\n      10888.500000\n      5990.250000"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#haversine-definition",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#haversine-definition",
    "title": "Increase Loop Speed",
    "section": "Haversine definition",
    "text": "Haversine definition\n두 위치 사이의 거리를 계산하는 함수 - https://stricky.tistory.com/284\n\n\nCode\ndef haversine(lat1, lon1, lat2, lon2):\n    miles_constant = 3959\n    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    mi = miles_constant * c\n    return mi"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#task",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#task",
    "title": "Increase Loop Speed",
    "section": "Task",
    "text": "Task\n어떤 위치, (40.671, -73.985)에서 df에 존재하는 모든 호텔까지의 거리를 구해봅시다."
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#looping-haversine",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#looping-haversine",
    "title": "Increase Loop Speed",
    "section": "Looping Haversine",
    "text": "Looping Haversine\n\n\nCode\ndef haversine_looping(df):\n    distance_list = []  # 빈 리스트를 생성\n    for i in range(0, len(df)):\n        d = haversine(40.671, -73.985,\n                      df.iloc[i]['latitude'], df.iloc[i]['longitude'])\n        distance_list.append(d)\n    return distance_list\n\n\n%%timeit은 Jupyter Notebook에서 사용되는 매직 명령어 중 하나로, 코드 실행 시간을 측정하는 도구입니다.\n%%timeit 매직 명령어를 사용하면 해당 셀의 코드를 여러 번 실행하여 실행 시간을 평균적으로 계산합니다. 이를 통해 코드의 실행 성능을 쉽게 측정하고 비교할 수 있습니다.\n\n\nCode\n%%timeit\n# Haversine 반복 함수 실행하기\ndf['distance'] = haversine_looping(df)\n# > 299 ms ± 8.85 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n277 ms ± 895 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nCode\ndf['distance'].describe()\n\n\ncount    1631.000000\nmean      111.318922\nstd       107.476086\nmin         0.163480\n25%         6.305530\n50%        71.070425\n75%       199.395866\nmax       314.936306\nName: distance, dtype: float64"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#iterrows-haversine",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#iterrows-haversine",
    "title": "Increase Loop Speed",
    "section": "Iterrows Haversine",
    "text": "Iterrows Haversine\n반복문을 돌려야 할 때 iterrows() 메서드를 사용하는 건 행을 반복하기 위한 더 좋은 방법이다.\niterrows()는 데이터 프레임의 행을 반복하며 행 자체를 포함하는 객체에 덧붙여 각 행의 색인을 반환하는 제너레이터다.\niterrows()는 판다스 데이터 프레임과 함께 작동하게끔 최적화되어 있으며 표준 함수 대부분을 실행하는 데 가장 효율적인 방법은 아니지만(나중에 자세히 설명) 단순 반복보다는 상당히 개선되었다.\n예제의 경우 iterrows()는 행을 수동으로 반복하는 것보다 거의 똑같은 문제를 약 4배 빠르게 해결한다.\n\n\nCode\nhaversine_series = []\nfor index, row in df.iloc[0:10].iterrows():\n    print(row['latitude'])\n\n\n42.68751\n42.68971\n42.7241\n42.65157\n42.68873\n42.72874\n42.68031\n42.65334\n42.72111\n42.67807\n\n\n\n\nCode\n%%timeit\n# Haversine applied on rows via iteration\nhaversine_series = []\nfor index, row in df.iterrows():\n    haversine_series.append(\n        haversine(40.671, -73.985, row['latitude'], row['longitude']))\ndf['distance'] = haversine_series\n# > 106 ms ± 6.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n103 ms ± 760 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nitertuples와 iterrows는 모두 Pandas 데이터프레임의 행을 순회(iterate)하는 메서드입니다. 그러나 itertuples는 iterrows보다 더욱 빠른 속도를 보이므로, 대체로 itertuples를 사용하는 것이 좋습니다.\n이유는 iterrows는 각 행(row)을 Series 객체로 반환하는 반면, itertuples는 각 행을 NamedTuple로 반환합니다.\nNamedTuple은 각 속성(attribute)에 이름이 지정되어 있기 때문에, Series보다 빠르게 데이터에 접근할 수 있습니다\n따라서 대용량의 데이터프레임을 다룰 때는 itertuples를 사용하는 것이 더욱 효율적입니다.\n\n\nCode\n%%timeit\nhaversine_series = []\nfor idx, lat, lon in df[['latitude', 'longitude']].itertuples():\n    haversine_series.append(haversine(40.671, -73.985, lat, lon))\n\ndf['distance'] = haversine_series\n# > 28.2 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n18.9 ms ± 80.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\ndf2 = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n\nfor row in df2.itertuples():\n    print(row)\n\n\nPandas(Index=0, A=1, B='a')\nPandas(Index=1, A=2, B='b')\nPandas(Index=2, A=3, B='c')"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#apply-haversine-on-rows",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#apply-haversine-on-rows",
    "title": "Increase Loop Speed",
    "section": "Apply Haversine on rows",
    "text": "Apply Haversine on rows\niterrows()보다 더 좋은 옵션은 데이터 프레임의 특정 축(행 또는 열을 의미)을 따라 함수를 적용하는 apply() 메서드를 사용하는 것이다.\napply()는 본질적으로 행을 반복하지만 Cython에서 이터레이터를 사용하는 것 같이 내부 최적화를 다양하게 활용하므로 iterrows()보다 훨씬 효율적이다.\n익명의 람다 함수를 사용하여 Haversine 함수를 각 행에 적용하며 각 행의 특정 셀을 함수 입력값으로 지정할 수 있다.\n람다 함수는 판다스가 행(axis = 1)과 열(axis = 0) 중 어디에 함수를 적용할지 정할 수 있게 축 매개 변수를 마지막에 포함한다.\n\nTiming “apply”\n\n\nCode\n%%timeit\ndf['distance'] = df.apply(lambda row: haversine(\n    40.671, -73.985, row['latitude'], row['longitude']), axis=1)\n# > 42.3 ms ± 835 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n32 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "title": "Increase Loop Speed",
    "section": "Vectorized implementation of Haversine applied on Pandas series",
    "text": "Vectorized implementation of Haversine applied on Pandas series\n\nTiming vectorized implementation\n함수 수행의 반복량 줄이는 방법을 이해하기 위해 판다스의 기본 단위, 데이터 프레임과 시리즈가 모두 배열 기반임을 알아둡시다. 기본 단위의 내부 구조는 개별 값(스칼라라고 함)마다 순차적으로 작동하는 대신 전체 배열 위로 작동하도록 설계된 내장 판다스 함수를 위해 변환된다. 벡터화는 전체 배열 위로 작업을 실행하는 프로세스다.\n판다스는 수학 연산에서 집계 및 문자열 함수(사용 가능한 함수의 광범위한 목록은 판다스 문서에서 확인해라)에 이르기까지 다양한 벡터화 함수를 포함하고 있다. 내장 함수는 판다스 시리즈와 데이터 프레임에서 작동하게끔 최적화되어있다. 결과적으로 벡터화 판다스 함수를 사용하는 건 비슷한 목적을 위해 손수 반복시키는 방법보다 거의 항상 바람직하다.\n지금까지는 Haversine 함수에 스칼라를 전달하였다. 그러나 Haversine 함수 내에서 사용하는 모든 함수를 배열 위로 작동시킬 수 있다. 이렇게 하면 거리 함수를 매우 간단하게 벡터화할 수 있다. 스칼라 값으로 각 위도, 경도를 전달하는 대신 전체 시리즈(열)를 전달한다. 이를 통해 판다스는 벡터화 함수에 적용 가능한 모든 최적화 옵션을 활용할 수 있고 특히 전체 배열에 대한 모든 계산을 동시에 수행하게 된다.\n\n\nCode\n%%timeit\n# Vectorized implementation of Haversine applied on Pandas series\ndf['distance'] = haversine(40.671, -73.985, df['latitude'], df['longitude'])\n# > 1.33 ms ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n1.32 ms ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n함수 벡터화를 통해 apply() 메서드 대비 50배 이상 개선시켰고 iterrows() 대비 100배 이상 개선시켰다. 입력 유형 변경하는 것 외에 아무것도 하지 않아도 됐다."
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "title": "Increase Loop Speed",
    "section": "Vectorized implementation of Haversine applied on NumPy arrays",
    "text": "Vectorized implementation of Haversine applied on NumPy arrays\n이 지점에서 그만두어도 괜찮다. 판다스 시리즈를 사용해 벡터화하면 상시 계산을 위한 최적화 요구 사항의 거의 대부분을 만족시킬 수 있다. 그러나 속도가 최우선이라면 넘파이 파이썬 라이브러리 형식에 도움을 요청해볼 수 있다.\n넘파이 라이브러리는 “과학 계산을 위한 파이썬 기본 패키지”를 표방하며 내부가 최적화된, 사전 컴파일된 C 코드로 작업을 수행한다. 판다스와 마찬가지로 넘파이는 배열 객체(ndarrays라고 함) 상에서 작동한다. 그러나 색인, 데이터 유형 확인 등과 같이 판다스 시리즈 작업으로 인한 오버헤드가 많이 발생하지 않는다. 결과적으로 넘파이 배열에 대한 작업은 판다스 시리즈에 대한 작업보다 훨씬 빠르다.\n판다스 시리즈가 제공하는 추가 기능이 중요하지 않을 때 넘파이 배열을 판다스 시리즈 대신 사용할 수 있다. 예를 들어 Haversine 함수의 벡터화 구현은 실제로 위도 또는 경도 시리즈의 색인을 사용하지 않으므로 사용할 수 있는 색인이 없어도 함수가 중단되지 않는다. 이에 비해 색인으로 값을 참조해야 하는 데이터 프레임의 조인 같은 작업을 수행한다면 판다스 개체를 계속 사용하는 편이 낫다.\n위도와 경도 배열을 시리즈의 values 메서드를 단순 사용해서 판다스 시리즈에서 넘파이 배열로 변환한다. 시리즈의 벡터화와 마찬가지로 넘파이 배열을 함수에 직접 전달하면 판다스가 전체 벡터에 함수를 적용시킨다.\n\nTiming vectorized implementation\n\n\nCode\n# Vectorized implementation of Haversine applied on NumPy arrays\n%%timeit\ndf['distance'] = haversine(\n    40.671, -73.985, df['latitude'].values, df['longitude'].values)\n# > 150 µs ± 1.57 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nUsageError: Line magic function `%%timeit` not found.\n\n\n\n\nCode\n%%timeit\n# Convert pandas arrays to NumPy ndarrays\nnp_lat = df['latitude'].values\nnp_lon = df['longitude'].values\n# > 7.07 µs ± 91.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n7.77 µs ± 78.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)"
  },
  {
    "objectID": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#summary",
    "href": "Data_Mining/Loop_Speed/Increase_Loop_Speed.html#summary",
    "title": "Increase Loop Speed",
    "section": "Summary",
    "text": "Summary\n판다스 코드 최적화에 관해 몇 가지 기본적인 결론을 내릴 수 있다.\n\n반복을 피해라. 사용 사례 대부분의 경우 반복은 느리고 불필요하다.\n\n반복해야 하는 경우 반복 함수가 아닌 apply()를 사용해라.\n\n보통은 벡터화가 스칼라 연산보다 낫다. 대부분의 판다스 작업은 벡터화시킬 수 있다.\n\n넘파이 배열에서의 벡터 연산은 판다스 시리즈에서 수행하는 것보다 효율적이다.\n\n\n실습\n아래의 조건을 만족하는 호텔의 List를 출력해 봅시다.\n\n현재 나는 (“latitude”, “longitude”) = (40.671, -73.985) 위치에 있고, 숙박할 호텔을 찾고 있습니다.\n직선거리 기준으로 200마일 안쪽에 있었으면 좋겠습니다.\nstar_rating이 4 이상인 호텔을 찾고 있습니다.\n\n해당 조건을 만족하는 호텔들을 출력해봅시다\n\n\nCode\ndf['distance'] = haversine(40.671, -73.985, df['latitude'], df['longitude'])\ndf[(df['distance'] <= 200) & (df['star_rating'] >= 4.0)\n   ][['name', 'distance', 'star_rating']]\n\n\n\n\n\n\n  \n    \n      \n      name\n      distance\n      star_rating\n    \n  \n  \n    \n      114\n      Topping Rose House\n      90.001264\n      4.5\n    \n    \n      129\n      Sheraton Brooklyn New York Hotel\n      1.423805\n      4.0\n    \n    \n      134\n      McCarren Hotel & Pool\n      3.786288\n      4.0\n    \n    \n      142\n      The Box House Hotel\n      4.885345\n      4.0\n    \n    \n      154\n      New York Marriott at the Brooklyn Bridge\n      1.577023\n      4.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1592\n      Viana Hotel & Spa, BW Premier Collection\n      23.431169\n      4.0\n    \n    \n      1600\n      The Ritz-Carlton New York, Westchester\n      27.466891\n      5.0\n    \n    \n      1601\n      Furnished Quarters Bank Street Commons\n      27.174550\n      4.0\n    \n    \n      1602\n      Global Luxury Apartments in White Plains\n      27.241501\n      4.0\n    \n    \n      1603\n      Global Luxury Suites at White Plains\n      27.604500\n      4.0\n    \n  \n\n245 rows × 3 columns"
  },
  {
    "objectID": "Data_Mining/matplotlib.html",
    "href": "Data_Mining/matplotlib.html",
    "title": "Matplotlib 기본",
    "section": "",
    "text": "Matplotlib 실습"
  },
  {
    "objectID": "Data_Mining/matplotlib.html#강의자료-출처",
    "href": "Data_Mining/matplotlib.html#강의자료-출처",
    "title": "Matplotlib 기본",
    "section": "강의자료 출처",
    "text": "강의자료 출처\n\n원문: by Aurélien Geron (Link)\nTranslated by Chansung PARK (Link)\n\nObject Oriented API Addition by Jehyun LEE (Link)\n\nTools - matplotlib\n이 노트북은 matplotlib 라이브러리를 사용하여 아름다운 그래프를 그리는 방법을 보여줍니다.\n\n이제현 주 : * 원 코드가 pyplot 기반으로 작성되었기에 object oriented API를 추가하였습니다. * pyplot은 pandas 같은 라이브러리와 함께 사용하며 그래프를 빠르게 그려보기 좋습니다. 그러나 코드의 가독성과 섬세한 제어는 object oriented API(객체지향 인터페이스)방식이 더 유리하게 느껴집니다. * pyplot과 object oriented API의 차이에 대해 상세히 알고 싶으시면 이 글을 참고하십시오"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html",
    "href": "Data_Mining/numpy/numpy.html",
    "title": "Numpy 기본",
    "section": "",
    "text": "Numpy 실습\n도구 - 넘파이(NumPy)\nNumPy 라이브러리는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.zeros",
    "href": "Data_Mining/numpy/numpy.html#np.zeros",
    "title": "Numpy 기본",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다.\n\n\nCode\nnp.zeros(5)\n\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다.\n\n\nCode\nnp.zeros((3, 4))\n\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#용어",
    "href": "Data_Mining/numpy/numpy.html#용어",
    "title": "Numpy 기본",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다.\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(예를 들어, \\(3 \\times 4 = 12\\) 입니다.).\n\n\n\nCode\na = np.zeros((3, 4))\na\n\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\n\nCode\na.shape\n\n\n(3, 4)\n\n\n\n\nCode\na.ndim  # len(a.shape)와 같음\n\n\n2\n\n\n\n\nCode\na.size\n\n\n12"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#n-차원-배열",
    "href": "Data_Mining/numpy/numpy.html#n-차원-배열",
    "title": "Numpy 기본",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크 = 3)입니다.\n\n\nCode\nnp.zeros((2, 3, 4))\n\n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#배열-타입",
    "href": "Data_Mining/numpy/numpy.html#배열-타입",
    "title": "Numpy 기본",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다.\n\n\nCode\ntype(np.zeros((3, 4)))\n\n\nnumpy.ndarray"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.ones",
    "href": "Data_Mining/numpy/numpy.html#np.ones",
    "title": "Numpy 기본",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\n\nCode\nnp.ones((3, 4))\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.full",
    "href": "Data_Mining/numpy/numpy.html#np.full",
    "title": "Numpy 기본",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\n\nCode\nnp.full((3, 4), np.pi)\n\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.empty",
    "href": "Data_Mining/numpy/numpy.html#np.empty",
    "title": "Numpy 기본",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다. (배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다.)\n\n\nCode\nnp.empty((2, 3))\n\n\narray([[ 4.63939877e-308, -5.64938534e+036, -1.78409531e+046],\n       [-4.32950182e-035, -8.72433218e-026, -1.57234264e-063]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.array",
    "href": "Data_Mining/numpy/numpy.html#np.array",
    "title": "Numpy 기본",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다.\n\n\nCode\nnp.array([[1, 2, 3, 4], [10, 20, 30, 40]])\n\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.arange",
    "href": "Data_Mining/numpy/numpy.html#np.arange",
    "title": "Numpy 기본",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다.\n\n\nCode\nnp.arange(1, 5)\n\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다.\n\n\nCode\nnp.arange(1.0, 5.0)\n\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다.\n\n\nCode\nnp.arange(1, 5, 0.5)\n\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다.\n\n\nCode\nprint(np.arange(0, 5/3, 1/3))  # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\n\nfor loops를 사용하지 않고 전체 array에 대한 연산 수행이 가능합니다.\n평균적으로 Numpy-based 알고리즘은 10~100배정도 속도가 더 빠르고 적은 메모리를 사용합니다.\n\n\nCode\nmy_arr = np.arange(1000000)\nmy_list = list(range(1000000))\n\n%time for _ in range(10): my_arr2 = my_arr * 2\n%time for _ in range(10): my_list2 = [x * 2 for x in my_list]\n\n\nCPU times: total: 15.6 ms\nWall time: 11 ms\nCPU times: total: 656 ms\nWall time: 658 ms\n\n\nFor loop를 돌릴 때의 속도 비교\n\n\nCode\nsize = 10\nfor x in range(size):\n    x ** 2\n\n\n\n\nCode\nsize = 10\n\n%timeit for x in range(size): x ** 2\n# out: 10 loops, best of 3: 136 ms per loop\n\n# avoid this\n%timeit for x in np.arange(size): x ** 2\n# out: 1 loops, best of 3: 1.16 s per loop\n\n# use this\n%timeit np.arange(size) ** 2\n# out: 100 loops, best of 3: 19.5 ms per loop\n\n\n2.14 µs ± 11.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n2.78 µs ± 60 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n1.31 µs ± 22.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.linspace",
    "href": "Data_Mining/numpy/numpy.html#np.linspace",
    "title": "Numpy 기본",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다.(arange와는 다르게 최댓값이 포함됩니다.)\n\n\nCode\nprint(np.linspace(0, 5/3, 6))  # np.linespace(min, max, cnt)\n\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.rand와-np.randn",
    "href": "Data_Mining/numpy/numpy.html#np.rand와-np.randn",
    "title": "Numpy 기본",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다.\n\n\nCode\nnp.random.rand(3, 4)\n\n\narray([[0.81111485, 0.03405234, 0.37080688, 0.35296648],\n       [0.94197711, 0.27110999, 0.45094818, 0.9486497 ],\n       [0.62916296, 0.13844609, 0.38923768, 0.68398249]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다.\n\n\nCode\nnp.random.randn(3, 4)\n\n\narray([[-0.30427505,  0.36393286,  0.10694428, -0.40721469],\n       [-0.68230465,  0.44406968, -0.71094999,  1.98631898],\n       [-1.72188395, -0.42928449,  0.82354692,  0.64813862]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다.(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요.)\n\n\nCode\n%matplotlib inline\nplt.hist(np.random.rand(100000), density=True, bins=100,\n         histtype='step', color='blue', label='rand')\nplt.hist(np.random.randn(100000), density=True, bins=100,\n         histtype='step', color='red', label='randn')\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc='upper left')\nplt.title('Random distributions')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.fromfunction",
    "href": "Data_Mining/numpy/numpy.html#np.fromfunction",
    "title": "Numpy 기본",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다.\n\n\nCode\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\n\nnp.fromfunction(my_function, (3, 2, 10))\n\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다.\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#dtype",
    "href": "Data_Mining/numpy/numpy.html#dtype",
    "title": "Numpy 기본",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다.\n\n\nCode\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\n\nint32 [1 2 3 4]\n\n\n\n\nCode\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다.\n\n\nCode\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#itemsize",
    "href": "Data_Mining/numpy/numpy.html#itemsize",
    "title": "Numpy 기본",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다.\n\n\nCode\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n\n8"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#data-버퍼",
    "href": "Data_Mining/numpy/numpy.html#data-버퍼",
    "title": "Numpy 기본",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\n\nCode\nf = np.array([[1, 2], [1000, 2000]], dtype=np.int32)\nf.data\n\n\n<memory at 0x0000017094138520>\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\n\nCode\nif (hasattr(f.data, 'tobytes')):\n    data_bytes = f.data.tobytes()  # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes()  # python 2\ndata_bytes\n\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#자신을-변경",
    "href": "Data_Mining/numpy/numpy.html#자신을-변경",
    "title": "Numpy 기본",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\n\nCode\ng = np.arange(24)\nprint(g)\nprint('랭크 : ', g.ndim)\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크 :  1\n\n\n\n\nCode\ng.shape = (6, 4)\nprint(g)\nprint('랭크 : ', g.ndim)\n\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크 :  2\n\n\n\n\nCode\ng.shape = (2, 3, 4)\nprint(g)\nprint('랭크 : ', g.ndim)\n\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크 :  3\n\n\n\n\nCode\ng[1, 1, 1]\n\n\n17"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#reshape",
    "href": "Data_Mining/numpy/numpy.html#reshape",
    "title": "Numpy 기본",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\n\nCode\ng2 = g.reshape(4, 6)\nprint(g2)\nprint('랭크 : ', g2.ndim)\n\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크 :  2\n\n\n\n\nCode\ng[0, 0, 0] = 10\ng2\n\n\narray([[10,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n행 1, 열 2의 원소를 999로 설정합니다. (인덱싱 방식은 아래를 참고하세요.)\n\n\nCode\ng2[1, 2] = 999\ng2\n\n\narray([[ 10,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\n\nCode\ng\n\n\narray([[[ 10,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n\n\n완전히 다른 공간에 값만 같게 복사를 하고 싶다면 copy를 사용합니다. 이렇게 할 경우 두 객체는 독립적인 객체로 존재함\n\n\nCode\ng2 = g.copy()"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#ravel",
    "href": "Data_Mining/numpy/numpy.html#ravel",
    "title": "Numpy 기본",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다.\n\n\nCode\ng.ravel()\n\n\narray([ 10,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#규칙-1",
    "href": "Data_Mining/numpy/numpy.html#규칙-1",
    "title": "Numpy 기본",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\n\nCode\nh = np.arange(5).reshape(1, 1, 5)\nh\n\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\n\nCode\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#규칙-2",
    "href": "Data_Mining/numpy/numpy.html#규칙-2",
    "title": "Numpy 기본",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\n\nCode\nk = np.arange(6).reshape(2, 3)\nk\n\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다.\n\n\nCode\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n(2,3) 크기의 ndarray에 (3,) 크기의 ndarray 더하기\n\n\nCode\nk\n\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n\nCode\n# 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\nk + [100, 200, 300]\n\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n\n\nCode\ntest = np.array([100, 200, 300])\ntest.shape\ntest\n\n\narray([100, 200, 300])\n\n\n\n\nCode\n# step 1\ntest = test.reshape(1, 3)\ntest\n\n\narray([[100, 200, 300]])\n\n\n\n\nCode\n# step 2\nnp.vstack((test, test))\n\n\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n\n\n\nCode\n# step 2\nnp.concatenate((test, test), axis=0)\n\n\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\n\nCode\nk\n\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n\nCode\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#규칙-3",
    "href": "Data_Mining/numpy/numpy.html#규칙-3",
    "title": "Numpy 기본",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\n\nCode\nk\n\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n\nCode\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요.\n\n\nCode\na = np.array([[0.0], [10.0], [20.0], [30.0]])\n\n\n\n\nCode\na\n\n\narray([[ 0.],\n       [10.],\n       [20.],\n       [30.]])\n\n\n\n\nCode\na = np.array([0.0, 10.0, 20.0, 30.0])\nb = np.array([1.0, 2.0, 3.0])\na[:, np.newaxis] + b\n\n\narray([[ 1.,  2.,  3.],\n       [11., 12., 13.],\n       [21., 22., 23.],\n       [31., 32., 33.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#업캐스팅",
    "href": "Data_Mining/numpy/numpy.html#업캐스팅",
    "title": "Numpy 기본",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\n\nCode\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\n\nuint8 [0 1 2 3 4]\n\n\n\n\nCode\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\n\nCode\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#ndarray-메서드",
    "href": "Data_Mining/numpy/numpy.html#ndarray-메서드",
    "title": "Numpy 기본",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면,\n\n\nCode\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint('평균 = ', a.mean())\n\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 =  6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다.\n\n\nCode\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, '=', func())\n\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면,\n\n\nCode\nc = np.arange(24).reshape(2, 3, 4)\nc\n\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\n\nCode\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\n\nCode\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n\n\nCode\nc.sum(axis=2)\n\n\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n\n\n여러 축에 대해서 더할 수도 있습니다.\n\n\nCode\nc\n\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\n\nCode\nc.sum(axis=(0, 2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\n\narray([ 60,  92, 124])\n\n\n\n\nCode\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n\n(60, 92, 124)"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#일반-함수",
    "href": "Data_Mining/numpy/numpy.html#일반-함수",
    "title": "Numpy 기본",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다.\n\n\nCode\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다.\n\n\nCode\nprint('원본 ndarray')\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign,\n             np.ceil, np.modf, np.isnan, np.cos):\n    print('\\n', func.__name__)\n    print(func(a))\n\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16152\\1597707201.py:6: RuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16152\\1597707201.py:6: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#이항-일반-함수",
    "href": "Data_Mining/numpy/numpy.html#이항-일반-함수",
    "title": "Numpy 기본",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다.\n\n\nCode\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\n\narray([ 3,  6,  2, 11])\n\n\n\n\nCode\nnp.greater(a, b)  # a > b 와 동일\n\n\narray([False, False,  True, False])\n\n\n\n\nCode\nnp.maximum(a, b)\n\n\narray([2, 8, 3, 7])\n\n\n\n\nCode\nnp.copysign(a, b)\n\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#차원-배열",
    "href": "Data_Mining/numpy/numpy.html#차원-배열",
    "title": "Numpy 기본",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다.\n\n\nCode\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n\n19\n\n\n\n\nCode\na[2:5]\n\n\narray([ 3, 19, 13])\n\n\n\n\nCode\na[2:-1]\n\n\narray([ 3, 19, 13,  7])\n\n\n\n\nCode\na[:2]\n\n\narray([1, 5])\n\n\n\n\nCode\na[2::2]\n\n\narray([ 3, 13,  3])\n\n\n\n\nCode\na[::-1]\n\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소도 수정할 수 있습니다.\n\n\nCode\na[3] = 999\na\n\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다.\n\n\nCode\na[2:5] = [997, 998, 999]\na\n\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "href": "Data_Mining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy 기본",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\n\nCode\na[2:5] = -1\na\n\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\nList는 브로드캐스팅으로 할당이 안됩니다.\n\n\nCode\nb = [1, 5, 3, 19, 13, 7, 3]\n# b[2:5] = -1 # 리스트는 브로드캐스팅으로 할당 불가능\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다.\n\n\nCode\ntry:\n    a[2:5] = [1, 2, 3, 4, 5, 6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\n\ncould not broadcast input array from shape (6,) into shape (3,)\n\n\n원소를 삭제할 수도 없습니다.\n\n\nCode\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\n\ncannot delete array elements\n\n\nList에서는 삭제가 가능\n\n\nCode\nb = [1, 5, 3, 19, 13, 7, 3]\ndel b[2:5]\nb\n\n\n[1, 5, 7, 3]\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\n\nCode\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다.\n\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\n\nCode\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다.\n\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\n\nCode\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다.\n\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\n\nCode\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다.\n\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#다차원-배열",
    "href": "Data_Mining/numpy/numpy.html#다차원-배열",
    "title": "Numpy 기본",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\n\nCode\nb = np.arange(48).reshape(4, 12)\nb\n\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\n\nCode\nb[1, 2]  # 행 1, 열 2\n\n\n14\n\n\n\n\nCode\nb[1, :]  # 행 1, 모든 열\n\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\n\nCode\nb[:, 1]  # 모든 행, 열 1\n\n\narray([ 1, 13, 25, 37])\n\n\n주의 : 다음 두 표현에는 미묘한 차이가 있습니다.\n\n\nCode\nb[1, :].shape\n\n\n(12,)\n\n\n\n\nCode\nb[1:2, :]\n\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "href": "Data_Mining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy 기본",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\n\nCode\nb[(0, 2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\n\nCode\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n\n\nCode\nb[2:, 0:2]\n\n\narray([[24, 25],\n       [36, 37]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\n\nCode\n# returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]\n\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#고차원",
    "href": "Data_Mining/numpy/numpy.html#고차원",
    "title": "Numpy 기본",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다.\n\n\nCode\nc = b.reshape(4, 2, 6)\nc\n\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\n\nCode\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n\n34\n\n\n\n\nCode\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\n\nCode\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#생략-부호-...",
    "href": "Data_Mining/numpy/numpy.html#생략-부호-...",
    "title": "Numpy 기본",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\n\nCode\nc[2, ...]  # 행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\n\nCode\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\n\nCode\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\n\narray([27, 33])\n\n\n\n\nCode\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#불리언-인덱싱",
    "href": "Data_Mining/numpy/numpy.html#불리언-인덱싱",
    "title": "Numpy 기본",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\n\nCode\nb = np.arange(48).reshape(4, 12)\nb\n\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\n\nCode\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\n\nCode\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#np.ix_",
    "href": "Data_Mining/numpy/numpy.html#np.ix_",
    "title": "Numpy 기본",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다.\n\n\nCode\nb[np.ix_((0, 2), (1, 4, 7, 10))]\n\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\n\nCode\nb[np.ix_(rows_on, cols_on)]\n\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\n\nCode\nnp.ix_(rows_on, cols_on)\n\n\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다.\n\n\nCode\nb.shape\n\n\n(4, 12)\n\n\n\n\nCode\nb[b % 3 == 1]\n\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#vstack",
    "href": "Data_Mining/numpy/numpy.html#vstack",
    "title": "Numpy 기본",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보겠습니다.\n\n\nCode\nq4 = np.vstack((q1, q2, q3))\nq4\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\n\nCode\nq4.shape\n\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다.(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다.)"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#hstack",
    "href": "Data_Mining/numpy/numpy.html#hstack",
    "title": "Numpy 기본",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다.\n\n\nCode\nnp.concatenate((q1, q3), axis=1)\n\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\n\nCode\nq5 = np.hstack((q1, q3))\nq5\n\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\n\nCode\nq5.shape\n\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다.\n\n\nCode\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\n\nall the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#concatenate",
    "href": "Data_Mining/numpy/numpy.html#concatenate",
    "title": "Numpy 기본",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\n\nCode\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\n\nCode\nq7.shape\n\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis = 1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#stack",
    "href": "Data_Mining/numpy/numpy.html#stack",
    "title": "Numpy 기본",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\n\nCode\nq1.shape\n\n\n(3, 4)\n\n\n\n\nCode\nq3.shape\n\n\n(3, 4)\n\n\n\n\nCode\nq8 = np.stack((q1, q3))\nq8\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\n\nCode\nq8.shape\n\n\n(2, 3, 4)"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#행렬-전치",
    "href": "Data_Mining/numpy/numpy.html#행렬-전치",
    "title": "Numpy 기본",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다.\n\n\nCode\nm1 = np.arange(10).reshape(2, 5)\nm1\n\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\n\nCode\nm1.T\n\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다.\n\n\nCode\nm2 = np.arange(5)\nm2\n\n\narray([0, 1, 2, 3, 4])\n\n\n\n\nCode\nm2.T\n\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다.\n\n\nCode\nm2r = m2.reshape(1, 5)\nm2r\n\n\narray([[0, 1, 2, 3, 4]])\n\n\n\n\nCode\nm2r.T\n\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#행렬-곱셈",
    "href": "Data_Mining/numpy/numpy.html#행렬-곱셈",
    "title": "Numpy 기본",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해봅시다.\n\n\nCode\nn1 = np.arange(10).reshape(2, 5)\nn1\n\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\n\nCode\nn2 = np.arange(15).reshape(5, 3)\nn2\n\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\n\nCode\nn1.dot(n2)\n\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#역행렬과-유사-역행렬",
    "href": "Data_Mining/numpy/numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy 기본",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다.\n\n\nCode\nm3 = np.array([[1, 2, 3], [5, 7, 11], [21, 29, 31]])\nm3\n\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\n\nCode\nlinalg.inv(m3)\n\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다.\n\n\nCode\nlinalg.pinv(m3)\n\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#단위-행렬",
    "href": "Data_Mining/numpy/numpy.html#단위-행렬",
    "title": "Numpy 기본",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다.(작은 소숫점 오차가 있습니다.)\n\n\nCode\nm3.dot(linalg.inv(m3))\n\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다.\n\n\nCode\nnp.eye(3)\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#qr-분해",
    "href": "Data_Mining/numpy/numpy.html#qr-분해",
    "title": "Numpy 기본",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다.\n\n\nCode\nq, r = linalg.qr(m3)\nq\n\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\n\nCode\nr\n\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\n\nCode\nq.dot(r)  # q.r는 m3와 같습니다\n\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#행렬식",
    "href": "Data_Mining/numpy/numpy.html#행렬식",
    "title": "Numpy 기본",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다.\n\n\nCode\nlinalg.det(m3)  # 행렬식 계산\n\n\n43.99999999999997"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#고윳값과-고유벡터",
    "href": "Data_Mining/numpy/numpy.html#고윳값과-고유벡터",
    "title": "Numpy 기본",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다.\n\n\nCode\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues  # λ\n\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\n\nCode\neigenvectors  # v\n\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\n\nCode\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\n\narray([[ 6.21724894e-15,  1.66533454e-15, -3.10862447e-15],\n       [ 3.55271368e-15,  5.30131494e-15, -5.32907052e-15],\n       [ 3.55271368e-14,  5.38458167e-15, -9.76996262e-15]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#특잇값-분해",
    "href": "Data_Mining/numpy/numpy.html#특잇값-분해",
    "title": "Numpy 기본",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다.\n\n\nCode\nm4 = np.array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0],\n               [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]])\nm4\n\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\n\nCode\nU, S_diag, V = linalg.svd(m4)\nU\n\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0., -1.,  0.]])\n\n\n\n\nCode\nS_diag\n\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다.\n\n\nCode\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\n\nCode\nV\n\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [ 0.        , -1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\n\nCode\nU.dot(S).dot(V)  # U.Σ.V == m4\n\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#대각원소와-대각합",
    "href": "Data_Mining/numpy/numpy.html#대각원소와-대각합",
    "title": "Numpy 기본",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\n\nCode\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\n\narray([ 1,  7, 31])\n\n\n\n\nCode\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n\n39"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#선형-방정식-풀기",
    "href": "Data_Mining/numpy/numpy.html#선형-방정식-풀기",
    "title": "Numpy 기본",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다.\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\n\nCode\ncoeffs = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\n\narray([-3.,  2.])\n\n\nsolution을 확인해 봅시다.\n\n\nCode\ncoeffs.dot(solution), depvars  # 동일한 결과\n\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해봅시다.\n\n\nCode\nnp.allclose(coeffs.dot(solution), depvars)\n\n\nTrue"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#바이너리-.npy-포맷",
    "href": "Data_Mining/numpy/numpy.html#바이너리-.npy-포맷",
    "title": "Numpy 기본",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 봅시다.\n\n\nCode\na = np.random.rand(2, 3)\na\n\n\narray([[0.24493352, 0.04036961, 0.7987492 ],\n       [0.121653  , 0.05816837, 0.57015007]])\n\n\n\n\nCode\nnp.save('my_array', a)\n\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다.\n\n\nCode\nwith open('my_array.npy', 'rb') as f:\n    content = f.read()\n\ncontent\n\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\npOfJ\\xfbY\\xcf?pBrPS\\xab\\xa4?\\xdb\\xf7h|Z\\x8f\\xe9?Pp\\xc8\\xa9\\xa6$\\xbf?@\\xa3\\xe5\\xc0>\\xc8\\xad?F\\xbb=Z\\xab>\\xe2?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다.\n\n\nCode\na_loaded = np.load('my_array.npy')\na_loaded\n\n\narray([[0.24493352, 0.04036961, 0.7987492 ],\n       [0.121653  , 0.05816837, 0.57015007]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#텍스트-포맷",
    "href": "Data_Mining/numpy/numpy.html#텍스트-포맷",
    "title": "Numpy 기본",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해봅시다.\n\n\nCode\nnp.savetxt('my_array.csv', a)\n\n\n파일 내용을 확인해 보겠습니다.\n\n\nCode\nwith open('my_array.csv', 'rt') as f:\n    print(f.read())\n\n\n2.449335206298388634e-01 4.036960942278688957e-02 7.987492017634808539e-01\n1.216530003079594469e-01 5.816837410640696149e-02 5.701500666162722109e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다.\n\n\nCode\nnp.savetxt('my_array.csv', a, delimiter=',')\n\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다.\n\n\nCode\na_loaded = np.loadtxt('my_array.csv', delimiter=',')\na_loaded\n\n\narray([[0.24493352, 0.04036961, 0.7987492 ],\n       [0.121653  , 0.05816837, 0.57015007]])"
  },
  {
    "objectID": "Data_Mining/numpy/numpy.html#압축된-.npz-포맷",
    "href": "Data_Mining/numpy/numpy.html#압축된-.npz-포맷",
    "title": "Numpy 기본",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다.\n\n\nCode\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\n\nCode\nnp.savez('my_arrays', my_a=a, my_b=b)\n\n\n파일 내용을 확인해 봅시다. .npz 파일 확장자가 자동으로 추가되었습니다.\n\n\nCode\nwith open('my_arrays.npz', 'rb') as f:\n    content = f.read()\n\nrepr(content)[:180] + '[...]'\n\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\xffD\\\\xd9j\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다.\n\n\nCode\nmy_arrays = np.load('my_arrays.npz')\nmy_arrays\n\n\n<numpy.lib.npyio.NpzFile at 0x17090d8fa00>\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\n\nCode\nlist(my_arrays.keys())\n\n\n['my_a', 'my_b']\n\n\n\n\nCode\nmy_arrays['my_a']\n\n\narray([[0.24493352, 0.04036961, 0.7987492 ],\n       [0.121653  , 0.05816837, 0.57015007]])"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html",
    "href": "Data_Mining/pandas/pandas.html",
    "title": "Pandas 기본",
    "section": "",
    "text": "Pandas 실습\n도구 - 판다스(Pandas)\npandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다.\n필요 라이브러리:\n넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#series-만들기",
    "href": "Data_Mining/pandas/pandas.html#series-만들기",
    "title": "Pandas 기본",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 봅시다.\n\n\nCode\nnp.array([2, -1, 3, 5])\n\n\narray([ 2, -1,  3,  5])\n\n\n\n\nCode\ns = pd.Series([2, -1, 3, 5])\ns\n\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#d-ndarray와-비슷",
    "href": "Data_Mining/pandas/pandas.html#d-ndarray와-비슷",
    "title": "Pandas 기본",
    "section": "1D ndarray와 비슷",
    "text": "1D ndarray와 비슷\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다.\n\n\nCode\nnp.exp(s)\n\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다.\n\n\nCode\ns + [1000, 2000, 3000, 4000]\n\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다.\n\n\nCode\ns\n\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\n\nCode\ns + 1000\n\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다.\n\n\nCode\ns < 0\n\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#인덱스-레이블",
    "href": "Data_Mining/pandas/pandas.html#인덱스-레이블",
    "title": "Pandas 기본",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다.\n\n\nCode\ns2 = pd.Series([68, 83, 112, 68], index=['alice', 'bob', 'charles', 'darwin'])\ns2\n\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다.\n\n\nCode\ns2['bob']\n\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다.\n\n\nCode\ns2[1]\n\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다.\n\n\nCode\ns2.loc['bob']\n\n\n83\n\n\n\n\nCode\ns2.iloc[1]\n\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다.\n\n\nCode\ns2.iloc[1:3]\n\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다.\n\n\nCode\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\n\nCode\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다.\n\n\nCode\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print('키 에러 : ', e)\n\n\n키 에러 :  0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다.\n\n\nCode\nsurprise_slice.iloc[0]\n\n\n1002"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dict에서-초기화",
    "href": "Data_Mining/pandas/pandas.html#dict에서-초기화",
    "title": "Pandas 기본",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다.\n\n\nCode\nweights = {'alice': 68, 'bob': 83, 'colin': 86, 'darwin': 68}\ns3 = pd.Series(weights)\ns3\n\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다.\n\n\nCode\ns4 = pd.Series(weights, index=['colin', 'alice'])\ns4\n\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#자동-정렬",
    "href": "Data_Mining/pandas/pandas.html#자동-정렬",
    "title": "Pandas 기본",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\n\nCode\ns2\n\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\n\nCode\ns3\n\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\n\nCode\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 'colin'이 없고 s3에 'charles'가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다.\n\n\nCode\ns5 = pd.Series([1000, 1000, 1000, 1000])\nprint('s2 =', s2.values)\nprint('s5 =', s5.values)\n\ns2 + s5\n\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#스칼라로-초기화",
    "href": "Data_Mining/pandas/pandas.html#스칼라로-초기화",
    "title": "Pandas 기본",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다. 모든 원소가 이 스칼라 값으로 설정됩니다.\n\n\nCode\nmeaning = pd.Series(42, ['life', 'universe', 'everything'])\nmeaning\n\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#series-이름",
    "href": "Data_Mining/pandas/pandas.html#series-이름",
    "title": "Pandas 기본",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다.\n\n\nCode\ns6 = pd.Series([83, 68], index=['bob', 'alice'], name='weights')\ns6\n\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\n\nCode\ns6.name\n\n\n'weights'"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#series-그래프-출력",
    "href": "Data_Mining/pandas/pandas.html#series-그래프-출력",
    "title": "Pandas 기본",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다.\n\n\nCode\n%matplotlib inline\ntemperatures = [4.4, 5.1, 6.1, 6.2, 6.1, 6.1, 5.7, 5.2, 4.7, 4.1, 3.9, 3.5]\ns7 = pd.Series(temperatures, name='Temperature')\ns7.plot()\nplt.show()\n\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#시간-범위",
    "href": "Data_Mining/pandas/pandas.html#시간-범위",
    "title": "Pandas 기본",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 봅시다. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\n\nCode\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\n\nCode\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다.\n\n\nCode\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 봅시다.\n\n\nCode\ntemp_series.plot(kind='bar')\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#리샘플링",
    "href": "Data_Mining/pandas/pandas.html#리샘플링",
    "title": "Pandas 기본",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다.\n\n\nCode\ntemp_series_freq_2H = temp_series.resample('2H')\ntemp_series_freq_2H\n\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x0000020EC5F12770>\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다.\n\n\nCode\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\n\n\nCode\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 봅시다.\n\n\nCode\ntemp_series_freq_2H.plot(kind='bar')\nplt.show()\n\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다.\n\n\nCode\ntemp_series_freq_2H = temp_series.resample('2H').mean()\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다.\n\n\nCode\ntemp_series_freq_2H = temp_series.resample('2H').apply(np.min)\ntemp_series_freq_2H\n\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#업샘플링과-보간",
    "href": "Data_Mining/pandas/pandas.html#업샘플링과-보간",
    "title": "Pandas 기본",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다.\n\n\nCode\ntemp_series_freq_15min = temp_series.resample('15Min').mean()\ntemp_series_freq_15min.head(n=10)  # `head`는 상위 n 개의 값만 출력합니다\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\n\nCode\ntemp_series_freq_15min = temp_series.resample(\n    '15Min').interpolate(method='cubic')\ntemp_series_freq_15min.head(n=10)\n\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\n\nCode\ntemp_series.plot(label='Period: 1 hour')\ntemp_series_freq_15min.plot(label='Period: 15 minutes')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#시간대",
    "href": "Data_Mining/pandas/pandas.html#시간대",
    "title": "Pandas 기본",
    "section": "시간대",
    "text": "시간대\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다. https://www.timeanddate.com/time/map/\n\n\nCode\ntemp_series\n\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\n\nCode\ntemp_series_ny = temp_series.tz_localize('America/New_York')\ntemp_series_ny\n\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다.\n\n\nCode\ntemp_series_paris = temp_series_ny.tz_convert('Europe/Paris')\ntemp_series_paris\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가봅시다.(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다.)\n\n\nCode\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다.\n\n\nCode\ntry:\n    temp_series_paris_naive.tz_localize('Europe/Paris')\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\n\nCode\ntemp_series_paris_naive.tz_localize('Europe/Paris', ambiguous='infer')\n\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#기간",
    "href": "Data_Mining/pandas/pandas.html#기간",
    "title": "Pandas 기본",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 봅시다.\n\n\nCode\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다.\n\n\nCode\nquarters + 3\n\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 봅시다.\n\n\nCode\nquarters.asfreq('M')\n\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\n\nCode\nquarters\n\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다.\n\n\nCode\nquarters.asfreq('M', how='start')\n\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다. pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\n\nCode\nquarters.asfreq('A')\n\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다.\n\n\nCode\nquarterly_revenue = pd.Series(\n    [300, 320, 290, 390, 320, 360, 310, 410], index=quarters)\nquarterly_revenue\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\n\nCode\nquarterly_revenue.plot(kind='line')\nplt.show()\n\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다.\n\n\nCode\nquarterly_revenue\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\n\nCode\nlast_hours = quarterly_revenue.to_timestamp(how='end', freq='H')\nlast_hours\n\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다.\n\n\nCode\nlast_hours.to_period()\n\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다.\n\n\nCode\nmonths_2022 = pd.period_range('2022', periods=12, freq='M')\none_day_after_last_days = months_2022.asfreq('D') + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\nlast_bdays.to_period('H') + 9\n\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dataframe-만들기",
    "href": "Data_Mining/pandas/pandas.html#dataframe-만들기",
    "title": "Pandas 기본",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다.\n\n\nCode\npeople_dict = {\n    'weight': pd.Series([68, 83, 112], index=['alice', 'bob', 'charles']),\n    'birthyear': pd.Series([1984, 1985, 1992], index=['bob', 'alice', 'charles'], name='year'),\n    'children': pd.Series([0, 3], index=['charles', 'bob']),\n    'hobby': pd.Series(['Biking', 'Dancing'], index=['alice', 'bob']),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n      children\n      hobby\n    \n  \n  \n    \n      alice\n      68\n      1985\n      NaN\n      Biking\n    \n    \n      bob\n      83\n      1984\n      3.0\n      Dancing\n    \n    \n      charles\n      112\n      1992\n      0.0\n      NaN\n    \n  \n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다('year'란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다.\n\n\nCode\npeople['birthyear']\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다.\n\n\nCode\npeople[['birthyear', 'hobby']]\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면,\n\n\nCode\npeople_dict\n\n\n{'weight': alice       68\n bob         83\n charles    112\n dtype: int64,\n 'birthyear': bob        1984\n alice      1985\n charles    1992\n Name: year, dtype: int64,\n 'children': charles    0\n bob        3\n dtype: int64,\n 'hobby': alice     Biking\n bob      Dancing\n dtype: object}\n\n\n\n\nCode\nd2 = pd.DataFrame(\n    people_dict,\n    columns=['birthyear', 'weight', 'height'],\n    index=['bob', 'alice', 'eugene']\n)\n\n\n\n\nCode\nd2\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      weight\n      height\n    \n  \n  \n    \n      bob\n      1984.0\n      83.0\n      NaN\n    \n    \n      alice\n      1985.0\n      68.0\n      NaN\n    \n    \n      eugene\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다.\n\n\nCode\nvalues = [\n    [1985, np.nan, 'Biking',   68],\n    [1984, 3,      'Dancing',  83],\n    [1992, 0,      np.nan,    112]\n]\nd3 = pd.DataFrame(\n    values,\n    columns=['birthyear', 'children', 'hobby', 'weight'],\n    index=['alice', 'bob', 'charles']\n)\nd3\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3.0\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0.0\n      NaN\n      112\n    \n  \n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다.\ndtype = object는 문자열 데이터를 의미합니다.\n\n\nCode\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\n\nCode\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n    masked_array,\n    columns=['birthyear', 'children', 'hobby', 'weight'],\n    index=['alice', 'bob', 'charles']\n)\nd3\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다.\n\n\nCode\nd3\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\n\n\nCode\nd4 = pd.DataFrame(\n    d3,\n    columns=['hobby', 'children'],\n    index=['alice', 'bob']\n)\nd4\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      NaN\n    \n    \n      bob\n      Dancing\n      3\n    \n  \n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다.\n\n\nCode\npeople = pd.DataFrame({\n    'birthyear': {'alice': 1985, 'bob': 1984, 'charles': 1992},\n    'hobby': {'alice': 'Biking', 'bob': 'Dancing'},\n    'weight': {'alice': 68, 'bob': 83, 'charles': 112},\n    'children': {'bob': 3, 'charles': 0}\n})\n\npeople\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#멀티-인덱싱",
    "href": "Data_Mining/pandas/pandas.html#멀티-인덱싱",
    "title": "Pandas 기본",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면,\n\n\nCode\nd5 = pd.DataFrame(\n    {\n        ('public', 'birthyear'):\n        {('Paris', 'alice'): 1985, ('Paris', 'bob'): 1984, ('London', 'charles'): 1992},\n        ('public', 'hobby'):\n        {('Paris', 'alice'): 'Biking', ('Paris', 'bob'): 'Dancing'},\n        ('private', 'weight'):\n        {('Paris', 'alice'): 68, ('Paris', 'bob'): 83, ('London', 'charles'): 112},\n        ('private', 'children'):\n        {('Paris', 'alice'): np.nan, ('Paris', 'bob'): 3, ('London', 'charles'): 0}\n    }\n)\nd5\n\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n이제 'public' 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다.\n\n\nCode\nd5['public']\n\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      London\n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n\n\nCode\nd5['public', 'hobby']  # d5['public']['hobby']와 같습니다.\n\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\n\nCode\nd5['public']['hobby']\n\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#레벨-낮추기",
    "href": "Data_Mining/pandas/pandas.html#레벨-낮추기",
    "title": "Pandas 기본",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 봅시다.\n\n\nCode\nd5\n\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다).\n\n\nCode\nd5.columns = d5.columns.droplevel(level=0)\nd5\n\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\n\nCode\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level=0)\nd6\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#전치",
    "href": "Data_Mining/pandas/pandas.html#전치",
    "title": "Pandas 기본",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다.\n\n\nCode\nd5\n\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\n\nCode\nd6 = d5.T\nd6\n\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#레벨-스택과-언스택",
    "href": "Data_Mining/pandas/pandas.html#레벨-스택과-언스택",
    "title": "Pandas 기본",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다.\n\n\nCode\nd6\n\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0\n    \n  \n\n\n\n\n\n\nCode\nd7 = d6.stack()\nd7\n\n\n\n\n\n\n  \n    \n      \n      \n      London\n      Paris\n    \n  \n  \n    \n      birthyear\n      alice\n      NaN\n      1985\n    \n    \n      bob\n      NaN\n      1984\n    \n    \n      charles\n      1992\n      NaN\n    \n    \n      hobby\n      alice\n      NaN\n      Biking\n    \n    \n      bob\n      NaN\n      Dancing\n    \n    \n      weight\n      alice\n      NaN\n      68\n    \n    \n      bob\n      NaN\n      83\n    \n    \n      charles\n      112\n      NaN\n    \n    \n      children\n      bob\n      NaN\n      3.0\n    \n    \n      charles\n      0.0\n      NaN\n    \n  \n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\n\nCode\nd8 = d7.unstack()\nd8\n\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN\n    \n  \n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다.\n\n\nCode\nd9 = d8.unstack()\nd9\n\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다.\n\n\nCode\nd10 = d9.unstack(level=(0, 1))\nd10\n\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "Data_Mining/pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "Pandas 기본",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\n\nPivot\n\n\nCode\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        'value': frame.to_numpy().ravel('F'),\n        'variable': np.asarray(frame.columns).repeat(N),\n        'date': np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=['date', 'variable', 'value'])\n\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\n\n\nCode\ndf\n\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -1.361025\n    \n    \n      1\n      2000-01-04\n      A\n      1.124727\n    \n    \n      2\n      2000-01-05\n      A\n      0.187734\n    \n    \n      3\n      2000-01-03\n      B\n      1.221447\n    \n    \n      4\n      2000-01-04\n      B\n      -0.645248\n    \n    \n      5\n      2000-01-05\n      B\n      0.368883\n    \n    \n      6\n      2000-01-03\n      C\n      1.550405\n    \n    \n      7\n      2000-01-04\n      C\n      -1.529291\n    \n    \n      8\n      2000-01-05\n      C\n      -1.041943\n    \n    \n      9\n      2000-01-03\n      D\n      -0.250513\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224425\n    \n    \n      11\n      2000-01-05\n      D\n      0.763475\n    \n  \n\n\n\n\nvariable 변수의 값이 A인 것들을 모두 출력해봅시다.\n\n\nCode\nfiltered = df[df['variable'] == 'A']\nfiltered\n\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -1.361025\n    \n    \n      1\n      2000-01-04\n      A\n      1.124727\n    \n    \n      2\n      2000-01-05\n      A\n      0.187734\n    \n  \n\n\n\n\n그러나 변수를 사용하여 시계열 연산을 수행하려고 합니다. 더 나은 표현은 열이 고유 변수이고 날짜 인덱스가 개별 관측치를 식별하는 위치입니다. 데이터를 이 형식으로 재구성하려면 DataFrame.pivot() 메서드(최상위 기능 pivot()으로도 구현됩니다.)를 사용합니다.\n\n\nCode\npivoted = df.pivot(index='date', columns='variable', values='value')\npivoted\n\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -1.361025\n      1.221447\n      1.550405\n      -0.250513\n    \n    \n      2000-01-04\n      1.124727\n      -0.645248\n      -1.529291\n      -0.224425\n    \n    \n      2000-01-05\n      0.187734\n      0.368883\n      -1.041943\n      0.763475\n    \n  \n\n\n\n\n\n\nCode\npivoted.columns\n\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\n\nCode\npivoted.index\n\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nvalues 인수가 생략되고 입력 DataFrame에 pivot() 할 열 또는 인덱스 입력으로 사용되지 않는 두 개 이상의 값 열이 있는 경우, 결과적으로 “pivoted” DataFrame에는 최상위 수준이 각 값 열을 나타내는 계층 열이 있습니다.\n\n\nCode\ndf['value2'] = df['value'] * 2\n\n\n\n\nCode\ndf\n\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n      value2\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -1.361025\n      -2.722050\n    \n    \n      1\n      2000-01-04\n      A\n      1.124727\n      2.249455\n    \n    \n      2\n      2000-01-05\n      A\n      0.187734\n      0.375469\n    \n    \n      3\n      2000-01-03\n      B\n      1.221447\n      2.442895\n    \n    \n      4\n      2000-01-04\n      B\n      -0.645248\n      -1.290496\n    \n    \n      5\n      2000-01-05\n      B\n      0.368883\n      0.737767\n    \n    \n      6\n      2000-01-03\n      C\n      1.550405\n      3.100810\n    \n    \n      7\n      2000-01-04\n      C\n      -1.529291\n      -3.058582\n    \n    \n      8\n      2000-01-05\n      C\n      -1.041943\n      -2.083886\n    \n    \n      9\n      2000-01-03\n      D\n      -0.250513\n      -0.501025\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224425\n      -0.448850\n    \n    \n      11\n      2000-01-05\n      D\n      0.763475\n      1.526951\n    \n  \n\n\n\n\n\n\nCode\npivoted = df.pivot(index='date', columns='variable')\npivoted\n\n\n\n\n\n\n  \n    \n      \n      value\n      value2\n    \n    \n      variable\n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -1.361025\n      1.221447\n      1.550405\n      -0.250513\n      -2.722050\n      2.442895\n      3.100810\n      -0.501025\n    \n    \n      2000-01-04\n      1.124727\n      -0.645248\n      -1.529291\n      -0.224425\n      2.249455\n      -1.290496\n      -3.058582\n      -0.448850\n    \n    \n      2000-01-05\n      0.187734\n      0.368883\n      -1.041943\n      0.763475\n      0.375469\n      0.737767\n      -2.083886\n      1.526951\n    \n  \n\n\n\n\n\n\nCode\npivoted['value']\n\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -1.361025\n      1.221447\n      1.550405\n      -0.250513\n    \n    \n      2000-01-04\n      1.124727\n      -0.645248\n      -1.529291\n      -0.224425\n    \n    \n      2000-01-05\n      0.187734\n      0.368883\n      -1.041943\n      0.763475"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#행-참조하기",
    "href": "Data_Mining/pandas/pandas.html#행-참조하기",
    "title": "Pandas 기본",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가봅시다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다.\n\n\nCode\npeople['birthyear']\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\n\nCode\npeople.loc['charles']\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다.\n\n\nCode\npeople.iloc[2]\n\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\n\nCode\npeople.iloc[1:3]\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다.\n\n\nCode\npeople[np.array([True, False, True])]\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다.\n\n\nCode\npeople['birthyear'] < 1990\n\n\nalice       True\nbob         True\ncharles    False\nName: birthyear, dtype: bool\n\n\n\n\nCode\npeople[people['birthyear'] < 1990]\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#열-추가-삭제",
    "href": "Data_Mining/pandas/pandas.html#열-추가-삭제",
    "title": "Pandas 기본",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\n\nCode\npeople['age'] = 2022 - people['birthyear']  # 'age' 열을 추가합니다\npeople['over 30'] = people['age'] > 30      # 'over 30' 열을 추가합니다\n\npeople\n\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n      age\n      over 30\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n      37\n      True\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n      38\n      True\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n      30\n      False\n    \n  \n\n\n\n\n\n\nCode\nbirthyears = people.pop('birthyear')\ndel people['children']\n\n\n\n\nCode\nbirthyears\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      age\n      over 30\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      37\n      True\n    \n    \n      bob\n      Dancing\n      83\n      38\n      True\n    \n    \n      charles\n      NaN\n      112\n      30\n      False\n    \n  \n\n\n\n\n\n\nCode\n# 딕셔너리도 유사함\nweights = {'alice': 68, 'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\n\nCode\nweights.pop('alice')\n\n\n68\n\n\n\n\nCode\nweights\n\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\n\nCode\ndel weights['bob']\n\n\n\n\nCode\nweights\n\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다.\n\n\nCode\npeople.index\n\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\n\nCode\n# alice 누락됨, eugene은 무시됨\npeople['pets'] = pd.Series({'bob': 0, 'charles': 5, 'eugene': 1})\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다.\n\n\nCode\npeople.insert(1, 'height', [172, 181, 185])\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#새로운-열-할당하기",
    "href": "Data_Mining/pandas/pandas.html#새로운-열-할당하기",
    "title": "Pandas 기본",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다.\n\n\nCode\npeople.assign(\n    body_mass_index=people['weight'] / (people['height'] / 100) ** 2,\n    has_pets=people['pets'] > 0\n)\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      has_pets\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\nCode\npeople['body_mass_index'] = people['weight'] / (people['height'] / 100) ** 2\n\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n\n\nCode\ndel people['body_mass_index']\n\n\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\n\nCode\ntry:\n    people.assign(\n        body_mass_index=people['weight'] / (people['height'] / 100) ** 2,\n        overweight=people['body_mass_index'] > 25\n    )\nexcept KeyError as e:\n    print('키 에러 : ', e)\n\n\n키 에러 :  'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\n\nCode\nd6 = people.assign(\n    body_mass_index=people['weight'] / (people['height'] / 100) ** 2)\nd6.assign(overweight=d6['body_mass_index'] > 25)\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다.\n\n\nCode\ntry:\n    (people\n     .assign(body_mass_index=people['weight'] / (people['height'] / 100) ** 2)\n     .assign(overweight=people['body_mass_index'] > 25)\n     )\nexcept KeyError as e:\n    print('키 에러 : ', e)\n\n\n키 에러 :  'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다.\n\n\nCode\n(people\n .assign(body_mass_index=lambda df: df['weight'] / (df['height'] / 100) ** 2)\n .assign(overweight=lambda df: df['body_mass_index'] > 25)\n )\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n문제가 해결되었군요!\n\n\nCode\npeople['body_mass_index'] = people['weight'] / (people['height'] / 100) ** 2\npeople['overweight'] = people['body_mass_index'] > 25\n\n\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#표현식-평가",
    "href": "Data_Mining/pandas/pandas.html#표현식-평가",
    "title": "Pandas 기본",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\nCode\n'weight / (height/100) ** 2 > 25'\n\n\n'weight / (height/100) ** 2 > 25'\n\n\n\n\nCode\npeople.eval('weight / (height/100) ** 2 > 25')\n\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다.\n\n\nCode\npeople.eval('body_mass_index = weight / (height/100) ** 2', inplace=True)\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\nCode\noverweight_threshold = 30\npeople.eval('overweight = body_mass_index > @overweight_threshold', inplace=True)\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dataframe-쿼리하기",
    "href": "Data_Mining/pandas/pandas.html#dataframe-쿼리하기",
    "title": "Pandas 기본",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다.\n\n\nCode\npeople\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\nCode\npeople.query('age > 30 and pets == 0')\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n  \n\n\n\n\n\n\nCode\npeople[(people['age'] > 30) & (people['pets'] == 0)]\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n  \n\n\n\n\n\n\nCode\nmask = (people['age'] > 30) & (people['pets'] == 0)\n\n\n\n\nCode\npeople[mask]\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dataframe-정렬",
    "href": "Data_Mining/pandas/pandas.html#dataframe-정렬",
    "title": "Pandas 기본",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해봅시다..\n\n\nCode\npeople.sort_index(ascending=False)\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      height\n      weight\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      charles\n      NaN\n      185\n      112\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n    \n      bob\n      Dancing\n      181\n      83\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      alice\n      Biking\n      172\n      68\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n  \n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다.\n\n\nCode\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n  \n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다.\n\n\nCode\npeople.sort_values(by='age', inplace=True)\npeople\n\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dataframe-그래프-그리기",
    "href": "Data_Mining/pandas/pandas.html#dataframe-그래프-그리기",
    "title": "Pandas 기본",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다.\n\n\nCode\npeople.plot(kind='line', x='body_mass_index', y=['height', 'weight'])\nplt.show()\n\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\n\nCode\npeople.plot(kind='scatter', x='height', y='weight', s=[40, 120, 200])\nplt.show()\n\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\nCode\ndf4 = pd.DataFrame(\n    {\n        'a': np.random.randn(1000) + 1,\n        'b': np.random.randn(1000),\n        'c': np.random.randn(1000) - 1,\n    },\n    columns=['a', 'b', 'c'],\n)\n\nplt.figure()\n\ndf4.plot.hist(alpha=0.5)\n\n\n<Axes: ylabel='Frequency'>\n\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n\nCode\ndf4\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1.671477\n      1.027885\n      -3.028373\n    \n    \n      1\n      0.470194\n      -0.173525\n      -1.075715\n    \n    \n      2\n      2.049118\n      -1.589988\n      -1.445850\n    \n    \n      3\n      0.980754\n      0.660612\n      -0.560864\n    \n    \n      4\n      0.837670\n      0.339101\n      -2.250989\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      1.192872\n      1.697013\n      -0.426873\n    \n    \n      996\n      1.221444\n      1.394927\n      -1.229346\n    \n    \n      997\n      0.235303\n      1.104347\n      -1.535131\n    \n    \n      998\n      1.269441\n      -0.649671\n      -1.079828\n    \n    \n      999\n      1.111246\n      0.568459\n      -1.436033\n    \n  \n\n1000 rows × 3 columns\n\n\n\n\n\nCode\ndf4.plot(kind='hist', alpha=0.5, x='a')\nplt.show()\n\n\n\n\n\n\n\nCode\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\n\n\nBoxplot\n\n\nCode\ndf\n\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n      value2\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -1.361025\n      -2.722050\n    \n    \n      1\n      2000-01-04\n      A\n      1.124727\n      2.249455\n    \n    \n      2\n      2000-01-05\n      A\n      0.187734\n      0.375469\n    \n    \n      3\n      2000-01-03\n      B\n      1.221447\n      2.442895\n    \n    \n      4\n      2000-01-04\n      B\n      -0.645248\n      -1.290496\n    \n    \n      5\n      2000-01-05\n      B\n      0.368883\n      0.737767\n    \n    \n      6\n      2000-01-03\n      C\n      1.550405\n      3.100810\n    \n    \n      7\n      2000-01-04\n      C\n      -1.529291\n      -3.058582\n    \n    \n      8\n      2000-01-05\n      C\n      -1.041943\n      -2.083886\n    \n    \n      9\n      2000-01-03\n      D\n      -0.250513\n      -0.501025\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224425\n      -0.448850\n    \n    \n      11\n      2000-01-05\n      D\n      0.763475\n      1.526951\n    \n  \n\n\n\n\n\n\nCode\ndf = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\n\ndf.plot.box()\n\n\n<Axes: >\n\n\n\n\n\n\n\nCode\ndf = pd.DataFrame(np.random.rand(10, 2), columns=['Col1', 'Col2'])\n\ndf['X'] = pd.Series(['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'])\n\ndf\n\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      X\n    \n  \n  \n    \n      0\n      0.321042\n      0.232036\n      A\n    \n    \n      1\n      0.633293\n      0.236471\n      A\n    \n    \n      2\n      0.243009\n      0.816254\n      A\n    \n    \n      3\n      0.411629\n      0.909193\n      A\n    \n    \n      4\n      0.242503\n      0.770808\n      A\n    \n    \n      5\n      0.920928\n      0.096246\n      B\n    \n    \n      6\n      0.803955\n      0.660160\n      B\n    \n    \n      7\n      0.015313\n      0.467449\n      B\n    \n    \n      8\n      0.606109\n      0.470333\n      B\n    \n    \n      9\n      0.875516\n      0.039099\n      B\n    \n  \n\n\n\n\n\n\nCode\nplt.figure()\n\nbp = df.boxplot(by='X')\n\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#dataframe-연산",
    "href": "Data_Mining/pandas/pandas.html#dataframe-연산",
    "title": "Pandas 기본",
    "section": "DataFrame 연산",
    "text": "DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어봅시다.\n\n\nCode\ngrades_array = np.array([[8, 8, 9], [10, 9, 9], [4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array,\n                      columns=['sep', 'oct', 'nov'],\n                      index=['alice', 'bob', 'charles', 'darwin'])\ngrades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다.\n\n\nCode\nnp.sqrt(grades)\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      2.828427\n      2.828427\n      3.000000\n    \n    \n      bob\n      3.162278\n      3.000000\n      3.000000\n    \n    \n      charles\n      2.000000\n      2.828427\n      1.414214\n    \n    \n      darwin\n      3.000000\n      3.162278\n      3.162278\n    \n  \n\n\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다.\n\n\nCode\ngrades + 1\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      9\n      9\n      10\n    \n    \n      bob\n      11\n      10\n      10\n    \n    \n      charles\n      5\n      9\n      3\n    \n    \n      darwin\n      10\n      11\n      11\n    \n  \n\n\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다.\n\n\nCode\ngrades >= 5\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      True\n      True\n      True\n    \n    \n      bob\n      True\n      True\n      True\n    \n    \n      charles\n      False\n      True\n      False\n    \n    \n      darwin\n      True\n      True\n      True\n    \n  \n\n\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다.\n\n\nCode\ngrades.mean()\n\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nall 메서드도 집계 연산입니다. 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 봅시다.\n\n\nCode\n(grades > 5).all()\n\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\n이러한 함수의 대부분은 작업을 실행할 DataFrame의 축을 따라 지정할 수 있는 선택적인 axis 매개 변수를 사용합니다. 기본값은 axis = 0이며, 이는 각 열에서 작업이 수직으로 실행됨을 의미합니다. axis = 1을 설정하여 작업을 수평으로(각 행에서) 실행할 수 있습니다. 예를 들어, 어떤 학생들이 5 이상의 모든 성적을 받았는지 알아보겠습니다.\n\n\nCode\ngrades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n\nCode\n(grades > 5).all(axis=1)\n\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아봅시다.\n\n\nCode\n(grades == 10).any(axis=1)\n\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼봅시다.\n\n\nCode\ngrades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n\nCode\ngrades.mean()\n\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\n\n\nCode\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.25\n      -0.75\n      1.5\n    \n    \n      bob\n      2.25\n      0.25\n      1.5\n    \n    \n      charles\n      -3.75\n      -0.75\n      -5.5\n    \n    \n      darwin\n      1.25\n      1.25\n      2.5\n    \n  \n\n\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다.\n\n\nCode\npd.DataFrame([[7.75, 8.75, 7.50]]*4,\n             index=grades.index, columns=grades.columns)\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      7.75\n      8.75\n      7.5\n    \n    \n      bob\n      7.75\n      8.75\n      7.5\n    \n    \n      charles\n      7.75\n      8.75\n      7.5\n    \n    \n      darwin\n      7.75\n      8.75\n      7.5\n    \n  \n\n\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다.\n\n\nCode\ngrades.values.mean()\n\n\n8.0\n\n\n\n\nCode\ngrades - grades.values.mean()  # 모든 점수에서 전체 평균(8.00)을 뺍니다.\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      1.0\n    \n    \n      bob\n      2.0\n      1.0\n      1.0\n    \n    \n      charles\n      -4.0\n      0.0\n      -6.0\n    \n    \n      darwin\n      1.0\n      2.0\n      2.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#자동-정렬-1",
    "href": "Data_Mining/pandas/pandas.html#자동-정렬-1",
    "title": "Pandas 기본",
    "section": "자동 정렬",
    "text": "자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다.\n\n\nCode\ngrades_array = np.array([[8, 8, 9], [10, 9, 9], [4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=['sep', 'oct', 'nov'],\n                      index=['alice', 'bob', 'charles', 'darwin'])\ngrades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n\nCode\nbonus_array = np.array([[0, np.nan, 2], [np.nan, 1, 0], [0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=['oct', 'nov', 'dec'],\n                            index=['bob', 'colin', 'darwin', 'charles'])\nbonus_points\n\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\n\nCode\ngrades + bonus_points\n\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      bob\n      NaN\n      NaN\n      9.0\n      NaN\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#누락된-데이터-다루기",
    "href": "Data_Mining/pandas/pandas.html#누락된-데이터-다루기",
    "title": "Pandas 기본",
    "section": "누락된 데이터 다루기",
    "text": "누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해봅시다. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다.\n\n\nCode\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      bob\n      0.0\n      0.0\n      9.0\n      0.0\n    \n    \n      charles\n      0.0\n      5.0\n      11.0\n      0.0\n    \n    \n      colin\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      darwin\n      0.0\n      11.0\n      10.0\n      0.0\n    \n  \n\n\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다.\n\n\nCode\nbonus_points\n\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\n\nCode\nfixed_bonus_points = bonus_points.fillna(0)  # NA 값 0으로 바꾸기\nfixed_bonus_points.insert(loc=0, column='sep', value=0)  # 누락된 컬럼 만들기\nfixed_bonus_points.loc['alice'] = 0  # 누락된 행 만들기\nfixed_bonus_points\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0\n      0.0\n      0.0\n      2.0\n    \n    \n      colin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      darwin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\n\nCode\ngrades + fixed_bonus_points\n\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      9.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n훨씬 좋은 결과입니다. 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 봅시다.\n\n\nCode\nbonus_points\n\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\ninterpolate 메서드를 사용해봅시다. 기본적으로 수직 방향(axis = 0)으로 보간합니다. 따라서 수평으로(axis = 1)으로 보간하도록 지정합니다.\n\n\nCode\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\n\n\nCode\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, 'sep', 0)\nbetter_bonus_points.loc['alice'] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      0.0\n      0.5\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0.0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해봅시다.\n\n\nCode\ngrades + better_bonus_points\n\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다.(grade DataFrame에는 'dec' 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다.\n\n\nCode\ngrades['dec'] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다. dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다.\n\n\nCode\nfinal_grades_clean = final_grades.dropna(how='all')\nfinal_grades_clean\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n그 다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다.\n\n\nCode\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how='all')\nfinal_grades_clean\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#groupby로-집계하기",
    "href": "Data_Mining/pandas/pandas.html#groupby로-집계하기",
    "title": "Pandas 기본",
    "section": "groupby로 집계하기",
    "text": "groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다.\n\n\nCode\nfinal_grades['hobby'] = ['Biking', 'Dancing', np.nan, 'Dancing', 'Biking']\nfinal_grades\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n      hobby\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n      Biking\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n      Dancing\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n      Dancing\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n      Biking\n    \n  \n\n\n\n\nhobby로 이 DataFrame을 그룹핑해봅시다.\n\n\nCode\ngrouped_grades = final_grades.groupby('hobby')\ngrouped_grades\n\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020EF9ED52D0>\n\n\n이제 hobby마다 평균 점수를 계산할 수 있습니다.\n\n\nCode\ngrouped_grades.mean()\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n\n\nCode\nfinal_grades.groupby('hobby').mean()\n\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#피봇-테이블",
    "href": "Data_Mining/pandas/pandas.html#피봇-테이블",
    "title": "Pandas 기본",
    "section": "피봇 테이블",
    "text": "피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어봅시다.\n\n\nCode\nbonus_points.stack().reset_index()\n\n\n\n\n\n\n  \n    \n      \n      level_0\n      level_1\n      0\n    \n  \n  \n    \n      0\n      bob\n      oct\n      0.0\n    \n    \n      1\n      bob\n      dec\n      2.0\n    \n    \n      2\n      colin\n      nov\n      1.0\n    \n    \n      3\n      colin\n      dec\n      0.0\n    \n    \n      4\n      darwin\n      oct\n      0.0\n    \n    \n      5\n      darwin\n      nov\n      1.0\n    \n    \n      6\n      darwin\n      dec\n      0.0\n    \n    \n      7\n      charles\n      oct\n      3.0\n    \n    \n      8\n      charles\n      nov\n      3.0\n    \n    \n      9\n      charles\n      dec\n      0.0\n    \n  \n\n\n\n\n\n\nCode\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = ['name', 'month', 'grade']\nmore_grades['bonus'] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n\n  \n    \n      \n      name\n      month\n      grade\n      bonus\n    \n  \n  \n    \n      0\n      alice\n      sep\n      8.0\n      NaN\n    \n    \n      1\n      alice\n      oct\n      8.0\n      NaN\n    \n    \n      2\n      alice\n      nov\n      9.0\n      NaN\n    \n    \n      3\n      bob\n      sep\n      10.0\n      0.0\n    \n    \n      4\n      bob\n      oct\n      9.0\n      NaN\n    \n    \n      5\n      bob\n      nov\n      10.0\n      2.0\n    \n    \n      6\n      charles\n      sep\n      4.0\n      3.0\n    \n    \n      7\n      charles\n      oct\n      11.0\n      3.0\n    \n    \n      8\n      charles\n      nov\n      5.0\n      0.0\n    \n    \n      9\n      darwin\n      sep\n      9.0\n      0.0\n    \n    \n      10\n      darwin\n      oct\n      10.0\n      1.0\n    \n    \n      11\n      darwin\n      nov\n      11.0\n      0.0\n    \n  \n\n\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다.\n\n\nCode\npd.pivot_table(more_grades, index='name', values='grade')\n\n\n\n\n\n\n  \n    \n      \n      grade\n    \n    \n      name\n      \n    \n  \n  \n    \n      alice\n      8.333333\n    \n    \n      bob\n      9.666667\n    \n    \n      charles\n      6.666667\n    \n    \n      darwin\n      10.000000\n    \n  \n\n\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다.\n\n\nCode\npd.pivot_table(more_grades, index='name', values=[\n               'grade', 'bonus'], aggfunc=np.max)\n\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n    \n    \n      bob\n      2.0\n      10.0\n    \n    \n      charles\n      3.0\n      11.0\n    \n    \n      darwin\n      1.0\n      11.0\n    \n  \n\n\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다.\n\n\nCode\npd.pivot_table(more_grades, index='name', values='grade',\n               columns='month', margins=True)\n\n\n\n\n\n\n  \n    \n      month\n      nov\n      oct\n      sep\n      All\n    \n    \n      name\n      \n      \n      \n      \n    \n  \n  \n    \n      alice\n      9.00\n      8.0\n      8.00\n      8.333333\n    \n    \n      bob\n      10.00\n      9.0\n      10.00\n      9.666667\n    \n    \n      charles\n      5.00\n      11.0\n      4.00\n      6.666667\n    \n    \n      darwin\n      11.00\n      10.0\n      9.00\n      10.000000\n    \n    \n      All\n      8.75\n      9.5\n      7.75\n      8.666667\n    \n  \n\n\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다.\n\n\nCode\npd.pivot_table(more_grades, index=('name', 'month'), margins=True)\n\n\n\n\n\n\n  \n    \n      \n      \n      bonus\n      grade\n    \n    \n      name\n      month\n      \n      \n    \n  \n  \n    \n      alice\n      nov\n      NaN\n      9.00\n    \n    \n      oct\n      NaN\n      8.00\n    \n    \n      sep\n      NaN\n      8.00\n    \n    \n      bob\n      nov\n      2.000\n      10.00\n    \n    \n      oct\n      NaN\n      9.00\n    \n    \n      sep\n      0.000\n      10.00\n    \n    \n      charles\n      nov\n      0.000\n      5.00\n    \n    \n      oct\n      3.000\n      11.00\n    \n    \n      sep\n      3.000\n      4.00\n    \n    \n      darwin\n      nov\n      0.000\n      11.00\n    \n    \n      oct\n      1.000\n      10.00\n    \n    \n      sep\n      0.000\n      9.00\n    \n    \n      All\n      \n      1.125\n      8.75"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#함수",
    "href": "Data_Mining/pandas/pandas.html#함수",
    "title": "Pandas 기본",
    "section": "함수",
    "text": "함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다.\n\n\nCode\nmuch_data = np.fromfunction(lambda x, y: (x+y*y) % 17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3, 'some_text', 'Blabla')\nlarge_df\n\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      NaN\n      NaN\n      33.0\n      Blabla\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n      ...\n      NaN\n      NaN\n      NaN\n      33.0\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n    \n    \n      9996\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      9997\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n10000 rows × 27 columns\n\n\n\nhead() 메서드는 처음 5개 행을 반환합니다.\n\n\nCode\nlarge_df.head(n=10)\n\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      5\n      55.0\n      66.0\n      99.0\n      Blabla\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n      ...\n      66.0\n      55.0\n      66.0\n      99.0\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n    \n    \n      6\n      66.0\n      77.0\n      110.0\n      Blabla\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n      ...\n      77.0\n      66.0\n      77.0\n      110.0\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n    \n    \n      7\n      77.0\n      88.0\n      121.0\n      Blabla\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n      ...\n      88.0\n      77.0\n      88.0\n      121.0\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n    \n    \n      8\n      88.0\n      99.0\n      132.0\n      Blabla\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n      ...\n      99.0\n      88.0\n      99.0\n      132.0\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n    \n    \n      9\n      99.0\n      110.0\n      143.0\n      Blabla\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n      ...\n      110.0\n      99.0\n      110.0\n      143.0\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n    \n  \n\n10 rows × 27 columns\n\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다.\n\n\nCode\nlarge_df.tail(n=2)\n\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n2 rows × 27 columns\n\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다.\n\n\nCode\nlarge_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다. * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\n\n\nCode\nlarge_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n      F\n      G\n      H\n      I\n      J\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      count\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n      8823.000000\n      ...\n      8824.000000\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n    \n    \n      mean\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n      88.022441\n      ...\n      87.972575\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n    \n    \n      std\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n      47.535911\n      ...\n      47.535523\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n    \n    \n      min\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      ...\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n    \n    \n      25%\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      ...\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n    \n    \n      50%\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      ...\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n    \n    \n      75%\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      ...\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n    \n    \n      max\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      ...\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n    \n  \n\n8 rows × 26 columns"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#저장",
    "href": "Data_Mining/pandas/pandas.html#저장",
    "title": "Pandas 기본",
    "section": "저장",
    "text": "저장\nCSV, HTML, JSON로 저장해봅시다.\n\n\nCode\nmy_df.to_csv('my_df.csv')\nmy_df.to_html('my_df.html')\nmy_df.to_json('my_df.json')\n\n\n저장된 내용을 확인해봅시다.\n\n\nCode\nfor filename in ('my_df.csv', 'my_df.html', 'my_df.json'):\n    print('#', filename)\n    with open(filename, 'rt') as f:\n        print(f.read())\n        print()\n\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hobby</th>\n      <th>weight</th>\n      <th>birthyear</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alice</th>\n      <td>Biking</td>\n      <td>68.5</td>\n      <td>1985</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bob</th>\n      <td>Dancing</td>\n      <td>83.1</td>\n      <td>1984</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다.\n\n\nCode\ntry:\n    my_df.to_excel('my_df.xlsx', sheet_name='People')\nexcept ImportError as e:\n    print(e)"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#로딩",
    "href": "Data_Mining/pandas/pandas.html#로딩",
    "title": "Pandas 기본",
    "section": "로딩",
    "text": "로딩\nCSV 파일을 DataFrame으로 로드해봅시다.\n\n\nCode\nmy_df_loaded = pd.read_csv('my_df.csv', index_col=0)\nmy_df_loaded\n\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      birthyear\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      68.5\n      1985\n      NaN\n    \n    \n      bob\n      Dancing\n      83.1\n      1984\n      3.0\n    \n  \n\n\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해봅시다.\n\n\nCode\nus_cities = None\ntry:\n    csv_url = 'https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv'\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n\n  \n    \n      \n      State\n      Population\n      lat\n      lon\n    \n    \n      City\n      \n      \n      \n      \n    \n  \n  \n    \n      Marysville\n      Washington\n      63269\n      48.051764\n      -122.177082\n    \n    \n      Perris\n      California\n      72326\n      33.782519\n      -117.228648\n    \n    \n      Cleveland\n      Ohio\n      390113\n      41.499320\n      -81.694361\n    \n    \n      Worcester\n      Massachusetts\n      182544\n      42.262593\n      -71.802293\n    \n    \n      Columbia\n      South Carolina\n      133358\n      34.000710\n      -81.034814\n    \n  \n\n\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#sql-조인",
    "href": "Data_Mining/pandas/pandas.html#sql-조인",
    "title": "Pandas 기본",
    "section": "SQL 조인",
    "text": "SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어봅시다.\n\n\nCode\ncity_loc = pd.DataFrame(\n    [\n        ['CA', 'San Francisco', 37.781334, -122.416728],\n        ['NY', 'New York', 40.705649, -74.008344],\n        ['FL', 'Miami', 25.791100, -80.320733],\n        ['OH', 'Cleveland', 41.473508, -81.739791],\n        ['UT', 'Salt Lake City', 40.755851, -111.896657]\n    ], columns=['state', 'city', 'lat', 'lng'])\ncity_loc\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\n\nCode\ncity_pop = pd.DataFrame(\n    [\n        [808976, 'San Francisco', 'California'],\n        [8363710, 'New York', 'New-York'],\n        [413201, 'Miami', 'Florida'],\n        [2242193, 'Houston', 'Texas']\n    ], index=[3, 4, 5, 6], columns=['population', 'city', 'state'])\ncity_pop\n\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해봅시다.\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on='city')\n\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n  \n\n\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how='outer'로 지정합니다.\n\n\nCode\nall_cities = pd.merge(left=city_loc, right=city_pop, on='city', how='outer')\nall_cities\n\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\n물론 LEFT OUTER JOIN은 how='left'로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how='right'는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면,\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on='city', how='right')\n\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n    \n      3\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193\n      Texas\n    \n  \n\n\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어,\n\n\nCode\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = ['population', 'name', 'state']\ncity_pop2\n\n\n\n\n\n\n  \n    \n      \n      population\n      name\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\n\nCode\npd.merge(left=city_loc, right=city_pop2, left_on='city', right_on='name')\n\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      name\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      San Francisco\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New York\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Miami\n      Florida"
  },
  {
    "objectID": "Data_Mining/pandas/pandas.html#연결",
    "href": "Data_Mining/pandas/pandas.html#연결",
    "title": "Pandas 기본",
    "section": "연결",
    "text": "연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다.\n\n\nCode\ncity_loc\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\n\nCode\ncity_pop\n\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\n\nCode\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다.\n\n\nCode\nresult_concat.loc[3]\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n  \n\n\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\n\n\nCode\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      5\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      6\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      7\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      8\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join='inner'로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다.\n\n\nCode\npd.concat([city_loc, city_pop], join='inner')\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n    \n    \n      1\n      NY\n      New York\n    \n    \n      2\n      FL\n      Miami\n    \n    \n      3\n      OH\n      Cleveland\n    \n    \n      4\n      UT\n      Salt Lake City\n    \n    \n      3\n      California\n      San Francisco\n    \n    \n      4\n      New-York\n      New York\n    \n    \n      5\n      Florida\n      Miami\n    \n    \n      6\n      Texas\n      Houston\n    \n  \n\n\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다.\n\n\nCode\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n      city\n      state\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      808976.0\n      San Francisco\n      California\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      8363710.0\n      New York\n      New-York\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      413201.0\n      Miami\n      Florida\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Houston\n      Texas\n    \n  \n\n\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해봅시다.\n\n\nCode\npd.concat([city_loc.set_index('city'), city_pop.set_index('city')], axis=1)\n\n\n\n\n\n\n  \n    \n      \n      state\n      lat\n      lng\n      population\n      state\n    \n    \n      city\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      San Francisco\n      CA\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      New York\n      NY\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      Miami\n      FL\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      Cleveland\n      OH\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      Salt Lake City\n      UT\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      Houston\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\n\n\nCode\npd.concat([city_loc, city_pop])\n\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "Data_Mining/seaborn.html",
    "href": "Data_Mining/seaborn.html",
    "title": "Seaborn 기본",
    "section": "",
    "text": "Seaborn 실습"
  },
  {
    "objectID": "Data_Mining/seaborn.html#load-data",
    "href": "Data_Mining/seaborn.html#load-data",
    "title": "Seaborn 기본",
    "section": "1.1 Load data",
    "text": "1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female"
  },
  {
    "objectID": "Data_Mining/seaborn.html#figure-and-axes",
    "href": "Data_Mining/seaborn.html#figure-and-axes",
    "title": "Seaborn 기본",
    "section": "1.2 Figure and Axes",
    "text": "1.2 Figure and Axes\n\nmatplotlib으로 도화지 figure를 깔고 축공간 axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\n\nCode\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/seaborn.html#plot-with-matplotlib",
    "href": "Data_Mining/seaborn.html#plot-with-matplotlib",
    "title": "Seaborn 기본",
    "section": "1.3 plot with matplotlib",
    "text": "1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다.\n\nf-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요\n\n\n\n\nCode\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n조금 더 간단히 그리는 방법 matplotlib는 기본적으로 Categorical 변수를 color로 바로 사용하지 못함\n\n\nCode\n# We transform text categorical variables into numerical variables\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\naxes[0].scatter(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", c=\"species_codes\", alpha=0.3)\n\n\n<matplotlib.collections.PathCollection at 0x28659ead930>"
  },
  {
    "objectID": "Data_Mining/seaborn.html#plot-with-seaborn",
    "href": "Data_Mining/seaborn.html#plot-with-seaborn",
    "title": "Seaborn 기본",
    "section": "1.4 Plot with seaborn",
    "text": "1.4 Plot with seaborn\n\n\nCode\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다."
  },
  {
    "objectID": "Data_Mining/seaborn.html#matplotlib-seaborn-seaborn-matplotlib",
    "href": "Data_Mining/seaborn.html#matplotlib-seaborn-seaborn-matplotlib",
    "title": "Seaborn 기본",
    "section": "1.5 matplotlib + seaborn & seaborn + matplotlib",
    "text": "1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n파이썬 코드는 다음과 같습니다.\n\n\n\nCode\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/seaborn.html#seaborn-seaborn-matplotlib",
    "href": "Data_Mining/seaborn.html#seaborn-seaborn-matplotlib",
    "title": "Seaborn 기본",
    "section": "1.6 seaborn + seaborn + matplotlib",
    "text": "1.6 seaborn + seaborn + matplotlib\n\n안 될 이유가 없습니다.\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "Data_Mining/seaborn.html#quiz",
    "href": "Data_Mining/seaborn.html#quiz",
    "title": "Seaborn 기본",
    "section": "Quiz",
    "text": "Quiz\n\nbill length를 10단위로 나눈 후, bill length에 따른 depth의 boxplot을 그려봅시다.\n\n\n\nCode\n# penguins['bill_length'] = (penguins['bill_length_mm'] // 10 * 10)\n# sns.boxplot(x = 'bill_length', y = 'bill_depth_mm', data = penguins)\n# plt.show()\n\npenguins['bill_length_10'] = (penguins['bill_length_mm'] // 10 * 10)\nsns.boxplot(x = 'bill_length_10', y = 'bill_depth_mm', data = penguins)\nsns.stripplot(x = 'bill_length_10', y = 'bill_depth_mm', data = penguins, color = 'black', size = 4)\n\nsns.set_style('whitegrid')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.scatterplot(x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'species', data = penguins, alpha = 0.3)\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.FacetGrid(penguins, col = 'species', hue = 'species', col_wrap = 3)\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()\n\n\n\n\n\n\n\nCode\npenguins['bill_length_group'] = pd.cut(penguins['bill_length_mm'], bins = 3,\n                                       labels = ['0~40', '40~50', '50~60'])\n\ng = sns.FacetGrid(penguins, col = 'species', col_wrap = 3)\ng.map(sns.boxplot, 'bill_length_group', 'bill_depth_mm', order = ['0~40', '40~50', '50~60'])\ng.map(sns.stripplot, 'bill_length_group', 'bill_depth_mm', color = 'black', size = 4, order = ['0~40', '40~50', '50~60'])\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()"
  },
  {
    "objectID": "Data_Mining.html",
    "href": "Data_Mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nUber Data를 활용한 분석\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nh3\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nPydeck 기본\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nPydeck\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nExercise1 : Your First Map\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nExercise2 : Coordinate Reference Systems\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nExercise3 : Interactive Maps\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nExercise4 : Manipulating Geospatial Data\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nExercise5 : Proximity Analysis\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial Analysis\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nGeopandas\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nMatplotlib 기본\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nSeaborn 기본\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nSeaborn\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nIncrease Loop Speed\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nPandas 기본\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nNumpy 기본\n\n\n\n\n\n\n\nPython\n\n\nCode\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Visualization/Color_Scale/Color_Scale.html",
    "href": "Data_Visualization/Color_Scale/Color_Scale.html",
    "title": "Color Scales",
    "section": "",
    "text": "Code\n# ColorBrewer palettes\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\nCode\n# Read csv File\nUS_census <- read.csv(\"../data/US_census.csv\")\nUS_regions <- read.csv(\"../data/US_regions.csv\")\n\nUS_census %>% names()\n#>  [1] \"state\"                                    \n#>  [2] \"name\"                                     \n#>  [3] \"FIPS\"                                     \n#>  [4] \"pop2010\"                                  \n#>  [5] \"pop2000\"                                  \n#>  [6] \"age_under_5\"                              \n#>  [7] \"age_under_18\"                             \n#>  [8] \"age_over_65\"                              \n#>  [9] \"female\"                                   \n#> [10] \"white\"                                    \n#> [11] \"black\"                                    \n#> [12] \"native\"                                   \n#> [13] \"asian\"                                    \n#> [14] \"pac_isl\"                                  \n#> [15] \"two_plus_races\"                           \n#> [16] \"hispanic\"                                 \n#> [17] \"white_not_hispanic\"                       \n#> [18] \"no_move_in_one_plus_year\"                 \n#> [19] \"foreign_born\"                             \n#> [20] \"foreign_spoken_at_home\"                   \n#> [21] \"hs_grad\"                                  \n#> [22] \"bachelors\"                                \n#> [23] \"veterans\"                                 \n#> [24] \"mean_work_travel\"                         \n#> [25] \"housing_units\"                            \n#> [26] \"home_ownership\"                           \n#> [27] \"housing_multi_unit\"                       \n#> [28] \"median_val_owner_occupied\"                \n#> [29] \"households\"                               \n#> [30] \"persons_per_household\"                    \n#> [31] \"per_capita_income\"                        \n#> [32] \"median_household_income\"                  \n#> [33] \"poverty\"                                  \n#> [34] \"private_nonfarm_establishments\"           \n#> [35] \"private_nonfarm_employment\"               \n#> [36] \"percent_change_private_nonfarm_employment\"\n#> [37] \"nonemployment_establishments\"             \n#> [38] \"firms\"                                    \n#> [39] \"black_owned_firms\"                        \n#> [40] \"native_owned_firms\"                       \n#> [41] \"asian_owned_firms\"                        \n#> [42] \"pac_isl_owned_firms\"                      \n#> [43] \"hispanic_owned_firms\"                     \n#> [44] \"women_owned_firms\"                        \n#> [45] \"manufacturer_shipments_2007\"              \n#> [46] \"mercent_whole_sales_2007\"                 \n#> [47] \"sales\"                                    \n#> [48] \"sales_per_capita\"                         \n#> [49] \"accommodation_food_service\"               \n#> [50] \"building_permits\"                         \n#> [51] \"fed_spending\"                             \n#> [52] \"area\"                                     \n#> [53] \"density\"\nUS_regions %>% names()\n#> [1] \"state\"     \"state_abr\" \"region\"    \"division\""
  },
  {
    "objectID": "Data_Visualization/Color_Scale/Color_Scale.html#make-color-vector-in-order-of-the-state",
    "href": "Data_Visualization/Color_Scale/Color_Scale.html#make-color-vector-in-order-of-the-state",
    "title": "Color Scales",
    "section": "Make color vector in order of the state",
    "text": "Make color vector in order of the state\n\n\nCode\n# 4개 지방의 색 지정\nregion_colors <- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\")\nstate_colors <- region_colors[as.numeric(popgrowth_df$region)]\n\n# 색상을 밝게 또는 어둡게 생성 (음수는 어둡게)\nstate_colors_dark <- colorspace::darken(state_colors, 0.4)\n\nggplot(popgrowth_df, aes(x = state, y = 100 * popgrowth, fill = region)) +\n    geom_col() +\n    scale_y_continuous(\n        name = \"population growth, 2000 to 2010\",\n        labels = scales::percent_format(scale = 1),\n        expand = c(0, 0)\n    ) +\n    scale_fill_manual(values = region_colors) +\n    coord_flip() +\n    theme_light() +\n    theme(\n        panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.text.y = element_text(size = 10, color = state_colors),\n        legend.position = c(.58, .68),\n        legend.background = element_rect(fill = \"#ffffffb0\")\n    )\n#> Warning: Vectorized input to `element_text()` is not officially supported.\n#> ℹ Results may be unexpected or may change in future versions of ggplot2."
  },
  {
    "objectID": "Data_Visualization/Color_Scale/Color_Scale.html#make-color-vector-in-order-of-the-city",
    "href": "Data_Visualization/Color_Scale/Color_Scale.html#make-color-vector-in-order-of-the-city",
    "title": "Color Scales",
    "section": "Make color vector in order of the city",
    "text": "Make color vector in order of the city\n\n\nCode\n# 4개 지역의 색 지정\nregion_colors <- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\")\n\n# ColorBrewer palettes\n# region_colors <- RColorBrewer::brewer.pal(4, 'Set2')\nstate_colors <- region_colors[as.numeric(kor_202202_use$시도)]\nstate_colors_dark <- colorspace::darken(state_colors, 0.4)\n\nggplot(popgrowth_df, aes(x = state, y = 100 * popgrowth, fill = region)) +\n    geom_col() +\n    scale_y_continuous(\n        name = \"총인구수, 2022년 2월\",\n        expand = c(0, 0),\n        labels = scales::comma\n    ) +\n    scale_x_discrete(aes(color = state_colors)) +\n    scale_color_manual(values = state_colors) +\n    scale_fill_manual(values = region_colors) +\n    coord_flip() +\n    theme_light() +\n    theme(\n        panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.text.y = element_text(size = 10, color = state_colors_dark),\n        legend.position = c(.58, .68),\n        legend.background = element_rect(fill = \"#ffffffb0\")\n    )\n#> Warning: Vectorized input to `element_text()` is not officially supported.\n#> ℹ Results may be unexpected or may change in future versions of ggplot2."
  },
  {
    "objectID": "Data_Visualization/Coordinate_systems_and_axes/Coordinate_systems_and_axes.html",
    "href": "Data_Visualization/Coordinate_systems_and_axes/Coordinate_systems_and_axes.html",
    "title": "Coordinate systems and axes",
    "section": "",
    "text": "Code\n# Read csv File\nncdc_normals <- read.csv(\"../data/ncdc_normals.csv\")\n\n# station_loc Dataframe 생성\nstation_loc <- data.frame(\n    station_id = c(\"USW00014819\", \"USC00042319\", \"USW00093107\", \"USW00012918\"),\n    location = c(\"Chicago\", \"Death Valley\", \"San Diego\", \"Houston\")\n)\nstation_loc\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n# Raw Data와 Inner Join\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by = \"station_id\")\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\" \n#>    location \n#> \"character\"\n\n# Date Type Convert : Character -> Date\ntemps_long$date <- temps_long$date %>% as.Date(\"%Y-%m-%d\")\n\n\n\n\n\n\nCode\n# Arrange multiple plots into a grid\nlibrary(cowplot) # plot_grid()\n\n# data_Houston 데이터프레임 생성\ndata_Houston <- temps_long %>% filter(location == \"Houston\")\ndata_Houston %>% head()\n#>    station_id month day temperature flag       date location\n#> 1 USW00012918     1   1        53.9    S 0000-01-01  Houston\n#> 2 USW00012918     1   2        53.8    S 0000-01-02  Houston\n#> 3 USW00012918     1   3        53.8    S 0000-01-03  Houston\n#> 4 USW00012918     1   4        53.8    S 0000-01-04  Houston\n#> 5 USW00012918     1   5        53.8    S 0000-01-05  Houston\n#> 6 USW00012918     1   6        53.7    S 0000-01-06  Houston\n\nggplot(data_Houston, aes(x = date, y = temperature)) +\n    geom_line(linewidth = 1, color = \"royalblue\")\n\n\n\n\n\nCode\n\n# X축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\") # 3달 간격 Date 생성\n\n# ggplot + 축 생성\ntemp_plot <- ggplot(data_Houston, aes(x = date, y = temperature)) +\n    geom_line(linewidth = 1, color = \"royalblue\") +\n    scale_x_date(\n        name = \"month\", breaks = break_date,\n        labels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\")\n    ) +\n    scale_y_continuous(name = \"temp\") + # limits = c(0, 100) -> y 범위 지정\n    # labs(title = 'Fig. 2.3', subtitle = 'Daily temperature normals') +\n    theme_light()\ntemp_plot\n\n\n\n\n\n\n\nCode\n# 2 subplots\nplot_ab <- plot_grid(temp_plot, temp_plot,\n    nrow = 1, rel_widths = c(1, 2), labels = c(\"a\", \"b\")\n)\nplot_ab\n\n\n\n\n\n\n\nCode\n# 3 subplots\nplot_abc <- plot_grid(plot_ab, temp_plot,\n    ncol = 1, rel_widths = c(1, 5, 1), labels = c(\"\", \"c\")\n)\nplot_abc\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggrepel) # geom_text_repel()\n\n# Read csv File\nUs_census <- read.csv(\"../data/US_census.csv\")\nUs_census %>% head(2)\n#>     state           name FIPS pop2010 pop2000 age_under_5 age_under_18\n#> 1 Alabama Autauga County 1001   54571   43671         6.6         26.8\n#> 2 Alabama Baldwin County 1003  182265  140415         6.1         23.0\n#>   age_over_65 female white black native asian pac_isl two_plus_races hispanic\n#> 1        12.0   51.3  78.5  17.7    0.4   0.9      NA            1.6      2.4\n#> 2        16.8   51.1  85.7   9.4    0.7   0.7      NA            1.5      4.4\n#>   white_not_hispanic no_move_in_one_plus_year foreign_born\n#> 1               77.2                     86.3          2.0\n#> 2               83.5                     83.0          3.6\n#>   foreign_spoken_at_home hs_grad bachelors veterans mean_work_travel\n#> 1                    3.7    85.3      21.7     5817             25.1\n#> 2                    5.5    87.6      26.8    20396             25.8\n#>   housing_units home_ownership housing_multi_unit median_val_owner_occupied\n#> 1         22135           77.5                7.2                    133900\n#> 2        104061           76.7               22.6                    177200\n#>   households persons_per_household per_capita_income median_household_income\n#> 1      19718                   2.7             24568                   53255\n#> 2      69476                   2.5             26469                   50147\n#>   poverty private_nonfarm_establishments private_nonfarm_employment\n#> 1    10.6                            877                      10628\n#> 2    12.2                           4812                      52233\n#>   percent_change_private_nonfarm_employment nonemployment_establishments firms\n#> 1                                      16.6                         2971  4067\n#> 2                                      17.4                        14175 19035\n#>   black_owned_firms native_owned_firms asian_owned_firms pac_isl_owned_firms\n#> 1              15.2                 NA               1.3                  NA\n#> 2               2.7                0.4               1.0                  NA\n#>   hispanic_owned_firms women_owned_firms manufacturer_shipments_2007\n#> 1                  0.7              31.7                          NA\n#> 2                  1.3              27.3                     1410273\n#>   mercent_whole_sales_2007   sales sales_per_capita accommodation_food_service\n#> 1                       NA  598175            12003                      88157\n#> 2                       NA 2966489            17166                     436955\n#>   building_permits fed_spending    area density\n#> 1              191       331142  594.44    91.8\n#> 2              696      1119082 1589.78   114.6\n\ntx_counties <- Us_census %>%\n    filter(state == \"Texas\") %>% # Texas 주만 추출\n    select(name, pop2010) %>% # name과 pop2010 열만 추출\n    mutate(\n        county = gsub(\" County\", \"\", name), # \" County\" 제거하여 county 열 생성\n        popratio = pop2010 / median(pop2010)\n    ) %>% # pop2010을 중앙값으로 나누어 비율 생성\n    arrange(desc(popratio)) %>% # 내림차순으로 정렬\n    mutate(\n        index = 1:n(), # index 열 생성\n        label = ifelse(index <= 3 | index > n() - 3 | runif(n()) < .04, county, \"\")\n    )\n# index 기준으로 상위 3위와 하위 3위인 열만 label을 지정\ntx_counties %>% head()\n#>             name pop2010  county  popratio index   label\n#> 1  Harris County 4092459  Harris 222.64616     1  Harris\n#> 2  Dallas County 2368139  Dallas 128.83624     2  Dallas\n#> 3 Tarrant County 1809034 Tarrant  98.41869     3 Tarrant\n#> 4   Bexar County 1714773   Bexar  93.29052     4        \n#> 5  Travis County 1024266  Travis  55.72417     5        \n#> 6 El Paso County  800647 El Paso  43.55840     6\n\n\n\n\nCode\n# Figure 3.6\nggplot(tx_counties, aes(x = index, y = popratio)) +\n    geom_hline(yintercept = 0, linetype = 2, color = \"grey40\") +\n    geom_point(size = 1, color = \"royalblue\") +\n    geom_text_repel(aes(label = label),\n        min.segment.length = 0,\n        max.overlaps = 100\n    ) +\n    theme_light() +\n    theme(panel.border = element_blank())\n\n\n\n\n\n\n\nCode\n# Figure 3.5\nlabel_log10 <- sapply(-2:2, function(i) as.expression(bquote(10^.(i))))\n\nggplot(tx_counties, aes(x = index, y = popratio)) +\n    geom_hline(yintercept = 0, linetype = 2, color = \"grey40\") + # 수평선 생성 (yintercept : y절편)\n    geom_point(size = 1, color = \"royalblue\") +\n    geom_text_repel(aes(label = label),\n        min.segment.length = 0, # label을 표시하기 위해 연결해야하는 최소한의 선분 길이\n        max.overlaps = 100\n    ) + # label 간의 최대 겹침 횟수\n    scale_y_log10(\n        name = \"population number / median\",\n        breaks = 10^(-2:2),\n        labels = label_log10\n    ) +\n    scale_x_continuous(\n        name = \"Texas counties, from most to leas populous\",\n        breaks = NULL\n    ) +\n    theme_light() +\n    theme(panel.border = element_blank())\n#> Warning: Transformation introduced infinite values in continuous y-axis\n\n\n\n\n\n\n\n\n\n\nCode\ndata_202202 <- read.csv(\"../data/행정구역_시군구_별_주민등록세대수_202302.csv\", encoding = \"UTF-8\")\ndata_202202 %>% head()\n#>   행정구역.시군구.별 X2022.11 X2022.12 X2023.01 X2023.02\n#> 1             종로구    72666    72524    72479    72773\n#> 2               중구    63167    63139    63123    63492\n#> 3             용산구   109905   109805   109734   109778\n#> 4             성동구   133435   133305   133293   133517\n#> 5             광진구   169376   169291   169416   169648\n#> 6           동대문구   170154   169873   169716   170766\n\nkor_census <- data_202202 %>%\n    filter(X2023.02 > 0) %>%\n    mutate(popratio = X2023.02 / median(X2023.02)) %>%\n    arrange(popratio %>% desc()) %>%\n    mutate(index = 1:n(), label = ifelse(index <= 5 | index > n() - 5 | index == median(index), 행정구역.시군구.별, \"\"))\n\nlabel_log10 <- sapply(-2:2, function(i) as.expression(bquote(10^.(i))))\n\nggplot(kor_census, aes(x = index, y = popratio)) +\n    geom_hline(yintercept = 1, linetype = 2, color = \"grey40\") +\n    geom_point(size = 1, color = \"royalblue\") +\n    geom_text_repel(aes(label = label),\n        min.segment.length = 0,\n        max.overlaps = 100\n    ) +\n    scale_y_log10(\n        name = \"인구 수 / 중위 수\",\n        breaks = 10^(-2:2),\n        labels = label_log10,\n        limits = c(10^-1.3, 10^1.3)\n    ) +\n    scale_x_discrete(\n        name = \"행정구역(시군구)별 주민등록세대수\",\n        breaks = NULL\n    ) +\n    theme_light() +\n    theme(panel.border = element_blank())\n\n\n\n\n\n\n\n\n\n\nCode\n# X축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\") # 3달 간격 Date 생성\ndata_lab <- format(break_date, \"%B\")\n\nggplot(temps_long, aes(x = date, y = temperature, color = location)) +\n    geom_line(linewidth = 1.2) +\n    scale_x_date(name = \"month\", breaks = break_date, labels = data_lab) +\n    scale_y_continuous(name = \"temperature\", limits = c(0, 105)) +\n    coord_polar(theta = \"x\", start = pi, direction = -1) + # 6시 위치에서 반시계 방향\n    # coord_polar(theta = \"x\", start = 0, direction = 1) +    # 12시 위치에서 반시계 방향\n    theme_light() +\n    theme(panel.border = element_blank())\n\n\n\n\n\n\n\n\n\n\nCode\n# Read csv File\ndata_2022 <- read.csv(\"../data/OBS_ASOS_DD_20230322080932.csv\", fileEncoding = \"CP949\")\n\n# '대전', '서울', '세종', '제주'만 추출\ndata_2022 <- data_2022 %>% filter(지점명 %in% c(\"대전\", \"서울\", \"세종\", \"제주\"))\ndata_2022$지점명 %>% unique()\n#> [1] \"서울\" \"대전\" \"제주\" \"세종\"\n\n# Date Type Convert : Character -> Date\ndata_2022$일시 <- data_2022$일시 %>% as.Date(\"%Y-%m-%d\")\ndata_2022 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n# X축 눈금 지정\ndate_s <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"2023-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"2 month\") # 2달 간격 Date 생성\ndata_lab <- format(break_date, \"%B\")\n\nggplot(data_2022, aes(x = 일시, y = 평균기온..C., color = 지점명)) +\n    geom_line(linewidth = 1.2) +\n    scale_x_date(name = \"월\", breaks = break_date, labels = data_lab) +\n    scale_y_continuous(name = \"평균기온\", limits = c(-20, 30)) +\n    coord_polar(theta = \"x\", start = pi, direction = -1) + # 6시 위치에서 반시계 방향\n    theme_light() +\n    theme(panel.border = element_blank())"
  },
  {
    "objectID": "Data_Visualization/ggplot2_tutorial/ggplot2_tutorial.html",
    "href": "Data_Visualization/ggplot2_tutorial/ggplot2_tutorial.html",
    "title": "ggplot2 Tutorial",
    "section": "",
    "text": "ggplot2 실습\n\n\n\n데이터 시각화란 데이터를 그래프 등의 시각적 요소로 요약하여 보여주는 것을 의미한다. R에서는 데이터 시각화를 R의 기본 기능에 포함된 graphics 패키지를 사용하여 시각화하는 방법과 ggplot2패키지를 이용하는 방법이 있다. 이 장에서는 ggplot2를 이용하여 데이터를 시각화하는 기본적인 방법을 배운다. 여기서는 통계분석에 필요한 기본적인 그래프를 그리기 위한 기본적인 문법을 소개하는 것이지 ggplot2에 대한 체계적인 설명을 하지 않을 것이다. ggplot2는 자유로운 형식으로 그래프를 그릴 수 있는 그래프 문법을 가지고 있기 때문에, ggplot2에 대한 더 체계적인 이해를 원하는 독자는 졸저 ’R 프로그래밍’의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n\n\nR은 패키지란 단위로 R에서 사용할 수 있는 기능을 제공한다. R을 설치하면 base, stat, dataset, graphics 등의 기본 패키지가 자동으로 설치되고, R을 시작할 때마다 이러한 기본 패키지가 자동으로 적대되어 사용될 수 있도록 준비된다. 만약 R에서 기본으로 제공하는 패키지 말고 다른 패키지를 사용하려면 그 패키지를 R에 설치해야 한다. ggplot2 패키지는 기본 기능에 포함되지 않으므로 먼저 설치를 해야 한다.\nggplot2 패키지를 설치하려면 다음 명령을 실행하면 된다. 패키지의 이름은 문자열이므로 따옴표 안에 기술해야 한다.\n\n\nCode\n# install.packages('ggplot2')\n\n\n또는 RStudio의 우측 하단의 Packages 탭에서 [Install]을 클릭한 후 ggplot2라고 입력을 하면 된다. 패키지 설치는 한 번만 수행하면 된다.\n패키지를 사용하려면 메모리에 적재를 하여야 한다. 패키지를 메모리에 적재하는 것은 library() 함수를 사용한다. 이 때 주의할 점은 이미 설치된 패키지를 지정할 때는 따옴표 없이 변수처럼 패키지를 기술해야 한다는 것이다.\n\n\nCode\nlibrary(ggplot2)\n\n\n패키지의 설치는 한 번만 수행하면 되지만, 패키지를 메모리에 적재하는 작업을 패키지를 사용할 때마다 수행하여야 한다. 한번 메모리에 적재된 패키지는 R 세션이 종료되기 전까지 유지된다. 그러므로 하나의 R 세션에서는 다시 library() 함수로 동일한 패키지를 적재하지 않아도 된다. 그러나 R 세션을 종료하고 다시 시작하였다면, 기본 패키지가 아니면 자동 적재되지 않으므로 사용하기 전에 패키지를 다시 적재하여야 한다.\n\n\n\n이 절에서는 ggplot2에서 제공하는 mpg 데이터를 이용하여 ‘배기량이 커지면 연비가 낮아지는가?’ 라는 물음을 그래프를 이용하여 탐색해 보자. mpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다.\n다음 명령을 이용하여 mpg 데이터를 출력해 보자. mpg 데이터는 tibble이라는 데이터 프레임의 일종으로, 사용자의 화면의 크기에 따라 출력 내용을 조정한다. 그러므로 화면의 크기에 따라 출력되는 내용이 책과는 조금 다를 수 있다.\nYou can also embed plots, for example:\n\n\nCode\nmpg\n#> # A tibble: 234 × 11\n#>    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n#>    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n#>  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n#>  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n#>  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n#>  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n#>  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n#>  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n#>  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n#>  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n#>  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n#> 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n#> # … with 224 more rows\n\n\nmpg는 1999년과 2008년에 미국 EPA에서 조사하여 발표한 자동차 주요 모델별 연비 데이터이다. 데이터는 234 개의 행이 있으며, 각 행은 다음과 같은 변수로 구성되어 있다.\n\nmanufacturer : 자동차 제조사\nmodel : 자동차 모델명\ndispl : 자동차 배기량\nyear : 제조년도\ncyl : 엔진 실린더 수\ntrans : 자동차 트랜스미션 종류\ndrv : 자동차 구동 방식. f = 전륜구동, r = 후륜구동, 4 = 사륜구동\ncty : 도심 연비 (마일/갤론)\nhwy : 고속도로 연비 (마일/갤론)\nfl : 연료 종류\nclass : 자동차 분류\nmpg 데이터에 대한 더 자세한 설명은 콘솔에 다음을 입력하여 R 도움말을 참조하기 바란다.\n\n\n\nCode\n?mpg\n\n\n\n\nmpg 데이터로부터 배기량과 고속도로 연비의 관계를 살펴보기 위해서 배기량(displ)을 x 축으로, 고속도로 연비(hwy)를 y 축으로 하는 산점도를 그려보자. 산점도에서 배기량이 커짐지면 연비가 줄어드는 경향을 관찰할 수 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n그러면 이 산점도 그린 ggplot2 명령어의 문법을 살펴보자.\n\nggplot2의 명령어는 항상 ggplot() 함수로 시작하고, + 연산자를 사용하여 그래프에 추가될 요소를 덧붙여 나간다. 이렇게 함수를 +로 연결하여 사용하는 방식은 ggplot2 패키지의 독특한 문법으로 대부분의 다른 R 명령어에서는 이러한 방식을 사용하지 않는다.\nggplot() 함수는 그래프의 좌표축과 좌표평면을 만드는 함수이다. 그러므로 다음처럼 ggplot() 함수만 사용하고 그래프에 추가할 요소를 지정하지 않으면 좌표축과 좌표평면만 그린다.\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy))\n\n\n\n\n\n\nggplot() 함수의 첫번째 인수는 그래프를 그릴 때 사용할 데이터를 지정하고, 두번째 인수는 그래프 속성과 데이터 열의 관계를 지정한다. 그래프 속성과 데이터 열의 관계는 항상 aes() 함수 내에 기술되고, 다음처럼 <그래프 속성>=<데이터 열>의 형식으로 기술된다.\n\n\n\nCode\nggplot(데이터, aes(속성1=열1, 속성2=열2, ...)) + geom함수()\n\n\n앞의 산점도에서는 x라는 그래프의 가로축 속성에 mpg 데이터의 배기량 열 displ이 매핑되었고, y라는 그래프의 세로축 속성에 고속도로 연비 열 hwy가 매핑되었다. 다음은 그래프의 가로축에 데이터의 도심 연비 열인 cty을 매핑하여 산점도를 그린 예이다. 도심 연비가 좋은 차가 고속도로 연비도 좋다는 것을 알 수 있다.\n\n\nCode\nggplot(mpg, aes(x = cty, y = hwy)) + geom_point()\n\n\n\n\n\n\nggplot() 함수에 +로 연결되는 geom 함수는 그래프에 그릴 도형을 지정한다. geom_point() 함수는 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 점(points)이라는 도형을 그린다. ggplot2에는 점을 그리는 geom_point() 함수뿐 아니라 다양한 도형을 그리는 geom 함수들이 있다. 만약 다음처럼 geom_point() 함수가 아니라 geom_smooth() 함수를 연결하면 점이 아니라 데이터의 추세선을 ggplot() 함수에 정의된 그래프 속성과 열의 관계를 이용하여 그래프에 그린다.\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) + geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nggplot() 함수에 여러 개의 geom 함수를 연결하여 두 개 이상의 그래픽적 도형을 그래프에 그릴 수 있다. 이 경우 먼저 기술된 geom 함수의 도형이 아래 층에 그려지고 뒤에 기술된 geom 함수의 도형이 윗 층에 그려진다.\n\n\n\n\n\n앞의 산점도에서 배기량에 따라 연비가 줄어드는 관계를 조금 벗어나는 관측치들이 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n\n이 예외적인 관측치들이 자동차 종류의 차이 때문에 발생했다, 라고 가설을 세웠다 하자. 이 가설을 확인해 보려면 자동차 종류별로 관측치를 시각화할 필요가 있다. 앞서 본 geom_point() 함수는 ’점’이라는 도형을 좌표평면 상에서 그린다. 점이라는 도형은 x-축의 위치(x)와 y-축의 위치(y)뿐 아니라 색상(color), 모양(shape), 크기(size), 투명도(alpha) 등의 다른 시각적 속성을 가지고 있다. 우리는 이러한 속성 중 하나에 mpg 데이터의 class 열을 대응시켜 자동차 종류 별로 좌표평면에서 시각적으로 구분되는 점으로 표현할 수 있다.\n\n\n다음은 관측치의 종류(class)에 따라 점을 서로 다른 색상(color)으로 표현한 예이다. 자동차의 종류에 따라 점이 다른 색상으로 표현되고, 어떤 색상이 어떤 자동차 종류에 대응되었는지에 대한 범례가 자동 생성된다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n\n앞선 그래프에서 이상치로 표현되었던 점들 중 한 점만 제외하고 모두 2seater 자동차의 관측치였음을 알 수 있다. 이 종류의 차는 스포츠카로 배기량에 비해 가벼운 몸체를 가지고 있어 예외적인 연비가 관측된 것으로 보인다.\n다음으로 class 열을 shape, size, alpha 등의 속성에 대응시켜 어떤 결과가 나오는지 살펴보자.\n\n\n\nshape 속성은 점의 모양을 결정한다. 다음은 앞의 산점도를 구동 방식(drv)에 따라 점의 모양이 다르게 표시한 예이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point()\n\n\n\n\n\n점의 모양과 색상을 하나의 데이터 열에 매핑하여 좀 더 데이터가 뚜렷이 구분되게 그래프를 그리기도 한다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = drv)) +\n  geom_point()\n\n\n\n\n\n물론 다음처럼 점의 색상과 모양을 각각 데이터의 다른 열에 매핑할 수도 있다. 다음은 점의 색은 자동차의 종류(class)에 모양은 자동차의 구동방식(drv)에 매핑한 결과이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, shape = drv, color = class)) +\n  geom_point()\n\n\n\n\n\nshape을 사용할 때 주의할 점은 shape은 최대 6개의 모양으로만 점을 구분하기 때문에 class 열처럼 6개보 많은 종류가 있는 열에 매핑되면 데이터가 제대로 표시가 되지 않는다. 다음 예처럼 shape 속성에 class 열을 매핑하니 경고가 나타나고 suv 데이터를 표시하지 못한 것을 확인할 수 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) + geom_point()\n#> Warning: The shape palette can deal with a maximum of 6 discrete values because\n#> more than 6 becomes difficult to discriminate; you have 7. Consider\n#> specifying shapes manually if you must have them.\n#> Warning: Removed 62 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n모양(shape) 속성은 소수의 구분되는 값으로 표현되는 범주형 변수를 표현하기 좋다. 데이터의 열이 연속형 변수이면 연속적인 값을 표현하기 좋은 가로축(x), 세로축(y), 크기(size), 투명도(alpha) 등을 이용하는 것이 좋다. 색상(color)은 범주형 변수와 연속형 변수에 모두 매핑될 수 있다. 범주형 변수로 매핑되면 구분되는 색상으로, 연속형 변수로 매핑되면 색상의 그라데이션으로 값을 표시한다.\n다음은 도심 연비와 고속도로 연비를 가로축과 세로축으로 하는 그래프에서 점의 크기 속성을 배기량 열에 매핑한 결과이다. 도심 연비와 고속도로 연비가 좋은 차들은 배기량이 작은 차임을 알 수 있다.\n\n\nCode\nggplot(mpg, aes(x = cty, y = hwy, size = displ)) + geom_point()\n\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 색상을 배기량 열에 매핑한 결과이다. 범주형 변수가 매핑될 때와는 달리 색상의 연속적인 변화인 그라데이션을 사용하여 배기량을 표현하고 있음을 볼 수 있다.\n\n\nCode\nggplot(mpg, aes(x = cty, y = hwy, color = displ)) + geom_point()\n\n\n\n\n\n다음은 동일한 도심 연비와 고속도로 연비 산점도에서 그래프에서 점의 투명도를 실린더 수 열에 매핑한 결과이다.\n\n\nCode\nggplot(mpg, aes(x = cty, y = hwy, alpha = cyl)) + geom_point()\n\n\n\n\n\n\n\n\n\nggplot은 매우 강력한 기능을 가지고 있지만 Excel 등의 GUI 프로그램에만 익숙한 사람은 문자 기반 명령어를 입력하는 것에 어려움을 느낄 수 있다. 컴퓨터는 사람만큼의 유연성을 발휘하지 못하므로 컴퓨터는 자신이 실행해야 할 명령문의 문법에 매우 까다롭게 반응한다. ggplot 명령어 입력시 흔히 발생하는 문제들은 다음과 같다.\n\nR 명령문은 대소문자를 구분한다. 따라서 Color와 color는 ggplot에서 서로 다른 인수로 인식되어 오류가 발생한다.\nggplot 명령문의 키워드의 철자가 틀리면 다른 키워드로 간주하기 때문에 오류가 발생할 수 있다. 이를 방지하려면 키워드의 일부만 입력한 후 Tab 키를 눌러 자동완성 기능을 사용하여 입력하는 것을 권장한다.\nggplot2의 명령문을 입력할 때 여러 함수를 합쳐서 실행하기 위하여 + 연산자를 이용한다. ggplot2의 명령문이 길어지면 명령문을 여러 줄로 쓰는 것이 필요한데, 보통 +로 연결되는 곳에서 줄바꿈하는 것이 읽기에 좋다. 이 때 주의할 점이, 줄바꿈을 + 앞이 아니라 뒤에서 해야 한다는 것이다. + 앞에서 하면 R은 명령문의 입력이 완성된 것으로 간주하기 때문이다.\n\n다음은 산점도와 추세선을 한 그래프에 그린 예이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n그런데 위의 명령어는 길기 때문에 스크립트 파일을 작성할 때 보기에 불편하다. 이러한 경우에 위의 명령은 다음처럼 세 줄로 나누어 기술될 수 있다. 세 함수를 연결하는 + 위치가 어디에 있는지 살펴보라. (다음 예에서 왼쪽의 > 프롬프트 아래 있는 +는 R 콘솔에서 명령문이 계속되고 있음을 나타내는 표시이다. 이 표시와 사용자가 입력한 +를 혼동하면 안 된다.)\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n만약 다음처럼 + 위치가 잘못되면 오류가 발생한다. 왜 이런 결과가 나왔고 오류 메시지의 의미는 무엇일까? R은 Enter로 명령문을 구분한다. 그러므로 첫번째 줄은 +가 없으므로 완벽한 명령문이기 입력된 것으로 간주하고 실행이되어 좌표평면만 그린 것이다. 그러고 나서 두번째 줄을 새로운 명령문으로 실행을 한다. 그런데 갑자기 명령문이 +로 시작하니 R은 명령문에 오류가 있다고 판단한다. 왜냐하면 + 연산은 왼편과 오른편에 더할 요소가 있어야 하는데, 왼편의 요소가 기술되지 않았기 때문이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = drv))\n+ geom_point()\n\n\n\nR 명령문이 조금 길어지면 가장 흔하게 발생하는 실수가 ( )와 \" \"을 짝을 맞추어 제대로 입력하지 못하는 것이다. ggplot2의 명령문도 많은 함수를 사용하다 보니 이를 주의하여야 한다. 이러한 실수를 하게 되면면 R 콘솔은 명령이 계속 입력 중이라고 생각하여 >가 아니라 +를 콘솔의 프롬프트로 표시한다. 이 경우 가장 간단한 해결책은 Esc 키를 눌러 명령 입력에서 빠져나와 다시 명령문을 입력하는 것이다.\n\n\n\n\nggplot2의 장점은 필요에 따라 다양한 형식의 그래프를 쉽게 만들 수 있고, 만들 수 있는 형식도 무궁무진하다는데 있다. 그리고 ggplot2 그래프의 계층적 구조가 이러한 무궁무진한 그래프 형식을 만들어 내는 핵심 요소라 할 수 있다. ggplot2는 좌표평면 위에 여러 계층으로 그래프를 겹쳐 그려서 하나의 좌표평면에 나타냄으로써 복잡한 형식의 그래프를 만들어 낼 수 있다.\n다음 그래프는 배기량과 고속도로 연비의 산점도와 추세선을 한 그래프에 그렸다. ggplot() 함수에 지정한 데이터와 그래프 속성과 데이터 열 매핑이 산점도(geom_point())와 추세선(geom_smooth())에 모두 동일하게 정의되었음을 볼 수 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nggplot() 함수가 여러 개의 geom 함수와 연결되면, 하나의 좌표평면에 각각의 geom() 함수의 결과를 층층이 그린다. 이 때, 명령문에 나타나는 순서에 따라 첫번째 나온 geom 함수의 도형이 가장 아래 계층에, 다음에 나오는 geom 함수의 도형이 차례로 그 윗 계층에 그려진다.\n\n\n앞의 배기량과 고속도로 연비의 산점도와 추세선을 그린 그래프에서 추세선을 선 종류(linttype)가 구동 방식(drv)에 따라 다르게 표현하고 싶다. 그런데 산점도는 점이라는 도형으로 그래프를 그리므로 선 종류라는 속성을 가지고 있지 않다. 그리고 산점도도 점의 모양(shape)이 구동 방식에 따라 다르게 표현하고 싶다고 하자. 마찬가지로 추세선은 선이라는 도형으로 그래프를 그리므로 점의 모양이라는 속성을 가지고 있지 않다.\n이렇듯 여러 geom 함수를 연결하여 그래프를 그릴 때, 특정 geom 함수에만 해당하는 속성은 해당 geom 함수에서 속성과 데이터 열을 매핑하는 것이 좋다. geom 함수도 ggplot() 함수처럼 aes() 함수를 이용하여 그래프 속성과 데이터 열을 매핑하는데, 이 매핑이 geom 함수의 첫 번째 인수로 기술된다는 점만 다르다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(aes(shape = drv)) +\n  geom_smooth(aes(linetype = drv))\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n따라서 지금까지 배운 내용으로 ggplot2 그래프를 그리는 문법을 확장하면 다음과 같다.\n\n\nCode\nggplot(데이터, aes(공통속성1=열1, 공통속성2=열2, ...)) + \n  geom함수1(aes(geom함수1의 속성1=열1, geom함수1의 속성2=열2, ...)) + \n  geom함수2(aes(geom함수2의 속성1=열1, geom함수2의 속성2=열2, ...)) +\n  ....\n\n\n확장된 문법으로 맨처음 그린 배기량과 고속도로 연비의 산점도와 추세선 그래프에서, 산점도의 점은 구동 방식에 따라 다른 색으로 표시하지만, 추세선은 모든 데이터에 대하여 하나만 그리려면 어떻게 해야 할까? 답은 다음처럼 색상 속성을 공통 속성으로 ggplot()에 매핑하지 않고 산점도만의 속성 매핑이 되도록 geom_point()에 기술하는 것이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n마찬가지로 추세선을 구동 방식에 따라 다른 색상으로 표시하나 점은 모두 동일한 색으로 표시하고 싶으면 다음처럼 색상이 추세선만의 속성 매핑이 되도록 geom_smooth()에 기술하는 것이다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(aes(color = drv))\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n또한 ggplot() 함수에 데이터와 도형 속성에 대한 매핑이 되어 있어도, geom 함수에서 데이터와 도형 속성의 매핑을 재지정할 수도 있다. 이 경우 각 geom 함수에서 사용하는 data와 mapping은 다음 규칙에 의해 결정된다.\n\ngeom 함수는 ggplot() 함수에 설정된 data와 mapping을 상속받아 그래프를 그린다.\n만약 geom 함수에 data 인수가 설정되면 ggplot() 함수에 설정된 data는 무시된다.\n만약 geom 함수에 mapping 인수가 설정되면 ggplot() 함수에 설정된 mapping에 geom 함수에 설정된 mapping이 추가된다. 만약 동일한 도형 속성에 대한 정의가 두 군데 나타나면 geom 함수의 설정이 사용된다. 자세한 내용은 R 프로그래밍의 그래프 계층(layers)과 도형(geoms) 절을 참조하기 바란다.\n\n\n\n\n\n다음 그래프는 배기량과 고속도로 연비의 관계를 살펴보기 위하여 이 두 변수의 관계를 산점도로 살펴보고 나서, 이 두 변수의 관계가 자동차 종류에 따라 어떻게 달라지는지를 살펴보기 위해 그래프의 색상 속성을 자동차 종류를 나타내는 열에 매핑하여 다르게 표시되도록 하였다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point() \n\n\n\n\n\n이렇듯 두 변수의 관계를 제삼의 변수 관점에서 세분화하여 살펴보는 방법으로 제삼의 변수를 그래프 속성에 매핑하는 방법 말고도 제삼의 변수의 변수값에 따라 데이터를 별도의 그래프로 나누어 그려보는 방법이 있다. ggplot2에서는 이러한 방식을 측면(facets)으로 나누어 그래프를 그린다고 한다.\n\n\n다음은 facet_wrap() 함수의 사용법을 보여준다. ~ 은 R에서 수식을 표현할 때 사용되는데, facet_wrap() 함수는 수식을 첫 번재 인수로 입력받는다. facet_wrap() 함수는 ~ 우변에 서술된 변수의 변수값 별로 데이터를 나누어 그래프를 각각 그린다. 이 때 측면(facets)을 지정하는데 사용되는 변수는 범주형 데이터이어야 한다. facet_wrap()은 측면 그래프가 많아지면 줄바꿈하여 그래프를 표시한다. nrow나 ncol을 설정하면 그래프의 행과 열의 수를 지정하여 줄바꿈 처리를 제어할 수 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\n\n측면으로 나누어 그려진 그래프는 서로 비교가 용이하도록 동일한 좌표축으로 그려진다. 측면 그래프의 상단에는 어떤 측면의 데이터에 대한 그래프인지를 표시한다. 맨 처음 측면 그래프는 2seater 측면에서 배기량과 고속도로 연비의 산점도를 보여주고, 맨 마지막 측면 그래프는 SUV 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n두 개 이상의 변수를 조합하여 측면 그래프을 만드려면 다음처럼 수식의 우변에 두 개의 변수를 +로 연결하여 기술하면 된다. 다음은 구동 방식(drv)와 조사 년도(year)의 값에 따라 그래프를 나누어 그린 예이다. 역시 모든 그래프의 좌표축은 동일하고 그래프 상단에 어떤 측면의 그래프인지를 표시하고 있는데 윗줄에 표시된 내용은 구동 방식의 값이고 아랫줄은 조사년도의 값이다. 따라서 첫 번째 측면 그래프는 4륜 구동이고 1999년도 조사한 데이터 측면에서 배기량과 고속도로 연비의 산점도를 보여준다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~drv + year, nrow = 2)\n\n\n\n\n\n\n\n\n그래프를 두 변수의 측면에서 나누어 그릴 때는 face_wrap() 보다는 facet_grid()를 사용하는 것이 좋다. facet_grid()도 수식을 첫 번재 인수로 입력 받는데, 수식의 좌변과 우변에 측면으로 나누는데 사용할 변수를 지정할 수 있다. 수식의 좌변에 기술된 변수를 기준으로 측면 그래프를 행으로 배열하고, 우변에 기술된 변수를 기준으로 측면 그래프를 열로 배열한다. 다음 그래프는 행은 구동 방식으로, 열은 실린더 수를 기준으로 나누어 측면 그래프를 그린 예이다. 그러므로 두 번째 행-세 번째 열의 그래프는 전륜 구동(f)이고 실린더가 6자동차의 산점도를 나타낸다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\n\nfacet_wrap() 함수와 마찬가지로 수식의 좌변과 우변에 +로 하나 이상의 변수를 지정할 수도 있다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv + year ~ cyl)\n\n\n\n\n\n\n\n\n\nggplot2에는 지금까지 설명한 문법 요소 외에도 통계 변환(stat), 위치 조정(position), 스케일 변환(scale), 좌표축 변환(coord), 테마(theme) 등의 요소가 있다. ggplot2를 사용하여 복잡한 시각화를 수행하려면 이러한 문법 요소에 대한 체계적 이해와 습득이 필요하다. 그러나 이 책은 데이터 시각화 전반을 소개하는 것이 목적이 아니기 때문에, 통계데이터 분석을 위한 그래프를 그릴 때 이러한 문법 요소가 필요하면 그 요소를 단편적으로 설명할 예정이다. 그러므로 좀 더 ggplot2 그래프에 대한 체계적인 이해를 원하는 독자는 R 프로그래밍의 ggplot2를 이용한 데이터 시각화를 참조하기 바란다.\n이 절의 나머지 부분에서는 나머지 문법 요소 중 그래프의 외양을 변경하는 매우 간단한 한 가지 문법 요소만 살펴보도록 한다.\n\n\nggplot2 패키지의 labs() 함수는 그래프의 제목, 좌표축 이름, 범례의 이름을 쉽게 바꿀 수 있게 해준다. 다음은 mpg 데이터의 배기량과 고속도로 연비의 산점도를 자동차 종류 별로 다른 색상으로 그린 예이다. 그런데 ggplot2에서는 기본적으로 좌표축 레이블과 색상의 범례 레이블로, 좌표축과 색상에 매핑된 열의 이름을 사용한다. 그리고 그래프에 제목은 달지 않는다.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + geom_point()\n\n\n\n\n\n만약 자동으로 부여된 레이블이 마음에 들지 않으면 이를 labs() 함수로 변경할 수 있다. 위 그래프에서 다음처럼 범례 이름, 축의 이름 한글로 바꾸고, 그래프의 제목도 달아 보자.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  labs(title = '배기량과 고속도로 연비 산점도',\n       x = '배기량(리터)',\n       y = '고속도로 연비',\n       color = '자동차 종류')\n\n\n\n\n\nlabs() 함수는 ggplot2 그래프에 + 연산으로 결합하여 사용되면, 그래픽 속성 매핑에 사용된 x, y, color 인수에 사용할 이름을 지정하면 된다. 그래프의 제목을 지정하려면 title이라는 인수를 사용한다."
  },
  {
    "objectID": "Data_Visualization/NCDC_normals/NCDC_normals.html",
    "href": "Data_Visualization/NCDC_normals/NCDC_normals.html",
    "title": "NCDC Normals with ggplot2",
    "section": "",
    "text": "Code\n# Read csv File\nncdc_normals <- read.csv(\"../data/ncdc_normals.csv\")\nncdc_normals %>% head()\n#>    station_id month day temperature flag       date\n#> 1 AQW00061705     1   1        82.4    C 0000-01-01\n#> 2 AQW00061705     1   2        82.4    C 0000-01-02\n#> 3 AQW00061705     1   3        82.4    C 0000-01-03\n#> 4 AQW00061705     1   4        82.4    C 0000-01-04\n#> 5 AQW00061705     1   5        82.4    C 0000-01-05\n#> 6 AQW00061705     1   6        82.4    C 0000-01-06\n\nncdc_normals %>% dim()\n#> [1] 2745366       6\n\nncdc_normals %>% summary()\n#>   station_id            month             day         temperature    \n#>  Length:2745366     Min.   : 1.000   Min.   : 1.00   Min.   :-21.80  \n#>  Class :character   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 39.70  \n#>  Mode  :character   Median : 7.000   Median :16.00   Median : 54.60  \n#>                     Mean   : 6.514   Mean   :15.76   Mean   : 53.17  \n#>                     3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.: 68.10  \n#>                     Max.   :12.000   Max.   :31.00   Max.   :103.20  \n#>      flag               date          \n#>  Length:2745366     Length:2745366    \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\"\n\n\n\n\nCode\n# `date`열 Datetime type으로 변환\nncdc_normals$date <- ncdc_normals$date %>% as.Date(\"%Y-%m-%d\")\nncdc_normals$date %>% head()\n#> [1] \"0000-01-01\" \"0000-01-02\" \"0000-01-03\" \"0000-01-04\" \"0000-01-05\"\n#> [6] \"0000-01-06\"\n\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\"\n\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\"    \"double\"\n\n\n\n\nCode\n# station 개수 확인\nncdc_normals$station_id %>%\n    unique() %>%\n    length()\n#> [1] 7501\n\n# station_loc Dataframe 생성\nstation_loc <- data.frame(\n    station_id = c(\"USW00014819\", \"USC00042319\", \"USW00093107\", \"USW00012918\"),\n    location = c(\"Chicago\", \"Death Valley\", \"San Diego\", \"Houston\")\n)\nstation_loc\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n# Raw Data와 Inner Join\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by = \"station_id\")\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\n\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\" \n#>    location \n#> \"character\"\n\n\n\n\nCode\n# X축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\") # 3달 간격 Date 생성\n\n# ggplot + 축 설정\nggplot(temps_long, aes(x = date, y = temperature, color = location)) +\n    geom_line(linewidth = 1) +\n    scale_x_date(\n        name = \"month\", breaks = break_date,\n        labels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\")\n    ) +\n    scale_y_continuous(name = \"temp\") + # limits = c(0, 100) -> y 범위 지정\n    labs(title = \"Fig. 2.3\", subtitle = \"Daily temperature normals\") +\n    theme_light()\n\n\n\n\n\n\n\nCode\n# 월 평균\nmean_temps <- temps_long %>%\n    group_by(location, month) %>%\n    summarize(mean = mean(temperature)) %>%\n    ungroup() %>%\n    mutate(month = factor(paste(month), levels = paste(1:12)))\n#> `summarise()` has grouped output by 'location'. You can override using the\n#> `.groups` argument.\nmean_temps\n#> # A tibble: 48 × 3\n#>    location month  mean\n#>    <chr>    <fct> <dbl>\n#>  1 Chicago  1      24.8\n#>  2 Chicago  2      28.9\n#>  3 Chicago  3      38.8\n#>  4 Chicago  4      50.4\n#>  5 Chicago  5      60.9\n#>  6 Chicago  6      71.0\n#>  7 Chicago  7      75.8\n#>  8 Chicago  8      74.1\n#>  9 Chicago  9      66.4\n#> 10 Chicago  10     54.3\n#> # … with 38 more rows\n\n# ggplot + geom_tile + fill_color\nggplot(mean_temps, aes(x = month, y = location, fill = mean)) +\n    geom_tile(width = .95, height = .95) +\n    scale_fill_viridis_c(\n        option = \"B\", begin = 0.15, end = 0.98,\n        name = \"temperature\"\n    ) +\n    coord_fixed(expand = FALSE) +\n    ylab(NULL)"
  },
  {
    "objectID": "Data_Visualization/OBS_2021/OBS_2021.html",
    "href": "Data_Visualization/OBS_2021/OBS_2021.html",
    "title": "OBS with ggplot2",
    "section": "",
    "text": "데이터 출처 : https://data.kma.go.kr/data/grnd/selectAsosRltmList.do?pgmNo=36\n\n\n\nCode\n# Read csv File\ndata_2021 <- read.csv(\"../data/OBS_ASOS_DD_20220308125952.csv\", fileEncoding = \"CP949\")\ndata_2021 %>% dim()\n#> [1] 1460    6\n\ndata_2021 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2021-01-01         -4.2         -9.8          1.6\n#> 2  108   서울 2021-01-02         -5.0         -8.4         -1.4\n#> 3  108   서울 2021-01-03         -5.6         -9.1         -2.0\n#> 4  108   서울 2021-01-04         -3.5         -8.4          0.3\n#> 5  108   서울 2021-01-05         -5.5         -9.9         -2.1\n#> 6  108   서울 2021-01-06         -7.4        -12.0         -1.9\n\n# 자료형 확인\ndata_2021 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"  \"character\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n# Date Type Convert : Character -> Date\ndata_2021$일시 <- data_2021$일시 %>% as.Date(\"%Y-%m-%d\")\ndata_2021 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n# 기초통계량 확인\ndata_2021 %>% summary()\n#>       지점          지점명               일시             평균기온..C.   \n#>  Min.   :108.0   Length:1460        Min.   :2021-01-01   Min.   :-14.90  \n#>  1st Qu.:126.8   Class :character   1st Qu.:2021-04-02   1st Qu.:  7.90  \n#>  Median :158.5   Mode  :character   Median :2021-07-02   Median : 15.00  \n#>  Mean   :166.0                      Mean   :2021-07-02   Mean   : 14.77  \n#>  3rd Qu.:197.8                      3rd Qu.:2021-10-01   3rd Qu.: 23.10  \n#>  Max.   :239.0                      Max.   :2021-12-31   Max.   : 31.70  \n#>   최저기온..C.     최고기온..C.   \n#>  Min.   :-19.10   Min.   :-10.70  \n#>  1st Qu.:  3.10   1st Qu.: 13.18  \n#>  Median : 11.10   Median : 20.15  \n#>  Mean   : 10.69   Mean   : 19.56  \n#>  3rd Qu.: 19.60   3rd Qu.: 27.70  \n#>  Max.   : 28.10   Max.   : 36.50\n\n\n\n\nCode\n# X축 눈금 지정\ndate_s <- \"2021-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\") # 3달 간격 Date 생성\nlabel_date <- paste0(c(seq(1, 12, 3), 1), \"월\")\n\n# ggplot + 축 설정\ndata_2021 %>% names() # 열 이름 추출\n#> [1] \"지점\"         \"지점명\"       \"일시\"         \"평균기온..C.\" \"최저기온..C.\"\n#> [6] \"최고기온..C.\"\n\nggplot(data_2021, aes(x = 일시, y = 평균기온..C., color = 지점명)) +\n    geom_line(linewidth = 1) +\n    scale_x_date(name = \"월\", breaks = break_date, labels = label_date) +\n    scale_y_continuous(name = \"평균기온\") +\n    theme_light()\n\n\n\n\n\n\n\nCode\n# 월 평균\ndata_2021_month <- data_2021 %>%\n    mutate(month = format(일시, \"%B\")) %>% # month열 생성\n    group_by(지점명, month) %>%\n    summarize(mean = mean(평균기온..C., na.rm = TRUE)) %>%\n    ungroup() %>%\n    mutate(month = factor(month, levels = paste0(1:12, \"월\"))) # factor 형으로 변환\n#> `summarise()` has grouped output by '지점명'. You can override using the\n#> `.groups` argument.\ndata_2021_month\n#> # A tibble: 48 × 3\n#>    지점명 month   mean\n#>    <chr>  <fct>  <dbl>\n#>  1 대전   10월  15.7  \n#>  2 대전   11월   8.91 \n#>  3 대전   12월   1.89 \n#>  4 대전   1월   -0.984\n#>  5 대전   2월    3.99 \n#>  6 대전   3월    9.61 \n#>  7 대전   4월   14.7  \n#>  8 대전   5월   17.7  \n#>  9 대전   6월   23.5  \n#> 10 대전   7월   27.8  \n#> # … with 38 more rows\n\n# ggplot + geom_tile + fill_color\nggplot(data_2021_month, aes(x = month, y = 지점명, fill = mean)) +\n    geom_tile(width = .95, height = .95) +\n    scale_fill_viridis_c(\n        option = \"B\", begin = 0.15, end = 0.98, # 시작과 끝 색상 지정\n        name = \"temperature\"\n    ) +\n    coord_fixed(expand = FALSE) +\n    ylab(NULL)"
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nColor Scales\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate systems and axes\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nOBS with ggplot2\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nNCDC Normals with ggplot2\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\n  \n\n\n\n\nggplot2 Tutorial\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nJinwon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#profile",
    "href": "index.html#profile",
    "title": "Profile",
    "section": "Profile",
    "text": "Profile\n\n학력\n\n한남대학교 비즈니스통계학과 | 2018.03 ~ 2022.02\n한남대학교 빅데이터응용학과 | 2023.03 ~ \n\n\n\n수상 경력\n\n한국수자원공사장 표창상    | 2022.12\n과학기술정보통신부 장관상 | 2022.12 \n\n\n\n프로젝트\n\nCCTV 영상분석을 통한 도시침수 조기감지 서비스\n발화자의 감정인식 AI 경진대회\n제주도 도로 교통량 예측 AI 경진대회\n항공편 지연 예측 AI 경진대회\n드라이브스루 카페 매장 입지 선정 (진행중) \n\n\n\n자격증\n\n데이터 분석 준전문가 (ADsP) | 2021.09\n빅데이터분석기사 (진행중)\n사회조사분석사 2급 (진행중)"
  },
  {
    "objectID": "Project/Airplane_Delay_Prediction/Airplane_Delay_Prediction.html",
    "href": "Project/Airplane_Delay_Prediction/Airplane_Delay_Prediction.html",
    "title": "항공편 지연 예측 AI 경진대회",
    "section": "",
    "text": "주제 : 일부 레이블만 주어진 학습 데이터셋을 이용한 항공편 지연 여부 예측\n\n\n\n\n대회 페이지 바로가기\n\n\n\n \n\n최종 순위 : 21등 / 1,189팀\n팀명 : 환승이별\n팀원 : 이진원, 김민규, 진동준\n회의록"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html",
    "href": "Project/DriveThru/DriveThru.html",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "",
    "text": "2023년 국토교통 데이터 활용 경진대회"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#목차",
    "href": "Project/DriveThru/DriveThru.html#목차",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "목차",
    "text": "목차\n\n분석 배경 및 목적\n패키지 불러오기\n데이터 수집 및 전처리\n\n드라이브스루 매장 위치 데이터\n\n스타벅스 DT 매장 위치\n파스쿠찌 DT 매장 위치\n투썸플레이스 DT 매장 위치\n폴바셋 DT 매장 위치\n추가적인 DT 매장 위치\n\n고속도로 콘존 및 차로유형별 교통소통 데이터\n상권분석 데이터\n\n표준지공시지가 데이터\n\n노드별 교통량 & 표준지공시지가 결합\n\n유동인구 데이터\n\n\n시각화\n입지선정 분석\n\n버퍼 밖의 노드 추출\n회귀분석\n\n분석 1\n분석 2"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#분석-배경-및-목적",
    "href": "Project/DriveThru/DriveThru.html#분석-배경-및-목적",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "분석 배경 및 목적 ",
    "text": "분석 배경 및 목적 \n\n\n현대경제연구원의 조사에 따르면, 국내 성인 1인당 커피 소비량은 연간 353잔으로 세계 성인 평균인 연간 132잔보다 약 2.7배가 많음\n연도별 스타벅스 DT 매장 신규 입점 수를 보았을 때, 2012년부터 2020년까지 꾸준히 입점 수가 증가\n2019년부터 코로나19로 인한 비대면 주문의 선호도가 증가하여 브랜드별로 드라이브스루 매장의 입점을 늘리고 있는 추세임"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#패키지-불러오기",
    "href": "Project/DriveThru/DriveThru.html#패키지-불러오기",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "패키지 불러오기 ",
    "text": "패키지 불러오기 \n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport math\nimport time\nimport re\nimport psutil\nfrom tqdm import tqdm\n\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\n\nimport requests\nimport json\nimport xmltodict\nimport urllib\n\nimport folium\nfrom folium import Marker, LayerControl, GeoJson, CircleMarker\nfrom folium.plugins import MarkerCluster, HeatMap\nfrom folium.features import CustomIcon\nfrom branca.colormap import linear\n\nimport statsmodels.formula.api as smf\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.getcwd())\n\n# 한글 폰트 사용을 위해서 세팅\nfrom matplotlib import font_manager, rc, rcParams\nfont_path = \"C:/Windows/Fonts/malgun.ttf\"\nfont = font_manager.FontProperties(fname = font_path).get_name()\nrc('font', family = font)\n\n# 마이너스 깨짐 현상 해결\nrcParams['axes.unicode_minus'] = False\n\n# 소수점 자릿수 표기 설정 (지수표기법 제거)\npd.options.display.float_format = '{:.3f}'.format\n\n\nc:\\Users\\user\\anaconda3\\envs\\quartoenv\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn(\n\n\nd:\\Blog\\Project\\DriveThru\n\n\n\n\nCode\n# 카카오 API 지오코딩 함수\ndef geocoding_kakao(address) :\n  key = '95123874bec258e2ad4e1a93c8b6cc0c'\n  try : \n    url = 'https://dapi.kakao.com/v2/local/search/address.json?query=' + address\n    headers = {'Authorization' : f'KakaoAK {key}'}\n    res = json.loads(str(requests.get(url, headers = headers).text))\n\n    geo = res['documents'][0]['address']\n    lat_long = [float(geo['y']), float(geo['x'])]\n    return lat_long\n\n  except :\n    return [0, 0]"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#데이터-수집-및-전처리",
    "href": "Project/DriveThru/DriveThru.html#데이터-수집-및-전처리",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "데이터 수집 및 전처리 ",
    "text": "데이터 수집 및 전처리 \n \n\n드라이브스루 매장 위치 데이터 \n\n\n스타벅스 DT 매장 위치 \n\n\nCode\noptions = webdriver.ChromeOptions()\noptions.add_argument('--disable-geolocation')  # 위치 액세스 권한 비활성화\n\nservice = webdriver.chrome.service.Service(executable_path = '../chromedriver.exe')\ndriver = webdriver.Chrome(service = service, options = options)\ntime.sleep(3)\ndriver.get('https://www.starbucks.co.kr/store/store_map.do')\n\n\n\n\nCode\n# 위치 액세스 권한 비활성화가 안되서, 직접 위치 액세스 권한 비활성화 한 후 셀 실행\ndriver.find_element(By.XPATH, '//*[@id=\"container\"]/div/form/fieldset/div/section/article[1]/header/p/a').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"type2\"]').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"storeMap\"]/form/fieldset/div/ul/li[2]/a').click()\ntime.sleep(3)\n\nhtml = driver.page_source\nsoup = BeautifulSoup(html, 'html.parser')\n\ncontent = soup.find('div', id = 'mCSB_1_container')\ncontents = content.find_all('li')\nprint(len(contents))\n\n\n414\n\n\n\n\nCode\nstarbucks = []\n\nfor item in contents :\n    name = item.find('strong')['data-name']\n    lat = item['data-lat']\n    long = item['data-long']\n    address = item.find('p', {'class' : 'result_details'}).text.strip()[:-9]\n    address = address.split('(')[0].strip()\n    \n    starbucks.append([name, lat, long, address])\n    \n    columns = ['매장명', '위도', '경도', '주소']\n    starbucks_DT = pd.DataFrame(starbucks, columns = columns)\n    starbucks_DT['위도'] = starbucks_DT['위도'].astype('float')\n    starbucks_DT['경도'] = starbucks_DT['경도'].astype('float')\n    starbucks_DT['브랜드'] = '스타벅스'\n\nstarbucks_DT.loc[starbucks_DT['주소'].str.startswith('칠곡중앙대로'), '주소'] = '대구광역시 북구 칠곡중앙대로 637'\nstarbucks_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      송파마천사거리DT\n      37.50\n      127.15\n      서울특별시 송파구 거마로 78\n      스타벅스\n    \n    \n      1\n      송도컨벤시아대로DT\n      37.38\n      126.64\n      인천광역시 연수구 벤처로 10\n      스타벅스\n    \n    \n      2\n      포항포스코DT\n      36.00\n      129.39\n      경상북도 포항시 남구 동해안로6213번길 15-15\n      스타벅스\n    \n    \n      3\n      용인모현DT\n      37.35\n      127.25\n      경기도 용인시 처인구 모현읍 백옥대로 2529\n      스타벅스\n    \n    \n      4\n      화성향남발안DT\n      37.13\n      126.94\n      경기도 화성시 향남읍 발안로 308\n      스타벅스\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      409\n      목포터미널DT\n      34.81\n      126.42\n      전라남도 목포시 영산로 495\n      스타벅스\n    \n    \n      410\n      마산회원DT\n      35.22\n      128.57\n      경상남도 창원시 마산회원구 3·15대로 518\n      스타벅스\n    \n    \n      411\n      평촌역DT\n      37.40\n      126.97\n      경기도 안양시 동안구 시민대로 373\n      스타벅스\n    \n    \n      412\n      시흥대야DT\n      37.44\n      126.79\n      경기도 시흥시 뱀내장터로 27\n      스타벅스\n    \n    \n      413\n      수원경수인계DT\n      37.27\n      127.03\n      경기도 수원시 팔달구 경수대로 499\n      스타벅스\n    \n  \n\n414 rows × 5 columns\n\n\n\n\n\n파스쿠찌 DT 매장 위치 \n\n\nCode\noptions = webdriver.ChromeOptions()\noptions.add_argument('--disable-geolocation')  # 위치 액세스 권한 비활성화\n\nservice = webdriver.chrome.service.Service(executable_path = '../chromedriver.exe')\ndriver = webdriver.Chrome(service = service, options = options)\n\ndriver.implicitly_wait(5)\ndriver.get('https://www.caffe-pascucci.co.kr/store/storeList.asp')\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"searchFrm\"]/div/dl/dd[2]/div/span[1]/a').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"container\"]/div[3]/div[3]/div/a').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"container\"]/div[3]/div[3]/table').click()\ntime.sleep(3)\n\nhtml = driver.page_source\nsoup = BeautifulSoup(html, 'html.parser')\ncontents = soup.find_all('tr')\n\n\n\n\nCode\npascucci = []\n\nfor item in contents :\n    try :\n        name = item.find('strong').text\n        address = item.select_one('.addr').text\n        address = address.split(',')[0].strip()\n        address = address.split('(')[0].strip()\n        address = address.replace('충정북도', '충청북도').replace('우치로 948 1', '우치로 948')        \n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        pascucci.append([name, lat, long, address]) \n        \n        columns = ['매장명', '위도', '경도', '주소']\n        pascucci_DT = pd.DataFrame(pascucci, columns = columns)\n        pascucci_DT = pascucci_DT[~pascucci_DT['매장명'].str.contains('DI|D/I')]\n        pascucci_DT['브랜드'] = '파스쿠찌'\n        \n    except AttributeError :\n        pass\n\n# 시도 이름 통일\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b경기\\b', '경기도', x))\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b대구\\b', '대구광역시', x))\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b인천\\b', '인천광역시', x))\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b충북\\b', '충청북도', x))\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b충남\\b', '충청남도', x))\npascucci_DT['주소'] = pascucci_DT['주소'].apply(lambda x: re.sub(r'\\b경남\\b', '경상남도', x))\n\npascucci_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      청주엘지로DT\n      36.66\n      127.44\n      충청북도 청주시 흥덕구 엘지로11\n      파스쿠찌\n    \n    \n      1\n      광주용전DT\n      35.24\n      126.89\n      광주광역시 북구 우치로 948\n      파스쿠찌\n    \n    \n      2\n      구미강동DT\n      36.14\n      128.42\n      경상북도 구미시 옥계신당로 5안길 37\n      파스쿠찌\n    \n    \n      3\n      전주효자DT\n      35.80\n      127.10\n      전라북도 전주시 완산구 쑥고개로 285\n      파스쿠찌\n    \n    \n      4\n      김포DT\n      37.63\n      126.70\n      경기도 김포시 김포대로 1135\n      파스쿠찌\n    \n    \n      5\n      서청주DT\n      36.64\n      127.42\n      충청북도 청주시 흥덕구 2순환로 1067-2\n      파스쿠찌\n    \n    \n      6\n      남대전DT\n      36.29\n      127.46\n      대전광역시 동구 대전로 255\n      파스쿠찌\n    \n    \n      7\n      김천교동 D/T\n      36.13\n      128.10\n      경상북도 김천시 대학로 75\n      파스쿠찌\n    \n    \n      8\n      인천D/T\n      37.43\n      126.64\n      인천광역시 연수구 비류대로 11\n      파스쿠찌\n    \n    \n      10\n      계룡DT점\n      36.28\n      127.25\n      충청남도 계룡시 엄사면 엄사리 32-2\n      파스쿠찌\n    \n    \n      11\n      수원권선DT점\n      37.25\n      126.97\n      경기도 수원시 권선구 서수원로 245\n      파스쿠찌\n    \n    \n      13\n      전주 백제대로DT점\n      35.84\n      127.13\n      전라북도 전주시 덕진구 백제대로 556\n      파스쿠찌\n    \n    \n      14\n      대구칠곡태전DT\n      35.92\n      128.55\n      대구광역시 북구 칠곡중앙대로 238\n      파스쿠찌\n    \n    \n      15\n      첨단리버사이드DT점\n      35.22\n      126.86\n      광주광역시 북구 임방울대로 945\n      파스쿠찌\n    \n    \n      16\n      광주 방림DT점\n      35.13\n      126.92\n      광주광역시 남구 대남대로 85\n      파스쿠찌\n    \n  \n\n\n\n\n\n\n투썸플레이스 DT 매장 위치 \n\n\nCode\noptions = webdriver.ChromeOptions()\noptions.add_argument('--disable-geolocation')  # 위치 액세스 권한 비활성화\n\nservice = webdriver.chrome.service.Service(executable_path = '../chromedriver.exe')\ndriver = webdriver.Chrome(service = service, options = options)\n\ndriver.implicitly_wait(5)\ndriver.get('https://mo.twosome.co.kr/so/storeList.do')\n\ndriver.find_element(By.XPATH, '/html/body/div[1]/div/div/form/fieldset/div[2]/div/ul/li[2]/a').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '/html/body/div[1]/div/div/form/fieldset/div[2]/div/button[2]').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"option_wrap\"]/div/ul/li[5]/div/label').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"popup_shop_option\"]/div[3]/div/a').click()\ntime.sleep(3)\n\nhtml = driver.page_source\nsoup = BeautifulSoup(html, 'html.parser')\ncontent = soup.find('ul', id = 'infiniteScroll')\ncontents = content.find_all('li')\n\n\n\n\nCode\ntwosome = []\n\nfor item in contents :\n    try :\n        name = item.find('strong').text\n        address = item.find('dd', {'id': 'addr'}).text\n        address = address.split('(')[0].strip()\n        address = address.replace('여래리 51', '여래리 50-10')\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        twosome.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        twosome_DT = pd.DataFrame(twosome, columns = columns)\n        twosome_DT['브랜드'] = '투썸플레이스'\n        \n    except AttributeError :\n        pass\n\n# 시도 이름 통일\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b경기\\b', '경기도', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b광주\\b', '광주광역시', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b대전\\b', '대전광역시', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b대구\\b', '대구광역시', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b인천\\b', '인천광역시', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b충북\\b', '충청북도', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b충남\\b', '충청남도', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b전북\\b', '전라북도', x))\ntwosome_DT['주소'] = twosome_DT['주소'].apply(lambda x: re.sub(r'\\b경남\\b', '경상남도', x))\n    \ntwosome_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      경기광주쌍령DT점\n      37.40\n      127.28\n      경기도 광주시 경충대로 1411\n      투썸플레이스\n    \n    \n      1\n      광주수완대로DT점\n      35.21\n      126.82\n      광주광역시 광산구 도천동 275-10\n      투썸플레이스\n    \n    \n      2\n      순천순광로DT점\n      34.96\n      127.54\n      전라남도 순천시 순광로 188\n      투썸플레이스\n    \n    \n      3\n      대전가수원대로DT점\n      36.31\n      127.36\n      대전광역시 서구 가수원동 186-4\n      투썸플레이스\n    \n    \n      4\n      계룡DT점\n      36.29\n      127.25\n      충청남도 계룡시 엄사면 엄사리 25-1\n      투썸플레이스\n    \n    \n      5\n      화성북양DT점\n      37.22\n      126.85\n      경기도 화성시 남양읍 화성로 1500\n      투썸플레이스\n    \n    \n      6\n      분당궁내DT점\n      37.37\n      127.10\n      경기도 성남시 분당구 대왕판교로 310\n      투썸플레이스\n    \n    \n      7\n      순천용당DT점\n      34.97\n      127.49\n      전라남도 순천시 강변로 963\n      투썸플레이스\n    \n    \n      8\n      전주반월DT점\n      35.88\n      127.08\n      전라북도 전주시 덕진구 반월동 224-2번지\n      투썸플레이스\n    \n    \n      9\n      전주에코호수DT점\n      35.88\n      127.13\n      전라북도 전주시 덕진구 과학로 147\n      투썸플레이스\n    \n    \n      10\n      진영DT점\n      35.31\n      128.74\n      경상남도 김해시 진영읍 여래리 50-10\n      투썸플레이스\n    \n    \n      11\n      왜관DT점\n      35.99\n      128.41\n      경상북도 칠곡군 왜관읍 관문로 173\n      투썸플레이스\n    \n    \n      12\n      북대구IC주유소DT점\n      35.90\n      128.61\n      대구광역시 북구 동북로 113\n      투썸플레이스\n    \n    \n      13\n      광주마륵DT점\n      35.14\n      126.83\n      광주광역시 서구 상무대로 632\n      투썸플레이스\n    \n    \n      14\n      청주예술의전당DT점\n      36.64\n      127.47\n      충청북도 청주시 흥덕구 직지대로 680\n      투썸플레이스\n    \n    \n      15\n      대전용전DT점\n      36.36\n      127.44\n      대전광역시 동구 한밭대로 1246\n      투썸플레이스\n    \n    \n      16\n      대구두산동DT점\n      35.83\n      128.62\n      대구광역시 수성구 무학로 121\n      투썸플레이스\n    \n    \n      17\n      제주노형DT점\n      33.49\n      126.47\n      제주특별자치도 제주시 월랑로 70\n      투썸플레이스\n    \n    \n      18\n      원주무실DT점\n      37.33\n      127.93\n      강원도 원주시 북원로 2056\n      투썸플레이스\n    \n    \n      19\n      김포DT점\n      37.66\n      126.67\n      경기도 김포시 김포대로 1533\n      투썸플레이스\n    \n  \n\n\n\n\n\n\n폴바셋 DT 매장 위치 \n\n\nCode\noptions = webdriver.ChromeOptions()\noptions.add_argument('--disable-geolocation')  # 위치 액세스 권한 비활성화\n\nservice = webdriver.chrome.service.Service(executable_path = '../chromedriver.exe')\ndriver = webdriver.Chrome(service = service, options = options)\n\ndriver.implicitly_wait(5)\ndriver.get('https://www.baristapaulbassett.co.kr/store/Store.pb')\n\ndriver.find_element(By.XPATH, '//*[@id=\"storeOptionEl\"]/a').click()\ntime.sleep(5)\ndriver.find_element(By.XPATH, '//*[@id=\"option13\"]').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"storeOption\"]/div/a').click()\ntime.sleep(3)\ndriver.find_element(By.XPATH, '//*[@id=\"shopList\"]').click()\ntime.sleep(3)\n\nhtml = driver.page_source\nsoup = BeautifulSoup(html, 'html.parser')\ncontent = soup.find('ul', id = 'shopList')\ncontents = content.find_all('li')\n\n\n\n\nCode\npaulbassett = []\n\nfor item in contents :\n    try :\n        name = item.find('strong').text.split('\\t')[0].split('(')[0]\n        address = item.find('address').text.split(',')[0]\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        paulbassett.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        paulbassett_DT = pd.DataFrame(paulbassett, columns = columns)\n        paulbassett_DT['브랜드'] = '폴바셋'\n        \n    except AttributeError :\n        pass\n\n# 주소 통일성 있게 변경\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '전주 금암DT점', '주소'] = '전라북도 전주시 덕진구 금암동 711-3'\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '경산 임당역 DT점', '주소'] = '경상북도 경산시 중방동 35-5'\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '마산 해안대로DT점', '주소'] = '경상남도 창원시 마산합포구 중앙동3가 4-281'\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '광명 철산DT점', '주소'] = '경기도 광명시 오리로 883'\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '제주 아라DT점', '주소'] = '제주특별자치도 제주시 아라이동 2436'\npaulbassett_DT.loc[paulbassett_DT['매장명'] == '제주 이호 DT점', '주소'] = '제주특별자치도 제주시 이호일동 1907-1'\n\npaulbassett_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      전주 금암DT점\n      35.84\n      127.13\n      전라북도 전주시 덕진구 금암동 711-3\n      폴바셋\n    \n    \n      1\n      대구 만촌 DT점\n      35.87\n      128.65\n      대구광역시 수성구 만촌동 435-3\n      폴바셋\n    \n    \n      2\n      대구 방촌 DT점\n      35.88\n      128.66\n      대구광역시 동구 방촌동1084-201\n      폴바셋\n    \n    \n      3\n      청주 방서 DT점\n      36.60\n      127.50\n      충청북도 청주시 상당구 방서동 55-1\n      폴바셋\n    \n    \n      4\n      경산 임당역 DT점\n      35.83\n      128.74\n      경상북도 경산시 중방동 35-5\n      폴바셋\n    \n    \n      5\n      마산 회원 DT점\n      35.22\n      128.57\n      경상남도 창원시 마산회원구 회원동 45-9\n      폴바셋\n    \n    \n      6\n      마산 해안대로DT점\n      35.20\n      128.57\n      경상남도 창원시 마산합포구 중앙동3가 4-281\n      폴바셋\n    \n    \n      7\n      부산 전포 DT점\n      35.16\n      129.07\n      부산광역시 부산진구 전포동 194-23 16번지\n      폴바셋\n    \n    \n      8\n      부산 대연 DT점\n      35.13\n      129.10\n      부산광역시 남구 대연동 574-2번지\n      폴바셋\n    \n    \n      9\n      오산대 DT점\n      37.16\n      127.07\n      경기도 오산시 경기대로 464\n      폴바셋\n    \n    \n      10\n      용인시청 DT점\n      37.24\n      127.17\n      경기도 용인시 처인구 삼가동 198-15\n      폴바셋\n    \n    \n      11\n      북수원IC DT점\n      37.31\n      126.99\n      경기도 수원시 파장동 555-3\n      폴바셋\n    \n    \n      12\n      광명 철산DT점\n      37.48\n      126.86\n      경기도 광명시 오리로 883\n      폴바셋\n    \n    \n      13\n      강서 마곡DT점\n      37.57\n      126.84\n      서울특별시 강서구 양천로 300\n      폴바셋\n    \n    \n      14\n      김포 구래 DT점\n      37.65\n      126.64\n      경기도 김포시 양촌읍 석모리 1030\n      폴바셋\n    \n    \n      15\n      제주 용담DT점\n      33.52\n      126.49\n      제주특별자치도 제주시 서해안로 442-1\n      폴바셋\n    \n    \n      16\n      제주 아라DT점\n      33.49\n      126.54\n      제주특별자치도 제주시 아라이동 2436\n      폴바셋\n    \n    \n      17\n      제주 이호 DT점\n      33.50\n      126.45\n      제주특별자치도 제주시 이호일동 1907-1\n      폴바셋\n    \n  \n\n\n\n\n\n\n추가적인 DT 매장 위치 \n\n할리스 DT점\n\nhttps://m.hollys.co.kr/store/korStore2.do\n\n\n\nCode\nhollys_info = pd.DataFrame({'매장명' : ['제주도두해안DT점', '청주산남DT점', '평택이충DT점', '울산달동DT점', '화성능동DT점', '서전주DT점', \n                                        '오산대역DT점', '울산풍차DT점', '대구팔달교DT점', '대전도안DT점'],\n                            '주소' : ['제주특별자치도 제주시 도두일동 1686', '충청북도 청주시 서원구 1순환로 892', '경기도 평택시 추담로 30',\n                                      '울산광역시 남구 번영로 82', '경기도 화성시 동탄지성로 215', '전라북도 전주시 완산구 유연로 5',\n                                      '경기도 오산시 경기대로 491', '울산광역시 북구 산업로 1440', '대구광역시 서구 팔달로 34', '대전광역시 유성구 도안동로 488']})\nhollys = []\nfor (name, address) in zip(hollys_info['매장명'], hollys_info['주소']) :\n    try :\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        hollys.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        hollys_DT = pd.DataFrame(hollys, columns = columns)\n        hollys_DT['브랜드'] = '할리스'\n        \n    except AttributeError :\n        pass\n\nhollys_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      제주도두해안DT점\n      33.51\n      126.47\n      제주특별자치도 제주시 도두일동 1686\n      할리스\n    \n    \n      1\n      청주산남DT점\n      36.62\n      127.47\n      충청북도 청주시 서원구 1순환로 892\n      할리스\n    \n    \n      2\n      평택이충DT점\n      37.06\n      127.07\n      경기도 평택시 추담로 30\n      할리스\n    \n    \n      3\n      울산달동DT점\n      35.53\n      129.33\n      울산광역시 남구 번영로 82\n      할리스\n    \n    \n      4\n      화성능동DT점\n      37.21\n      127.05\n      경기도 화성시 동탄지성로 215\n      할리스\n    \n    \n      5\n      서전주DT점\n      35.82\n      127.09\n      전라북도 전주시 완산구 유연로 5\n      할리스\n    \n    \n      6\n      오산대역DT점\n      37.17\n      127.06\n      경기도 오산시 경기대로 491\n      할리스\n    \n    \n      7\n      울산풍차DT점\n      35.64\n      129.35\n      울산광역시 북구 산업로 1440\n      할리스\n    \n    \n      8\n      대구팔달교DT점\n      35.89\n      128.55\n      대구광역시 서구 팔달로 34\n      할리스\n    \n    \n      9\n      대전도안DT점\n      36.35\n      127.35\n      대전광역시 유성구 도안동로 488\n      할리스\n    \n  \n\n\n\n\n\n\n커피빈 DT점\n\nhttps://www.coffeebeankorea.com/store/store.asp\n\n\n\nCode\ncoffeebean_info = pd.DataFrame({'매장명' : ['제주애월DT점', '남양주화도DT점', '김포장기DT점', '파주DT점', '수원인계DT점', '동탄능동DT점', '수원송죽DT점'],\n                                '주소' : ['제주도 제주시 애월읍 애월해안로 560', '경기도 남양주시 경춘로 1641', '경기도 김포시 태장로 819',\n                                          '경기도 파주시 문발로 131', '경기도 수원시 팔달구 인계로79', '경기도 화성시 능동 453-4',\n                                          '경기도 수원시 장안구 경수대로 989']})\ncoffeebean = []\nfor (name, address) in zip(coffeebean_info['매장명'], coffeebean_info['주소']) :\n    try :\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        coffeebean.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        coffeebean_DT = pd.DataFrame(coffeebean, columns = columns)\n        coffeebean_DT['브랜드'] = '커피빈'\n        \n    except AttributeError :\n        pass\n\ncoffeebean_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      제주애월DT점\n      33.48\n      126.36\n      제주도 제주시 애월읍 애월해안로 560\n      커피빈\n    \n    \n      1\n      남양주화도DT점\n      37.65\n      127.27\n      경기도 남양주시 경춘로 1641\n      커피빈\n    \n    \n      2\n      김포장기DT점\n      37.64\n      126.68\n      경기도 김포시 태장로 819\n      커피빈\n    \n    \n      3\n      파주DT점\n      37.71\n      126.69\n      경기도 파주시 문발로 131\n      커피빈\n    \n    \n      4\n      수원인계DT점\n      37.27\n      127.03\n      경기도 수원시 팔달구 인계로79\n      커피빈\n    \n    \n      5\n      동탄능동DT점\n      37.21\n      127.06\n      경기도 화성시 능동 453-4\n      커피빈\n    \n    \n      6\n      수원송죽DT점\n      37.31\n      127.00\n      경기도 수원시 장안구 경수대로 989\n      커피빈\n    \n  \n\n\n\n\n\n\n엔제리너스 DT점\n\nhttps://www.lotteeatz.com/searchStore?divcd=20\n\n\n\nCode\nangelinus_info = pd.DataFrame({'매장명' : ['온수DT', '군산소룡D/T', '광주동림D/T', '광주광천D/T', '상무D/T', '남양산D/T'],\n                               '주소' : ['서울특별시 구로구 경인로 81', '전라북도 군산시 공단대로 599', '광주광역시 북구 하남대로 669',\n                                         '광주광역시 서구 무진대로 933', '광주광역시 서구 상무대로 778', '경상남도 양산시 동면 남양산길 11']})\nangelinus = []\nfor (name, address) in zip(angelinus_info['매장명'], angelinus_info['주소']) :\n    try :\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        angelinus.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        angelinus_DT = pd.DataFrame(angelinus, columns = columns)\n        angelinus_DT['브랜드'] = '엔제리너스'\n        \n    except AttributeError :\n        pass\n\nangelinus_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      온수DT\n      37.49\n      126.83\n      서울특별시 구로구 경인로 81\n      엔제리너스\n    \n    \n      1\n      군산소룡D/T\n      35.97\n      126.68\n      전라북도 군산시 공단대로 599\n      엔제리너스\n    \n    \n      2\n      광주동림D/T\n      35.17\n      126.87\n      광주광역시 북구 하남대로 669\n      엔제리너스\n    \n    \n      3\n      광주광천D/T\n      35.16\n      126.88\n      광주광역시 서구 무진대로 933\n      엔제리너스\n    \n    \n      4\n      상무D/T\n      35.15\n      126.85\n      광주광역시 서구 상무대로 778\n      엔제리너스\n    \n    \n      5\n      남양산D/T\n      35.33\n      129.03\n      경상남도 양산시 동면 남양산길 11\n      엔제리너스\n    \n  \n\n\n\n\n\n\n이디야커피 DT점\n\nhttps://www.ediya.com/contents/find_store.html#c\n\n\n\nCode\nediya_info = pd.DataFrame({'매장명' : ['여수한재DT점', '의정부솔뫼DT점'],\n                           '주소' : ['전라남도 여수시 한재로 118', '경기도 의정부시 시민로 450']})\n\nediya = []\nfor (name, address) in zip(ediya_info['매장명'], ediya_info['주소']) :\n    try :\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        ediya.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        ediya_DT = pd.DataFrame(ediya, columns = columns)\n        ediya_DT['브랜드'] = '이디야'\n    \n    except AttributeError :\n        pass\n\nediya_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      여수한재DT점\n      34.74\n      127.72\n      전라남도 여수시 한재로 118\n      이디야\n    \n    \n      1\n      의정부솔뫼DT점\n      37.73\n      127.08\n      경기도 의정부시 시민로 450\n      이디야\n    \n  \n\n\n\n\n\n\n빽다방 DT점\n\nhttps://paikdabang.com/store/\n\n\n\nCode\npaik_info = pd.DataFrame({'매장명' : ['천안용곡DT점', '제주봉개DT점'],\n                           '주소' : ['충청남도 천안시 동남구 용곡동 546-6 ', '제주특별자치도 제주시 봉개동 598']})\n\npaik = []\nfor (name, address) in zip(paik_info['매장명'], paik_info['주소']) :\n    try :\n        lat = geocoding_kakao(address)[0]\n        long = geocoding_kakao(address)[1]\n        paik.append([name, lat, long, address])\n        \n        columns = ['매장명', '위도', '경도', '주소']\n        paik_DT = pd.DataFrame(paik, columns = columns)\n        paik_DT['브랜드'] = '빽다방'\n    \n    except AttributeError :\n        pass\n\npaik_DT\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n    \n  \n  \n    \n      0\n      천안용곡DT점\n      36.79\n      127.14\n      충청남도 천안시 동남구 용곡동 546-6\n      빽다방\n    \n    \n      1\n      제주봉개DT점\n      33.48\n      126.60\n      제주특별자치도 제주시 봉개동 598\n      빽다방\n    \n  \n\n\n\n\n\n\n\n합치기\n\n\nCode\ndrivethru = pd.concat([starbucks_DT, pascucci_DT, twosome_DT, hollys_DT, paulbassett_DT,\n                       coffeebean_DT, angelinus_DT, ediya_DT, paik_DT], axis = 0)\n\ngeometry = gpd.points_from_xy(drivethru['경도'], drivethru['위도'])\ndrivethru_gdf = gpd.GeoDataFrame(drivethru, geometry = geometry, crs = 'epsg:4326')\ndrivethru_gdf.to_parquet('./data/드라이브스루매장_위치정보.parquet', index = False)\n\ndisplay(drivethru_gdf['브랜드'].value_counts().to_frame())\ndisplay(drivethru_gdf)\n\n\n\n\n\n\n  \n    \n      \n      count\n    \n    \n      브랜드\n      \n    \n  \n  \n    \n      스타벅스\n      414\n    \n    \n      투썸플레이스\n      20\n    \n    \n      폴바셋\n      18\n    \n    \n      파스쿠찌\n      15\n    \n    \n      할리스\n      10\n    \n    \n      커피빈\n      7\n    \n    \n      엔제리너스\n      6\n    \n    \n      이디야\n      2\n    \n    \n      빽다방\n      2\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      매장명\n      위도\n      경도\n      주소\n      브랜드\n      geometry\n    \n  \n  \n    \n      0\n      송파마천사거리DT\n      37.50\n      127.15\n      서울특별시 송파구 거마로 78\n      스타벅스\n      POINT (127.14767 37.49983)\n    \n    \n      1\n      송도컨벤시아대로DT\n      37.38\n      126.64\n      인천광역시 연수구 벤처로 10\n      스타벅스\n      POINT (126.63682 37.38245)\n    \n    \n      2\n      포항포스코DT\n      36.00\n      129.39\n      경상북도 포항시 남구 동해안로6213번길 15-15\n      스타벅스\n      POINT (129.38827 35.99697)\n    \n    \n      3\n      용인모현DT\n      37.35\n      127.25\n      경기도 용인시 처인구 모현읍 백옥대로 2529\n      스타벅스\n      POINT (127.24932 37.34919)\n    \n    \n      4\n      화성향남발안DT\n      37.13\n      126.94\n      경기도 화성시 향남읍 발안로 308\n      스타벅스\n      POINT (126.94121 37.13349)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5\n      남양산D/T\n      35.33\n      129.03\n      경상남도 양산시 동면 남양산길 11\n      엔제리너스\n      POINT (129.03306 35.32811)\n    \n    \n      0\n      여수한재DT점\n      34.74\n      127.72\n      전라남도 여수시 한재로 118\n      이디야\n      POINT (127.72091 34.74395)\n    \n    \n      1\n      의정부솔뫼DT점\n      37.73\n      127.08\n      경기도 의정부시 시민로 450\n      이디야\n      POINT (127.08260 37.73142)\n    \n    \n      0\n      천안용곡DT점\n      36.79\n      127.14\n      충청남도 천안시 동남구 용곡동 546-6\n      빽다방\n      POINT (127.14061 36.79090)\n    \n    \n      1\n      제주봉개DT점\n      33.48\n      126.60\n      제주특별자치도 제주시 봉개동 598\n      빽다방\n      POINT (126.60420 33.48404)\n    \n  \n\n494 rows × 6 columns\n\n\n\n\n\n\n고속도로 콘존 및 차로유형별 교통 소통 데이터(1개월 단위) \n\nhttps://www.bigdata-transportation.kr/frn/prdt/detail?prdtId=PRDTNUM_000000000009\n\n\n\n\nCode\n# 202201 ~ 202212 월별 교통량 데이터 병합\ntraffic_path = './conzone/'\ntraffic_csv = [file for file in os.listdir(traffic_path) if file.endswith('csv')]\ntraffic_list = []\n\nfor i in range(len(traffic_csv)) :\n    globals()[f'traffic_{traffic_csv[i][-10:-4]}'] = pd.read_csv(traffic_path + traffic_csv[i],\n                                                                 usecols = ['SUM_YRMTH', 'CZN_CD', 'TRFFCVLM', \n                                                                            'ST_ND_NM', 'ST_ND_GPS_LAT', 'ST_ND_GPS_LONG',\n                                                                            'END_ND_NM', 'END_ND_GPS_LAT', 'END_ND_GPS_LONG'])\n    traffic_list.append(f'traffic_{traffic_csv[i][-10:-4]}')\n\ntraffic_2022 = pd.concat([eval(traffic) for traffic in traffic_list]).reset_index(drop = True)\ntraffic_2022\n\n\n\n\n\n\n  \n    \n      \n      SUM_YRMTH\n      CZN_CD\n      TRFFCVLM\n      ST_ND_NM\n      ST_ND_GPS_LAT\n      ST_ND_GPS_LONG\n      END_ND_NM\n      END_ND_GPS_LAT\n      END_ND_GPS_LONG\n    \n  \n  \n    \n      0\n      202201\n      1040CZE030\n      -1\n      서부산TG\n      35.157\n      128.947\n      가락IC\n      35.165\n      128.897\n    \n    \n      1\n      202201\n      1040CZS040\n      1218481\n      서부산IC\n      35.156\n      128.952\n      가락IC\n      35.165\n      128.897\n    \n    \n      2\n      202201\n      0100CZS290\n      979428\n      덕천IC\n      35.215\n      129.008\n      대저JC\n      35.219\n      128.974\n    \n    \n      3\n      202201\n      0100CZE285\n      883937\n      북부산TG\n      35.221\n      128.941\n      대저JC\n      35.219\n      128.974\n    \n    \n      4\n      202201\n      0550CZS020\n      -1\n      초정IC\n      35.234\n      128.980\n      대저JC\n      35.219\n      128.974\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      17949\n      202212\n      5000CZS020\n      40747\n      남장성JC\n      NaN\n      NaN\n      남장성IC\n      NaN\n      NaN\n    \n    \n      17950\n      202212\n      5000CZE010\n      10452\n      남광산IC\n      NaN\n      NaN\n      남장성IC\n      NaN\n      NaN\n    \n    \n      17951\n      202212\n      0251CZE175\n      591202\n      북광산IC\n      NaN\n      NaN\n      남장성JC\n      NaN\n      NaN\n    \n    \n      17952\n      202212\n      0251CZS180\n      746773\n      장성IC\n      35.286\n      126.787\n      남장성JC\n      NaN\n      NaN\n    \n    \n      17953\n      202212\n      5000CZE020\n      14245\n      남장성IC\n      NaN\n      NaN\n      남장성JC\n      NaN\n      NaN\n    \n  \n\n17954 rows × 9 columns\n\n\n\n\n\nCode\n# 교통량 : -1 (결측치) 행 제거\ntraffic_2022_new = traffic_2022.drop(index = traffic_2022[traffic_2022['TRFFCVLM'] == -1].index)\n\n# 출발 노드와 도착 노드 위경도 좌표 없는 행 제거\ntraffic_2022_new = traffic_2022_new[(traffic_2022['ST_ND_GPS_LAT'].notnull()) & (traffic_2022['END_ND_GPS_LAT'].notnull())]\n\n# 콘존별 교통량 합계\ntraffic_2022_final = traffic_2022_new.pivot_table(values = 'TRFFCVLM', index = 'CZN_CD', columns = 'SUM_YRMTH', aggfunc = 'sum')\n\n# 콘존별 시작 노드명 & 위경도 / 도착 노드명 & 위경도 결합\ntraffic_2022_final['ST_ND_NM'] = traffic_2022_new.groupby('CZN_CD')['ST_ND_NM'].first()\ntraffic_2022_final['ST_ND_GPS_LAT'] = traffic_2022_new.groupby('CZN_CD')['ST_ND_GPS_LAT'].first()\ntraffic_2022_final['ST_ND_GPS_LONG'] = traffic_2022_new.groupby('CZN_CD')['ST_ND_GPS_LONG'].first()\ntraffic_2022_final['END_ND_NM'] = traffic_2022_new.groupby('CZN_CD')['END_ND_NM'].first()\ntraffic_2022_final['END_ND_GPS_LAT'] = traffic_2022_new.groupby('CZN_CD')['END_ND_GPS_LAT'].first()\ntraffic_2022_final['END_ND_GPS_LONG'] = traffic_2022_new.groupby('CZN_CD')['END_ND_GPS_LONG'].first()\n\n# Column 이름 변경\ntraffic_2022_final.columns.name = None\nold_columns = [int(f'{year}{month:02d}') for year in range(2022, 2023) for month in range(1, 13)]\nnew_columns = [f'TRAFFIC_{i:02d}' for i in range(1, 13)]\ntraffic_2022_final.rename(columns = dict(zip(old_columns, new_columns)), inplace = True)\n\n# 교통량 평균 계산\ntraffic_mean = traffic_2022_final[traffic_2022_final.columns[traffic_2022_final.columns.str.contains('TRAFFIC')]].mean(axis = 1)\ntraffic_mean_dict = {col : traffic_mean for col in traffic_2022_final.columns[traffic_2022_final.columns.str.contains('TRAFFIC')].tolist()}\n\ntraffic_2022_final.fillna(traffic_mean_dict, inplace = True)\ntraffic_2022_final['TRAFFIC_AVG'] = traffic_2022_final[traffic_2022_final.columns[traffic_2022_final.columns.str.contains('TRAFFIC')]].mean(axis = 1)\n\ndisplay(traffic_2022_final.head())\n\n\n\n\n\n\n  \n    \n      \n      TRAFFIC_01\n      TRAFFIC_02\n      TRAFFIC_03\n      TRAFFIC_04\n      TRAFFIC_05\n      TRAFFIC_06\n      TRAFFIC_07\n      TRAFFIC_08\n      TRAFFIC_09\n      TRAFFIC_10\n      TRAFFIC_11\n      TRAFFIC_12\n      ST_ND_NM\n      ST_ND_GPS_LAT\n      ST_ND_GPS_LONG\n      END_ND_NM\n      END_ND_GPS_LAT\n      END_ND_GPS_LONG\n      TRAFFIC_AVG\n    \n    \n      CZN_CD\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0010CZE010\n      1098605.000\n      974922.000\n      1019807.000\n      1123528.000\n      1181119.000\n      1095890.000\n      1131954.000\n      1172784.000\n      1140640.000\n      1195292.000\n      1140758.000\n      1103130.000\n      구서IC\n      35.246\n      129.093\n      영락IC\n      35.273\n      129.109\n      1114869.083\n    \n    \n      0010CZE011\n      1089491.000\n      968915.000\n      1010778.000\n      1111617.000\n      1170007.000\n      1086268.000\n      1139663.000\n      1160061.000\n      1124469.000\n      1185761.000\n      1130959.000\n      1094377.000\n      영락IC\n      35.273\n      129.109\n      부산TG\n      35.280\n      129.106\n      1106030.500\n    \n    \n      0010CZE020\n      1063107.000\n      949521.000\n      992378.000\n      1092333.000\n      1148854.000\n      1068289.000\n      1121252.000\n      1139664.000\n      1100534.000\n      1164558.000\n      1109219.000\n      1071773.000\n      부산TG\n      35.280\n      129.106\n      노포IC\n      35.291\n      129.093\n      1085123.500\n    \n    \n      0010CZE040\n      1458901.000\n      1301353.000\n      1339567.000\n      1475471.000\n      1585613.000\n      1459734.000\n      1547143.000\n      1614311.000\n      1487350.000\n      1618653.000\n      1494966.000\n      1432915.000\n      양산JC\n      35.335\n      129.040\n      양산IC\n      35.379\n      129.052\n      1484664.750\n    \n    \n      0010CZE090\n      716860.000\n      675394.000\n      661946.000\n      769595.000\n      829979.000\n      763660.000\n      788688.000\n      851738.000\n      805607.000\n      890376.000\n      788710.000\n      753958.000\n      경주IC\n      35.807\n      129.185\n      건천IC\n      35.844\n      129.085\n      774709.250\n    \n  \n\n\n\n\n\n\nCode\n# 시작 노드를 기준으로 교통량 계산\ntraffic_2022_ST = traffic_2022_final[['ST_ND_NM', 'ST_ND_GPS_LAT', 'ST_ND_GPS_LONG', 'TRAFFIC_AVG']].reset_index()\ntraffic_2022_END = traffic_2022_final[['END_ND_NM', 'END_ND_GPS_LAT', 'END_ND_GPS_LONG', 'TRAFFIC_AVG']].reset_index()\n\nnew_col = ['CZN_CD', 'ND_NM', 'ND_GPS_LAT', 'ND_GPS_LONG', 'TRAFFIC_AVG']\ntraffic_2022_ST.columns, traffic_2022_END.columns = new_col, new_col\n\nnode_traffic_2022 = pd.merge(traffic_2022_ST, traffic_2022_END, how = 'outer')\nnode_traffic_2022 = node_traffic_2022.groupby(['ND_NM', 'ND_GPS_LAT', 'ND_GPS_LONG'], as_index = False)['TRAFFIC_AVG'].agg(sum)\n\n# 노드별 교통량 평균 GeoDataFrame 생성\nGEO = gpd.points_from_xy(node_traffic_2022['ND_GPS_LONG'], node_traffic_2022['ND_GPS_LAT'])\nnode_traffic_2022_gdf = gpd.GeoDataFrame(node_traffic_2022, geometry = GEO, crs = 'epsg:4326')\n\ndisplay(node_traffic_2022_gdf.head())\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      ND_GPS_LAT\n      ND_GPS_LONG\n      TRAFFIC_AVG\n      geometry\n    \n  \n  \n    \n      0\n      88JC\n      37.578\n      126.818\n      1262486.333\n      POINT (126.81788 37.57823)\n    \n    \n      1\n      가락IC\n      35.165\n      128.897\n      2210073.333\n      POINT (128.89672 35.16514)\n    \n    \n      2\n      가산IC\n      36.096\n      128.534\n      1572837.917\n      POINT (128.53356 36.09579)\n    \n    \n      3\n      가조IC\n      35.704\n      128.027\n      1309385.583\n      POINT (128.02678 35.70360)\n    \n    \n      4\n      감곡IC\n      37.125\n      127.670\n      3893094.667\n      POINT (127.66984 37.12458)\n    \n  \n\n\n\n\n\n\n상권분석 데이터 \n\n\n표준지공시지가 데이터 \n\nhttps://www.data.go.kr/data/15004246/fileData.do\n\n\n\nCode\n# 행정동별 표준지공시지가 계산\nst_price = pd.read_csv('./data/2022_표준지_공시지가.csv')\n\n# 읍면동별 공간정보 데이터 (http://www.gisdeveloper.co.kr/?p=2332)\nemd_shp = gpd.read_file('./data/EMD_202302/emd.shp', encoding = 'EUC-KR')\n\nst_price['코드'] = [str(str(sgg_cd) + str(emd_cd))[:-2] for sgg_cd, emd_cd in zip(st_price['시군구'], st_price['읍면동리'])]\nst_price['주소'] = [sido + ' ' + li if sido == '세종특별자치시' else sido + ' '+ emd + ' ' + li \n                  for sido, emd, li in zip(st_price['시도명'], st_price['시군구명'], st_price['소재지'])]\nst_price['행정동'] = [' '.join(address.split()[:2]) if '세종특별자치시' in address\n                   else ' '.join(address.split()[:3]) for address in st_price['주소']]\n\nst_price = st_price[st_price['지목'].str.contains('대|임야|잡종지')][['코드', '주소', '행정동', '지목', '공시지가']].reset_index(drop = True)\n\nst_price_avg = st_price.groupby(['코드', '행정동'], as_index = False)['공시지가'].agg('mean')\n\nst_price_df = emd_shp.merge(st_price_avg, how = 'left', left_on = 'EMD_CD', right_on = '코드')[['코드', '행정동', '공시지가', 'geometry']].dropna()\nst_price_df = st_price_df.to_crs(epsg = 4326)\nst_price_df.head()\n\n\n\n\n\n\n  \n    \n      \n      코드\n      행정동\n      공시지가\n      geometry\n    \n  \n  \n    \n      0\n      11110101\n      서울특별시 종로구 청운동\n      4445166.667\n      POLYGON ((126.97556 37.58968, 126.97549 37.589...\n    \n    \n      1\n      11110102\n      서울특별시 종로구 신교동\n      4809714.286\n      POLYGON ((126.97031 37.58418, 126.97033 37.584...\n    \n    \n      2\n      11110103\n      서울특별시 종로구 궁정동\n      5756666.667\n      POLYGON ((126.97400 37.58654, 126.97401 37.586...\n    \n    \n      3\n      11110104\n      서울특별시 종로구 효자동\n      6244166.667\n      POLYGON ((126.97356 37.58323, 126.97355 37.582...\n    \n    \n      4\n      11110105\n      서울특별시 종로구 창성동\n      7117000.000\n      POLYGON ((126.97353 37.58182, 126.97354 37.581...\n    \n  \n\n\n\n\n\n\nCode\n# 노드별 교통량 & 표준지공시지가 데이터 결합\nnode_price = gpd.sjoin(node_traffic_2022_gdf, st_price_df, how = 'left', op = 'within')\nnode_price = node_price[['ND_NM', 'ND_GPS_LAT', 'ND_GPS_LONG', 'TRAFFIC_AVG', '행정동', '공시지가', 'geometry']].reset_index(drop = True)\n\n# 결측치 존재하는 행 처리\nnode_price.loc[node_price['ND_NM'] == '기장JC', ['행정동', '공시지가']] = ['부산광역시 기장군 기장읍', 1526396.644]\nnode_price.loc[node_price['ND_NM'] == '남양주IC', ['행정동', '공시지가']] = ['경기도 남양주시 다산동', 3260415.730]\nnode_price.loc[node_price['ND_NM'] == '옥련IC', ['행정동', '공시지가']] = ['인천광역시 연수구 옥련동', 1688750.000]\nnode_price.loc[node_price['ND_NM'] == '인천시점', ['행정동', '공시지가']] = ['인천광역시 미추홀구 용현동', 1711062.500]\n\nnode_price.to_parquet('./data/노드별_평균교통량_공시지가.parquet', index = True)\n\ndisplay(node_price.head())\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      ND_GPS_LAT\n      ND_GPS_LONG\n      TRAFFIC_AVG\n      행정동\n      공시지가\n      geometry\n    \n  \n  \n    \n      0\n      88JC\n      37.578\n      126.818\n      1262486.333\n      서울특별시 강서구 방화동\n      4249047.222\n      POINT (126.81788 37.57823)\n    \n    \n      1\n      가락IC\n      35.165\n      128.897\n      2210073.333\n      부산광역시 강서구 봉림동\n      619558.824\n      POINT (128.89672 35.16514)\n    \n    \n      2\n      가산IC\n      36.096\n      128.534\n      1572837.917\n      경상북도 칠곡군 가산면\n      119028.684\n      POINT (128.53356 36.09579)\n    \n    \n      3\n      가조IC\n      35.704\n      128.027\n      1309385.583\n      경상남도 거창군 가조면\n      96475.714\n      POINT (128.02678 35.70360)\n    \n    \n      4\n      감곡IC\n      37.125\n      127.670\n      3893094.667\n      충청북도 음성군 감곡면\n      239219.048\n      POINT (127.66984 37.12458)\n    \n  \n\n\n\n\n\n노드별 교통량 + 표준지공시지가 \n\n\nCode\n# 노드별 교통량 & 표준지공시지가 데이터 결합\nnode_price = gpd.sjoin(node_traffic_2022_gdf, st_price_df, how = 'left', op = 'within')\nnode_price = node_price[['ND_NM', 'ND_GPS_LAT', 'ND_GPS_LONG', 'TRAFFIC_AVG', '행정동', '공시지가', 'geometry']].reset_index(drop = True)\n\n# 결측치 존재하는 행 처리\nnode_price.loc[node_price['ND_NM'] == '기장JC', ['행정동', '공시지가']] = ['부산광역시 기장군 기장읍', 1526396.644]\nnode_price.loc[node_price['ND_NM'] == '남양주IC', ['행정동', '공시지가']] = ['경기도 남양주시 다산동', 3260415.730]\nnode_price.loc[node_price['ND_NM'] == '옥련IC', ['행정동', '공시지가']] = ['인천광역시 연수구 옥련동', 1688750.000]\nnode_price.loc[node_price['ND_NM'] == '인천시점', ['행정동', '공시지가']] = ['인천광역시 미추홀구 용현동', 1711062.500]\n\nnode_price.to_parquet('./data/노드별_평균교통량_공시지가.parquet', index = True)\n\ndisplay(node_price.head())\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      ND_GPS_LAT\n      ND_GPS_LONG\n      TRAFFIC_AVG\n      행정동\n      공시지가\n      geometry\n    \n  \n  \n    \n      0\n      88JC\n      37.578\n      126.818\n      1262486.333\n      서울특별시 강서구 방화동\n      4249047.222\n      POINT (126.81788 37.57823)\n    \n    \n      1\n      가락IC\n      35.165\n      128.897\n      2210073.333\n      부산광역시 강서구 봉림동\n      619558.824\n      POINT (128.89672 35.16514)\n    \n    \n      2\n      가산IC\n      36.096\n      128.534\n      1572837.917\n      경상북도 칠곡군 가산면\n      119028.684\n      POINT (128.53356 36.09579)\n    \n    \n      3\n      가조IC\n      35.704\n      128.027\n      1309385.583\n      경상남도 거창군 가조면\n      96475.714\n      POINT (128.02678 35.70360)\n    \n    \n      4\n      감곡IC\n      37.125\n      127.670\n      3893094.667\n      충청북도 음성군 감곡면\n      239219.048\n      POINT (127.66984 37.12458)\n    \n  \n\n\n\n\n\n\n\n유동인구 데이터 \n\n데이터안심구역에서 제공한 데이터\n7대광역시 및 서울특별시만 제공\n데이터 반출이 시군구 단위로만 가능\n\n\n데이터안심구역 분석 코드\n\n코드 실행 불가\n7대광역시 유동인구\n\n7대광역시 (부산, 대구, 인천, 광주, 대전, 울산, 세종) 유동인구 데이터 (2022년 1월 ~ 2023년 3월)\n\n\n\n\nCode\n# 202201 ~ 202303 월별 유동인구 데이터를 행정동별로 계산\npop7_dir = os.getcwd() + '/' + 'import_data' + '/' + 'TB_LGU_METROPOLITAN_POPL'\npop7_list = os.listdir(pop7_dir)\n\npop_all = pd.DataFrame()\n\nfor i in tqdm(range(len(pop7_list))) :\n    pop = pd.read_csv(pop7_dir + '/' + pop7_list[i])\n    pop['주소'] = [sido + ' ' + sigungu + ' ' + hgd for sido, sigungu, hgd in zip(pop['광역시도명'], pop['시군구명'], pop['행정동명'])]\n    pop['코드'] = [str(code) + '00' for code in pop['행정동코드']]\n    pop['년월'] = [str(date)[:6] for date in pop['기준일자']]\n    pop['남자합계'] = pop[pop.columns[pop.columns.str.contains('남성')]].sum(axis = 1)\n    pop['여자합계'] = pop[pop.columns[pop.columns.str.contains('여성')]].sum(axis = 1)\n    \n    globals()[f'pop_{pop7_list[i][-8:-4]}_count'] = pop.groupby(['주소', '코드', '년월'], as_index = False)[['남자합계', '여자합계', '소계(남녀합계)']].agg(sum)\n    pop_all = pd.concat([pop_all, eval(f'pop_{pop7_list[i][-8:-4]}_count')], axis = 0).reset_index(drop = True)\n\npop_all.to_csv('./7대광역시_행정동별_유동인구_2201_2303.csv', index = False)\n\n\n\n서울특별시 유동인구\n\n서울지역 시간대별 성별 연령별 50m X 50m 유동인구 데이터 (2022년 1월 ~ 2023년 3월)\n\n\n\n\nCode\n# 202201 ~ 202303 일별 유동인구 데이터를 셀별로 계산\nseoul_pop_dir = os.getcwd() + '/' + 'import_data' + '/' + 'TB_LGU_SEOUL_POPL'\nseoul_pop_list = os.listdir(seoul_pop_dir)\n\nym_list = [str(date.year) + str(date.month).zfill(2) for date in pd.date_range('2022-01-01', '2023-03-31', freq = 'M')]\n\nfor ym in ym_list :\n    pop_ym_list = [pop_list for pop_list in seoul_pop_list if ym in pop_list]\n    \n    pop_count = pd.DataFrame()\n    \n    for pop_ymd_data in tqdm(pop_ym_list) :\n        pop_ymd = pd.read_csv(seoul_pop_dir + '/' + pop_ymd_data)\n        pop_ymd['남자합계'] = pop_ymd[pop_ymd.columns[pop_ymd.columns.str.contains('남성')]].sum(axis = 1)\n        pop_ymd['여자합계'] = pop_ymd[pop_ymd.columns[pop_ymd.columns.str.contains('여성')]].sum(axis = 1)\n        \n        pop_ymd_count = pop_ymd.groupby(['셀번호', 'X좌표', 'Y좌표', '행정동코드', '일자'], as_index = False)[['남자합계', '여자합계', '소계(남녀합계)']].agg(sum)    \n        pop_count = pd.concat([pop_count, pop_ymd_count], axis = 0).reset_index(drop = True)\n        del pop_ymd, pop_ymd_count\n    \n    pop_ym_count = pop_count.groupby(['셀번호', 'X좌표', 'Y좌표', '행정동코드'], as_index = False)[['남자합계', '여자합계', '소계(남녀합계)']].agg(sum)    \n    globals()[f'pop_{ym[2:]}_count'] = pop_ym_count\n    pop_ym_count.to_csv(f'pop_{ym[2:]}_count.csv', index = False)\n    del pop_count, pop_ym_count\n\n\n\n\nCode\nym_list = [str(date.year) + str(date.month).zfill(2) for date in pd.date_range('2022-01-01', '2023-03-31', freq = 'M')]\n\nseoul_all = pd.DataFrame()\n\nfor ym in ym_list :\n    try :\n        globals()[f'pop_{ym[2:]}_count'] = pd.read_csv(f'./pop_{ym[2:]}_count.csv')\n        eval(f'pop_{ym[2:]}_count')['년월'] = ym\n        \n        seoul_all = pd.concat([seoul_all, eval(f'pop_{ym[2:]}_count')], axis = 0).reset_index(drop = True)\n    except FileNotFoundError :\n        pass\n\nseoul_all.to_csv('./서울특별시_셀별_유동인구_2201_2303.csv', index = False)\n\n\n\n유동인구 데이터 병합\n\n\n\nCode\n# 시군구코드\ncode = pd.read_csv('./import_data/COMM_CODE/TB_ADMIN_DONG_CODE.dat', sep = '\\a', header = None)\ncode.columns = ['코드구분(영문)', '코드구분(한글)', '코드', '주소']\nsgg_cd = code[code['코드구분(한글)'] == '시군구코드'][['코드', '주소']]\n\n# 7대광역시 유동인구\npop_all = pd.read_csv('./7대광역시_행정동별_유동인구_2201_2303.csv')\npop7_hgd_mean = pop_all.groupby(['코드'], as_index = False)['소계(남녀합계)'].agg('mean')\npop7_hgd_mean.columns = ['행정동코드', '합계']\n\n# 서울특별시 유동인구\nseoul_all = pd.read_csv('서울특별시_셀별_유동인구_2201_2303.csv')\n\n# 셀 별 남녀합계 월평균\nseoul_mean = seoul_all.groupby(['셀번호', 'X좌표', 'Y좌표', '행정동코드'], as_index = False)['소계(남녀합계)'].agg('mean').rename(columns = {'소계(남녀합계)' : '합계'})\n\n# 행정동별로 합치기\nseoul_hgd_mean = seoul_mean.groupby(['행정동코드'], as_index = False)['합계'].agg('sum')\nseoul_hgd_mean['행정동코드'] = [str(code) + '00' for code in seoul_hgd_mean['행정동코드']]\nseoul_hgd_mean['행정동코드'] = seoul_hgd_mean['행정동코드'].astype(int)\n\n# 7대광역시 + 서울특별시 합치기\nhgd_mean = pd.concat([pop7_hgd_mean, seoul_hgd_mean], axis = 0).reset_index(drop = True)\nhgd_mean['시군구코드'] = [str(code)[:-5] + '00000' for code in hgd_mean['행정동코드']]\nhgd_mean['시군구코드'] = hgd_mean['시군구코드'].astype(int)\n\n\n\n\nCode\n# 시군구별 유동인구 평균 계산\nhgd_nm_mean = pd.merge(hgd_mean, sgg_cd, how = 'left', left_on = '시군구코드', right_on = '코드')\nsgg_mean = hgd_nm_mean.groupby('주소', as_index = False)['합계'].agg('mean')\nsgg_mean.to_csv('./시군구별_유동인구평균_22012303.csv', index = False)\n\n\n\n\n공간정보 결합\n\n코드 실행 가능\n\n\n\nCode\n# 유동인구 데이터\nfoot_traffic = pd.read_csv('./data/시군구별_유동인구평균_22012303.csv')\n\n# 시군구별 공간정보 데이터 (http://www.gisdeveloper.co.kr/?p=2332)\nsgg_shp = gpd.read_file('./data/SIG_202302/sig.shp', encoding = 'EUC-KR')\n\n# 법정동 코드 데이터 (https://www.data.go.kr/data/15063424/fileData.do)\ncode = pd.read_csv('./data/국토교통부_전국 법정동_20221031.csv', encoding = 'CP949',\n                   usecols = ['법정동코드', '시도명', '시군구명', '읍면동명', '리명'])\n\n# 7대광역시 & 서울특별시만 추출\ncity_list = ['서울특별시', '부산광역시', '대구광역시', '인천광역시', '광주광역시', '대전광역시', '울산광역시', '세종특별자치시']\nsgg_code = code[(code['시도명'].isin(city_list)) & (code['읍면동명'].isnull()) & (code['시군구명'].notnull())]\n\n# 시군구코드 & 주소 생성\nsgg_code['시군구코드'] = [str(code)[:5] for code in sgg_code['법정동코드']]\nsgg_code['주소'] = sgg_code['시도명'] + ' ' + sgg_code['시군구명']\nsgg_code = sgg_code[['시군구코드', '주소']].reset_index(drop = True)\n\n# 세종특별자치시 전처리\nsgg_code.loc[sgg_code['주소'].str.contains('세종'), '주소'] = '세종특별자치시'\nsgg_code.head()\n\n\n\n\n\n\n  \n    \n      \n      시군구코드\n      주소\n    \n  \n  \n    \n      0\n      26470\n      부산광역시 연제구\n    \n    \n      1\n      26500\n      부산광역시 수영구\n    \n    \n      2\n      26530\n      부산광역시 사상구\n    \n    \n      3\n      26710\n      부산광역시 기장군\n    \n    \n      4\n      28245\n      인천광역시 계양구\n    \n  \n\n\n\n\n\n\nCode\n# 공간정보 결합하여 GeoDataFrame 생성\nfoot_traffic_sgg = pd.merge(foot_traffic, sgg_code, how = 'left', on = '주소')\nfoot_traffic_geo = pd.merge(foot_traffic_sgg, sgg_shp, how = 'left', left_on = '시군구코드', right_on = 'SIG_CD')[['주소', '합계', 'geometry']]\nfoot_traffic_geo.columns = ['시군구', '유동인구', 'geometry']\nfoot_traffic_gdf = gpd.GeoDataFrame(foot_traffic_geo).to_crs(epsg = 4326)\n\nfoot_traffic_gdf.to_parquet('./data/시군구별_유동인구평균_22012303.parquet', index = True)\ndisplay(foot_traffic_gdf.head())\n\n\n\n\n\n\n  \n    \n      \n      시군구\n      유동인구\n      geometry\n    \n  \n  \n    \n      0\n      광주광역시 광산구\n      5415573.602\n      POLYGON ((126.64470 35.14661, 126.64471 35.146...\n    \n    \n      1\n      광주광역시 남구\n      2970809.705\n      POLYGON ((126.90107 35.15234, 126.90125 35.152...\n    \n    \n      2\n      광주광역시 동구\n      2578661.530\n      POLYGON ((126.93018 35.16115, 126.93018 35.161...\n    \n    \n      3\n      광주광역시 북구\n      4351045.314\n      POLYGON ((126.91461 35.25821, 126.91461 35.258...\n    \n    \n      4\n      광주광역시 서구\n      4595717.864\n      POLYGON ((126.88588 35.16598, 126.88589 35.165..."
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#시각화",
    "href": "Project/DriveThru/DriveThru.html#시각화",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "시각화 ",
    "text": "시각화 \n\n\n드라이브스루 매장 반경 5km 안에 포함되어 있지 않은 노드찾기\n\n\n\nCode\n# 데이터 불러오기\ndrivethru = gpd.read_parquet('./data/드라이브스루매장_위치정보.parquet')\nnode_price = gpd.read_parquet('./data/노드별_평균교통량_공시지가.parquet')\nfoot_traffic = gpd.read_parquet('./data/시군구별_유동인구평균_22012303.parquet')\n\n# 드라이브스루 매장 아이콘\nbrand_list = list(drivethru['브랜드'].unique())\nurl_list = list(['./logo/starbucks.png', './logo/pascucci.png', './logo/twosome.jpg',\n                 './logo/hollys.png', './logo/paulbassett.png', './logo/coffeebean.png',\n                 './logo/angelinus.png', './logo/ediya.png', './logo/paik.png'])\nicon_dict = dict(zip(brand_list, url_list))\n\n# 드라이브스루 매장 5km 버퍼 생성\nbuffer_DT = drivethru.to_crs(epsg = 5174).buffer(5000).to_crs(epsg = 4326)\nunion_DT = buffer_DT.geometry.unary_union\n\n\n\n\nCode\n# Base Map 생성\nm = folium.Map(location = [36.5, 128], tiles = 'openstreetmap', zoom_start = 8)\n\n# 노드별 교통량 Color Map 생성\ntraffic_color_map = linear.PuRd_05.scale(node_price['TRAFFIC_AVG'].min(), node_price['TRAFFIC_AVG'].max())\ntraffic_color_map.caption = 'Traffic Average'\n\n# 행정동별 표준지공시지가 Color Map 생성 \nprice_color_map = linear.BuGn_05.scale(node_price['공시지가'].min(), node_price['공시지가'].max())\nprice_color_map.caption = 'Land Price'\n\n# 드라이브스루 매장 위치 시각화\nmc = MarkerCluster(name = 'DriveThru')\nfor idx, row in drivethru.iterrows() :\n    if not math.isnan(row['경도']) and not math.isnan(row['위도']) :\n        icon_url = icon_dict.get(row['브랜드'])\n        icon = CustomIcon(icon_url, icon_size = (30, 30))\n        mc.add_child(Marker(location = [row['위도'], row['경도']], \n                            tooltip = row['매장명'], popup = row['주소'],\n                            icon = icon))\nm.add_child(mc)\n\n# 드라이브 스루 매장 5km 버퍼 시각화\nGeoJson(union_DT, name = 'Buffer_DT').add_to(m)\n\n# 노드별 교통량 & 행정동별 표준지공시지가 시각화\ntraffic_group = folium.FeatureGroup(name = 'Traffic')\nprice_group = folium.FeatureGroup(name = 'Price')\nfor idx, row in node_price.iterrows() :\n    traffic_avg = round(row['TRAFFIC_AVG'])\n    price = round(row['공시지가'])\n    CircleMarker(location = [row['ND_GPS_LAT'], row['ND_GPS_LONG']], radius = traffic_avg / 1e+6,\n                 color = traffic_color_map(traffic_avg), fill = True, fill_color = traffic_color_map(traffic_avg), fill_opacity = 0.7,\n                 tooltip = f'노드명 : {row[\"ND_NM\"]} <br> 평균 교통량 : {traffic_avg} <br> 공시지가 : {price}').add_to(traffic_group)\n    CircleMarker(location = [row['ND_GPS_LAT'], row['ND_GPS_LONG']], radius = price / 1e+6,\n                 color = price_color_map(price), fill = True, fill_color = price_color_map(price), fill_opacity = 0.7,\n                 tooltip = f'노드명 : {row[\"ND_NM\"]} <br> 평균 교통량 : {traffic_avg} <br> 공시지가 : {price}').add_to(price_group)    \n\ntraffic_group.add_to(m)\ntraffic_color_map.add_to(m)\nprice_group.add_to(m)\nprice_color_map.add_to(m)\n\n# 시군구별 유동인구 시각화\nfolium.Choropleth(name = 'Foot Traffic', geo_data = foot_traffic.__geo_interface__,\n                  data = foot_traffic['유동인구'], key_on = 'feature.id',\n                  fill_color = 'YlGnBu', overlay = True, legend_name = 'Foot Traffic').add_to(m)\n\n# Layer 생성\nlayer_control = LayerControl()\nlayer_control.add_to(m)\n\n# m.save('drivethru.html')\nm"
  },
  {
    "objectID": "Project/DriveThru/DriveThru.html#입지-선정-분석",
    "href": "Project/DriveThru/DriveThru.html#입지-선정-분석",
    "title": "드라이브스루 카페 매장 입지 선정",
    "section": "입지 선정 분석 ",
    "text": "입지 선정 분석 \n \n\n버퍼 밖의 노드 추출 \n\n\nCode\n# 드라이브스루 매장 5km 버퍼 밖의 노드 추출\noutside_node = node_price.loc[~node_price['geometry'].apply(lambda x : union_DT.contains(x))]\nprint(f'5km 반경 내에 드라이브스루 매장이 없는 노드 수 : {len(outside_node)} / {node_price.shape[0]}')\ndisplay(outside_node)\n\n\n5km 반경 내에 드라이브스루 매장이 없는 노드 수 : 289 / 556\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      ND_GPS_LAT\n      ND_GPS_LONG\n      TRAFFIC_AVG\n      행정동\n      공시지가\n      geometry\n    \n  \n  \n    \n      1\n      가락IC\n      35.165\n      128.897\n      2210073.333\n      부산광역시 강서구 봉림동\n      619558.824\n      POINT (128.89672 35.16514)\n    \n    \n      2\n      가산IC\n      36.096\n      128.534\n      1572837.917\n      경상북도 칠곡군 가산면\n      119028.684\n      POINT (128.53356 36.09579)\n    \n    \n      3\n      가조IC\n      35.704\n      128.027\n      1309385.583\n      경상남도 거창군 가조면\n      96475.714\n      POINT (128.02678 35.70360)\n    \n    \n      4\n      감곡IC\n      37.125\n      127.670\n      3893094.667\n      충청북도 음성군 감곡면\n      239219.048\n      POINT (127.66984 37.12458)\n    \n    \n      5\n      강릉IC\n      37.750\n      128.839\n      1012336.667\n      강원도 강릉시 성산면\n      58057.025\n      POINT (128.83921 37.74965)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      549\n      황간IC\n      36.222\n      127.906\n      1969053.750\n      충청북도 영동군 황간면\n      68814.947\n      POINT (127.90614 36.22220)\n    \n    \n      550\n      황전IC\n      35.147\n      127.456\n      1421661.417\n      전라남도 순천시 황전면\n      21761.136\n      POINT (127.45645 35.14727)\n    \n    \n      552\n      회인IC\n      36.487\n      127.609\n      2718412.417\n      충청북도 보은군 회인면\n      30667.676\n      POINT (127.60872 36.48692)\n    \n    \n      553\n      횡성IC\n      37.505\n      127.970\n      1328979.250\n      강원도 횡성군 횡성읍\n      310868.510\n      POINT (127.96998 37.50492)\n    \n    \n      555\n      흥천이포IC\n      37.378\n      127.517\n      3021388.583\n      경기도 여주시 흥천면\n      95101.923\n      POINT (127.51665 37.37836)\n    \n  \n\n289 rows × 7 columns\n\n\n\n\n\n회귀분석 \n\n종속변수 : 드라이브스루 존재여부\n\n드라이브스루 매장 5km 버퍼 밖의 노드 → 드라이브스루 존재여부 = 0\n드라이브스루 매장 5km 버퍼 내의 노드 → 드라이브스루 존재여부 = 1\n\n설명변수 : 노드별 교통량(TRAFFIC_AVG), 행정동별 공시지가(공시지가) & 유동인구\n\n단위를 고려하기 위하여 Scale 하여 분석 진행\n공시지가 : 공시지가가 높을수록 입지선정의 감점요인으로 하기 위하여 -로 분석 진행\n유동인구 : 7대광역시 & 서울특별시의 시군구별 데이터가 존재하므로 따로 분석 진행\n\n\n\n\nCode\n# 종속변수 생성\nnode_price.loc[node_price.index[~node_price.index.isin(outside_node['ND_NM'].index)], '드라이브스루_존재여부'] = 1\nnode_price.loc[outside_node['ND_NM'].index, '드라이브스루_존재여부'] = 0\n\nnode_price['드라이브스루_존재여부'] = node_price['드라이브스루_존재여부'].astype(int)\nnode_price\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      ND_GPS_LAT\n      ND_GPS_LONG\n      TRAFFIC_AVG\n      행정동\n      공시지가\n      geometry\n      드라이브스루_존재여부\n    \n  \n  \n    \n      0\n      88JC\n      37.578\n      126.818\n      1262486.333\n      서울특별시 강서구 방화동\n      4249047.222\n      POINT (126.81788 37.57823)\n      1\n    \n    \n      1\n      가락IC\n      35.165\n      128.897\n      2210073.333\n      부산광역시 강서구 봉림동\n      619558.824\n      POINT (128.89672 35.16514)\n      0\n    \n    \n      2\n      가산IC\n      36.096\n      128.534\n      1572837.917\n      경상북도 칠곡군 가산면\n      119028.684\n      POINT (128.53356 36.09579)\n      0\n    \n    \n      3\n      가조IC\n      35.704\n      128.027\n      1309385.583\n      경상남도 거창군 가조면\n      96475.714\n      POINT (128.02678 35.70360)\n      0\n    \n    \n      4\n      감곡IC\n      37.125\n      127.670\n      3893094.667\n      충청북도 음성군 감곡면\n      239219.048\n      POINT (127.66984 37.12458)\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      551\n      회덕JC\n      36.404\n      127.419\n      7347094.333\n      대전광역시 대덕구 신대동\n      408184.211\n      POINT (127.41924 36.40428)\n      1\n    \n    \n      552\n      회인IC\n      36.487\n      127.609\n      2718412.417\n      충청북도 보은군 회인면\n      30667.676\n      POINT (127.60872 36.48692)\n      0\n    \n    \n      553\n      횡성IC\n      37.505\n      127.970\n      1328979.250\n      강원도 횡성군 횡성읍\n      310868.510\n      POINT (127.96998 37.50492)\n      0\n    \n    \n      554\n      흥덕IC\n      37.163\n      127.046\n      1051736.583\n      경기도 오산시 궐동\n      1502220.930\n      POINT (127.04561 37.16303)\n      1\n    \n    \n      555\n      흥천이포IC\n      37.378\n      127.517\n      3021388.583\n      경기도 여주시 흥천면\n      95101.923\n      POINT (127.51665 37.37836)\n      0\n    \n  \n\n556 rows × 8 columns\n\n\n\n\n분석 1 : 설명변수 (노드별 교통량 & 행정동별 공시지가) \n\n\nCode\n# 설명변수 : 노드별 교통량 + 행정동별 공시지가\nols = smf.ols('드라이브스루_존재여부~scale(TRAFFIC_AVG)+scale(-공시지가)', data = node_price).fit()\ntraffic_beta, price_beta = ols.params[1], ols.params[2]\nprint(ols.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            드라이브스루_존재여부   R-squared:                       0.184\nModel:                            OLS   Adj. R-squared:                  0.181\nMethod:                 Least Squares   F-statistic:                     62.46\nDate:                Tue, 20 Jun 2023   Prob (F-statistic):           3.49e-25\nTime:                        15:07:24   Log-Likelihood:                -346.48\nNo. Observations:                 556   AIC:                             699.0\nDf Residuals:                     553   BIC:                             711.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.4802      0.019     25.026      0.000       0.443       0.518\nscale(TRAFFIC_AVG)     0.1627      0.020      8.140      0.000       0.123       0.202\nscale(-공시지가)          -0.1013      0.020     -5.065      0.000      -0.141      -0.062\n==============================================================================\nOmnibus:                      358.166   Durbin-Watson:                   1.887\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               34.536\nSkew:                           0.118   Prob(JB):                     3.17e-08\nKurtosis:                       1.802   Cond. No.                         1.33\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# 최적 입지 선정\nDT_not_exist = node_price[node_price['드라이브스루_존재여부'] == 0]\nDT_not_exist['Value'] = traffic_beta * DT_not_exist['TRAFFIC_AVG'] + price_beta * DT_not_exist['공시지가']\nbest_DTplace = DT_not_exist.sort_values(by = 'Value', ascending = False)[['ND_NM', '행정동', 'TRAFFIC_AVG', '공시지가', 'Value', 'geometry']]\nbest_DTplace.head()\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      행정동\n      TRAFFIC_AVG\n      공시지가\n      Value\n      geometry\n    \n  \n  \n    \n      542\n      호법JC\n      경기도 이천시 호법면\n      12055637.667\n      125253.571\n      1949243.358\n      POINT (127.43140 37.22247)\n    \n    \n      257\n      북천안IC\n      충청남도 천안서북구 입장면\n      9245694.250\n      220232.407\n      1482336.576\n      POINT (127.18891 36.91753)\n    \n    \n      265\n      산곡JC\n      경기도 하남시 상산곡동\n      8036839.333\n      900745.238\n      1216694.660\n      POINT (127.23924 37.48998)\n    \n    \n      491\n      청주JC\n      충청북도 청주서원구 남이면\n      7369579.167\n      152684.559\n      1183859.080\n      POINT (127.43074 36.56477)\n    \n    \n      432\n      이천IC\n      경기도 이천시 대월면\n      7318889.583\n      158209.028\n      1175050.440\n      POINT (127.49872 37.23730)\n    \n  \n\n\n\n\n\n\n분석 2 : 설명변수 (노드별 교통량 & 행정동별 공시지가 & 시군구별 유동인구) \n\n\nCode\n# 7대광역시 & 서울특별시만 추출\nnode_price_8 = node_price[node_price['행정동'].str.contains('서울특별시|부산광역시|대구광역시|인천광역시|광주광역시|대전광역시|울산광역시|세종특별자치시')]\n\n# 시군구 열 생성\nnode_price_8['시군구'] = [' '.join(sgg.split(' ')[:2]) for sgg in node_price_8['행정동']]\nnode_price_8.loc[node_price_8['시군구'].str.contains('세종'), '시군구'] = '세종특별자치시'\n\n# 유동인구 데이터 결합\nnode_price_8_ft = pd.merge(node_price_8, foot_traffic, how = 'left', on = '시군구')[['ND_NM', '시군구', 'TRAFFIC_AVG', '공시지가', '유동인구', 'geometry_x', '드라이브스루_존재여부']]\nnode_price_8_ft.rename(columns = {'geometry_x' : 'geometry'}, inplace = True)\nnode_price_8_ft\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      시군구\n      TRAFFIC_AVG\n      공시지가\n      유동인구\n      geometry\n      드라이브스루_존재여부\n    \n  \n  \n    \n      0\n      88JC\n      서울특별시 강서구\n      1262486.333\n      4249047.222\n      11346899.301\n      POINT (126.81788 37.57823)\n      1\n    \n    \n      1\n      가락IC\n      부산광역시 강서구\n      2210073.333\n      619558.824\n      5403762.761\n      POINT (128.89672 35.16514)\n      0\n    \n    \n      2\n      강일IC\n      서울특별시 강동구\n      14038357.917\n      5080625.000\n      7361531.090\n      POINT (127.16778 37.57061)\n      1\n    \n    \n      3\n      검단양촌IC\n      인천광역시 서구\n      1931639.083\n      923603.774\n      4066178.788\n      POINT (126.61630 37.60277)\n      1\n    \n    \n      4\n      계룡IC\n      대전광역시 서구\n      1495770.333\n      67343.750\n      5410854.434\n      POINT (127.28002 36.25240)\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      91\n      판암IC\n      대전광역시 동구\n      3103784.833\n      189604.545\n      4162760.947\n      POINT (127.47249 36.31691)\n      1\n    \n    \n      92\n      학익JC\n      인천광역시 미추홀구\n      3635165.403\n      1852510.949\n      3013285.943\n      POINT (126.65038 37.43701)\n      1\n    \n    \n      93\n      해운대IC\n      부산광역시 기장군\n      4015413.583\n      1526396.644\n      7954301.890\n      POINT (129.20720 35.21800)\n      1\n    \n    \n      94\n      현풍JC\n      대구광역시 달성군\n      2336529.417\n      752376.336\n      6184350.915\n      POINT (128.43462 35.67642)\n      0\n    \n    \n      95\n      회덕JC\n      대전광역시 대덕구\n      7347094.333\n      408184.211\n      4574215.855\n      POINT (127.41924 36.40428)\n      1\n    \n  \n\n96 rows × 7 columns\n\n\n\n\n\nCode\n# 설명변수 : 노드별 교통량 + 행정동별 공시지가 + 시군구별 유동인구\nols = smf.ols('드라이브스루_존재여부~scale(TRAFFIC_AVG)+scale(-공시지가)+scale(유동인구)', data = node_price_8_ft).fit()\ntraffic_beta, price_beta, foot_beta = ols.params[1], ols.params[2], ols.params[3]\nprint(ols.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            드라이브스루_존재여부   R-squared:                       0.086\nModel:                            OLS   Adj. R-squared:                  0.056\nMethod:                 Least Squares   F-statistic:                     2.887\nDate:                Tue, 20 Jun 2023   Prob (F-statistic):             0.0398\nTime:                        15:07:24   Log-Likelihood:                -48.687\nNo. Observations:                  96   AIC:                             105.4\nDf Residuals:                      92   BIC:                             115.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.7708      0.042     18.401      0.000       0.688       0.854\nscale(TRAFFIC_AVG)     0.1175      0.042      2.804      0.006       0.034       0.201\nscale(-공시지가)           0.0338      0.050      0.671      0.504      -0.066       0.134\nscale(유동인구)           -0.0001      0.050     -0.003      0.998      -0.100       0.100\n==============================================================================\nOmnibus:                       16.861   Durbin-Watson:                   2.217\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               20.380\nSkew:                          -1.100   Prob(JB):                     3.75e-05\nKurtosis:                       2.499   Cond. No.                         1.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# 최적 입지 선정\nDT_not_exist = node_price_8_ft[node_price_8_ft['드라이브스루_존재여부'] == 0]\nDT_not_exist['Value'] = traffic_beta * DT_not_exist['TRAFFIC_AVG'] + price_beta * DT_not_exist['공시지가'] + foot_beta * DT_not_exist['유동인구']\nbest_DTplace = DT_not_exist.sort_values(by = 'Value', ascending = False)[['ND_NM', '시군구', 'TRAFFIC_AVG', '공시지가', '유동인구', 'Value', 'geometry']]\nbest_DTplace.head()\n\n\n\n\n\n\n  \n    \n      \n      ND_NM\n      시군구\n      TRAFFIC_AVG\n      공시지가\n      유동인구\n      Value\n      geometry\n    \n  \n  \n    \n      58\n      서울제물포선종점\n      서울특별시 영등포구\n      711671.000\n      21997916.667\n      10713417.284\n      826531.967\n      POINT (126.92253 37.52465)\n    \n    \n      61\n      서초IC\n      서울특별시 서초구\n      437.500\n      18325167.139\n      11298282.011\n      618583.657\n      POINT (127.02584 37.48362)\n    \n    \n      41\n      북대전IC\n      대전광역시 유성구\n      4209267.167\n      330571.429\n      9627192.018\n      504603.051\n      POINT (127.38142 36.41131)\n    \n    \n      88\n      장안IC\n      부산광역시 기장군\n      3716945.333\n      432402.874\n      7954301.890\n      450412.387\n      POINT (129.23646 35.32755)\n    \n    \n      77\n      온양IC\n      울산광역시 울주군\n      3463105.500\n      496312.169\n      4106180.778\n      423264.242\n      POINT (129.26061 35.41372)"
  },
  {
    "objectID": "Project/Earthquake/Earthquake.html",
    "href": "Project/Earthquake/Earthquake.html",
    "title": "대한민국, 지진으로부터 안전한가 ?",
    "section": "",
    "text": "2023 데이터시각화 프로젝트"
  },
  {
    "objectID": "Project/Earthquake/Earthquake.html#연구-개요",
    "href": "Project/Earthquake/Earthquake.html#연구-개요",
    "title": "대한민국, 지진으로부터 안전한가 ?",
    "section": "연구 개요",
    "text": "연구 개요\n\n최근 강원도 동해 상의 지진이 다수 발생\n2018 ~ 2023년까지의 지진 발생 횟수 & 지역별 발생 횟수 확인\n지진대피시설 마련 잘 되어있는지 확인"
  },
  {
    "objectID": "Project/Earthquake/Earthquake.html#뉴스",
    "href": "Project/Earthquake/Earthquake.html#뉴스",
    "title": "대한민국, 지진으로부터 안전한가 ?",
    "section": "뉴스",
    "text": "뉴스\n\n동해 지진 한달새 61번…‘미지의 단층’ 꿈틀댄다\n지진 자꾸 나는데…허술한 지진해일 대피시설\n동해 지진 다음은? 7.0 이상 배제할 수 없는 이유\n“55일간 55차례” 동해 해역서 지진 잇따라…센 지진은 모두 ‘한 곳’\n심상치 않은 동해 단층…“더 큰 지진 배제 못 해”\n전북 육지·해역서 5년간 규모 2.0 이상 지진 18건 발생\n동해 해저에 무슨 일이…최근 잇단 지진, 4.5 강진 전조였나\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport math\nimport requests\nimport xmltodict\nimport json\nimport urllib.request\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport squarify\n\nimport geopandas as gpd\nimport folium\nfrom folium import CircleMarker, Marker, LayerControl, GeoJson, Popup, IFrame\nfrom folium.plugins import MarkerCluster\nfrom folium.features import CustomIcon\n\nimport warnings\nwarnings.filterwarnings('ignore', category = UserWarning)\npd.options.mode.chained_assignment = None\n\n# 한글 폰트 사용을 위해서 세팅\nfrom matplotlib import font_manager, rc, rcParams\nfont_path = \"C:/Windows/Fonts/malgun.ttf\"\nfont = font_manager.FontProperties(fname = font_path).get_name()\nrc('font', family = font)\n\n# 마이너스 깨짐 현상 해결\nrcParams['axes.unicode_minus'] = False\n\n\nc:\\Users\\user\\anaconda3\\envs\\quartoenv\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn("
  },
  {
    "objectID": "Project/Earthquake/Earthquake.html#데이터-수집-및-전처리",
    "href": "Project/Earthquake/Earthquake.html#데이터-수집-및-전처리",
    "title": "대한민국, 지진으로부터 안전한가 ?",
    "section": "데이터 수집 및 전처리",
    "text": "데이터 수집 및 전처리\n\n국내 지진 발생 현황\n\nhttps://www.weather.go.kr/w/eqk-vol/search/korea.do\n\n\n\nCode\nEQ_korea_1822 = pd.read_excel('./국내지진목록_미세지진포함_2018-01-01_2022-12-31.xlsx', header = 1,\n                              usecols = ['발생시각', '규모', '깊이(km)', '위도', '경도', '위치'], na_values = '-')\nEQ_korea_23 = pd.read_excel('./국내지진목록_미세지진포함_2023-01-01_2023-05-31.xlsx', header = 1,\n                            usecols = ['발생시각', '규모', '깊이(km)', '위도', '경도', '위치'], na_values = '-')\n\nEQ_korea = pd.concat([EQ_korea_23, EQ_korea_1822], axis = 0).reset_index(drop = True)\nEQ_korea.head()\n\n\n\n\n\n\n  \n    \n      \n      발생시각\n      규모\n      깊이(km)\n      위도\n      경도\n      위치\n    \n  \n  \n    \n      0\n      2023-05-31 21:47:15\n      1.2\n      15.0\n      35.76 N\n      129.19 E\n      경북 경주시 남남서쪽 11km 지역\n    \n    \n      1\n      2023-05-31 10:09:54\n      1.3\n      18.0\n      35.73 N\n      127.31 E\n      전북 진안군 서남서쪽 12km 지역\n    \n    \n      2\n      2023-05-30 18:29:41\n      1.9\n      8.0\n      39.22 N\n      125.17 E\n      북한 평안북도 정주 남쪽 52km 해역\n    \n    \n      3\n      2023-05-30 16:21:58\n      1.6\n      10.0\n      38.13 N\n      127.10 E\n      경기 연천군 북북동쪽 4km 지역\n    \n    \n      4\n      2023-05-29 10:56:39\n      1.0\n      6.0\n      36.07 N\n      129.32 E\n      경북 포항시 북구 북서쪽 5km 지역\n    \n  \n\n\n\n\n\n\nCode\n# 결측치 -> -1로 표기\nEQ_korea.loc[EQ_korea['깊이(km)'].isnull(), '깊이(km)'] = -1\n\nEQ_korea['깊이(km)'] = EQ_korea['깊이(km)'].astype('float')\nEQ_korea['경도'] = [float(longitude.split()[0]) for longitude in EQ_korea['경도']]\nEQ_korea['위도'] = [float(latitude.split()[0]) for latitude in EQ_korea['위도']]\n\nEQ_korea['연도'] = EQ_korea['발생시각'].dt.year\nEQ_korea['년월'] = EQ_korea['발생시각'].dt.strftime('%Y-%m')\nEQ_korea['지역'] = EQ_korea['위치'].apply(lambda x: x.split()[0])\n\nEQ_korea.head()\n\n\n\n\n\n\n  \n    \n      \n      발생시각\n      규모\n      깊이(km)\n      위도\n      경도\n      위치\n      연도\n      년월\n      지역\n    \n  \n  \n    \n      0\n      2023-05-31 21:47:15\n      1.2\n      15.0\n      35.76\n      129.19\n      경북 경주시 남남서쪽 11km 지역\n      2023\n      2023-05\n      경북\n    \n    \n      1\n      2023-05-31 10:09:54\n      1.3\n      18.0\n      35.73\n      127.31\n      전북 진안군 서남서쪽 12km 지역\n      2023\n      2023-05\n      전북\n    \n    \n      2\n      2023-05-30 18:29:41\n      1.9\n      8.0\n      39.22\n      125.17\n      북한 평안북도 정주 남쪽 52km 해역\n      2023\n      2023-05\n      북한\n    \n    \n      3\n      2023-05-30 16:21:58\n      1.6\n      10.0\n      38.13\n      127.10\n      경기 연천군 북북동쪽 4km 지역\n      2023\n      2023-05\n      경기\n    \n    \n      4\n      2023-05-29 10:56:39\n      1.0\n      6.0\n      36.07\n      129.32\n      경북 포항시 북구 북서쪽 5km 지역\n      2023\n      2023-05\n      경북\n    \n  \n\n\n\n\n\n\n지진해일대피시설\n\nhttps://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=3058512\n\n\n\nCode\nurl = 'http://apis.data.go.kr/1741000/TsunamiShelter3/getTsunamiShelter1List'\nkey = 'shFuHgru3Fq%2FST42q0yoINPYirM93L5LD%2BZRckD1mFeW97H2u0Lak5thWkh7RzTum00bLj7GIlvyLlzXt3AxZw%3D%3D'\ndata_url = url + '?serviceKey=' + key + '&pageNo' + str(1) + '&numOfRows=' + str(1000) + '&type=' + str('xml')\n\nshelter = pd.DataFrame(xmltodict.parse(requests.get(data_url).text)['TsunamiShelter']['row'])\n\n# 자료형 변환\nshelter[['lon', 'lat']] = shelter[['lon', 'lat']].astype(float)\nshelter[['shel_av', 'lenth', 'height']] = shelter[['shel_av', 'lenth', 'height']].astype(int)\n\n# 이상치 처리\nshelter.loc[shelter['lat'] > 50, ['lon', 'lat']] = shelter.loc[shelter['lat'] > 50, ['lat', 'lon']].values\n\nTsunamishelter = shelter[['sido_name', 'sigungu_name', 'shel_nm', 'lon', 'lat']]\nTsunamishelter.columns = ['시도명', '시군구명', '대피소명', '경도', '위도']\nTsunamishelter\n\n\n\n\n\n\n  \n    \n      \n      시도명\n      시군구명\n      대피소명\n      경도\n      위도\n    \n  \n  \n    \n      0\n      강원도\n      삼척시\n      삼림욕장공터\n      129.227908\n      37.392228\n    \n    \n      1\n      강원도\n      삼척시\n      관덕정 인근 공터\n      129.222539\n      37.380109\n    \n    \n      2\n      강원도\n      고성군\n      거성초등학교 고지대\n      128.457383\n      38.452580\n    \n    \n      3\n      강원도\n      고성군\n      동호리뒷산 고지대\n      128.476095\n      38.391802\n    \n    \n      4\n      강원도\n      고성군\n      가진항뒷산 고지대\n      128.510808\n      38.368111\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      648\n      전라남도\n      화순군\n      천태초등학교 운동장\n      126.893400\n      34.945300\n    \n    \n      649\n      강원도\n      삼척시\n      나리골 향기원주차장\n      129.187600\n      37.442100\n    \n    \n      650\n      강원도\n      삼척시\n      나리골 바람전망대\n      129.188600\n      37.442600\n    \n    \n      651\n      강원도\n      속초시\n      영금공원고지대\n      128.599975\n      38.213880\n    \n    \n      652\n      강원도\n      속초시\n      중앙공원고지대\n      128.585083\n      38.215271\n    \n  \n\n653 rows × 5 columns\n\n\n\n\n\n지진옥외대피시설\n\nhttps://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=15039250\n\n\n\nCode\nurl = 'https://apis.data.go.kr/1741000/EmergencyAssemblyArea_Earthquake2/getArea1List'\nkey = 'shFuHgru3Fq%2FST42q0yoINPYirM93L5LD%2BZRckD1mFeW97H2u0Lak5thWkh7RzTum00bLj7GIlvyLlzXt3AxZw%3D%3D'\n\nbombshelter = pd.DataFrame()\n\nfor n in range(1, 13) :\n    data_url = url + '?serviceKey=' + key + '&pageNo=' + str(n) + '&numOfRows=' + str(1000) + '&type=' + str('xml')\n    outside_shelter = pd.DataFrame(xmltodict.parse(requests.get(data_url).text)['EarthquakeOutdoorsShelter']['row'])\n    bombshelter = pd.concat([bombshelter, outside_shelter], axis = 0).reset_index(drop = True)\n\nEarthquakeshelter = bombshelter[['ctprvn_nm', 'sgg_nm', 'vt_acmdfclty_nm', 'xcord', 'ycord']]\nEarthquakeshelter.columns = ['시도명', '시군구명', '대피소명', '경도', '위도']\nEarthquakeshelter[['경도', '위도']] = Earthquakeshelter[['경도', '위도']].astype(float)\nEarthquakeshelter\n\n\n\n\n\n\n  \n    \n      \n      시도명\n      시군구명\n      대피소명\n      경도\n      위도\n    \n  \n  \n    \n      0\n      서울특별시\n      서대문구\n      미동초등학교 운동장\n      126.965894\n      37.562845\n    \n    \n      1\n      서울특별시\n      서대문구\n      감리교신학대 운동장\n      126.963036\n      37.567195\n    \n    \n      2\n      서울특별시\n      서대문구\n      인창중학교 운동장\n      126.963616\n      37.565060\n    \n    \n      3\n      서울특별시\n      서대문구\n      인창고등학교운동장\n      126.963624\n      37.565066\n    \n    \n      4\n      서울특별시\n      은평구\n      신도초등학교 운동장\n      126.928937\n      37.632518\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      11226\n      경상북도\n      영덕군\n      남정리 승강장 앞 공터\n      129.361635\n      36.346435\n    \n    \n      11227\n      경상북도\n      영덕군\n      부흥리 경로당 앞 공터\n      129.373831\n      36.289740\n    \n    \n      11228\n      경상북도\n      영덕군\n      영덕문화예술체험장\n      129.302817\n      36.399566\n    \n    \n      11229\n      경상북도\n      영덕군\n      용평리경로당 앞 공터\n      129.288984\n      36.389845\n    \n    \n      11230\n      경상북도\n      영덕군\n      매일1리 마을회관 앞 공터\n      129.302919\n      36.374511\n    \n  \n\n11231 rows × 5 columns\n\n\n\n\n\nCode\nshelter_all = pd.concat([Tsunamishelter, Earthquakeshelter], axis = 0)\nshelter_all = shelter_all.drop_duplicates().reset_index(drop = True)\n\n# 이상치 처리\nshelter_all.loc[shelter_all['대피소명'] == '신도림초등학교 운동장', '경도'] = 126.883304\nshelter_all.shape\n\n\n(11881, 5)\n\n\n\n\n지진 판 경계\n\nhttps://github.com/fraxen/tectonicplates\n\n\n\nCode\nboundaries_url = 'https://raw.githubusercontent.com/fraxen/tectonicplates/master/GeoJSON/PB2002_boundaries.json'\nboundaries = json.load(urllib.request.urlopen(boundaries_url))\n\n\n\n\n대한민국 행정구역 경계\n\nhttps://github.com/southkorea/southkorea-maps\n\n\n\nCode\nsido_boundaries_url = 'https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json'\nsido_boundaries = json.load(urllib.request.urlopen(sido_boundaries_url))"
  },
  {
    "objectID": "Project/Earthquake/Earthquake.html#데이터-시각화",
    "href": "Project/Earthquake/Earthquake.html#데이터-시각화",
    "title": "대한민국, 지진으로부터 안전한가 ?",
    "section": "데이터 시각화",
    "text": "데이터 시각화\n\n(1) 2018~2023년 연도별 지진 발생 횟수\n\n\nCode\nEQ_count = EQ_korea.groupby('연도', as_index = False).size().rename(columns = {'size' : '지진발생횟수'})\nEQ_count = EQ_count[EQ_count['연도'] != 2023]\n\nEQ_korea_0105 = EQ_korea[EQ_korea['년월'].str[5:7].isin(['01', '02', '03', '04', '05'])]\nEQ_count_0105 = EQ_korea_0105.groupby('연도', as_index = False).size().rename(columns = {'size' : '지진발생횟수'})\n\nfig, ax = plt.subplots(figsize = (10, 8))\n\nsns.barplot(x = '연도', y = '지진발생횟수', data = EQ_count, color = '#ff9f9b', ax = ax, label = '1~12월 지진 발생 횟수')\nsns.pointplot(x = '연도', y = '지진발생횟수', data = EQ_count, color = '#ff9f9b', ax = ax, scale = .7)\nsns.barplot(x = '연도', y = '지진발생횟수', data = EQ_count_0105, color = '#a1c9f4', ax = ax, label = '1~5월 지진 발생 횟수')\nsns.pointplot(x = '연도', y = '지진발생횟수', data = EQ_count_0105, color = '#a1c9f4', ax = ax, scale = .7)\n\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:,.0f}건'.format(y)))\nax.set_xticklabels([f'{i}년' for i in range(2018, 2024)])\nax.set_xlabel('연도', fontsize = 15, fontweight = 'semibold')\nax.set_ylabel('발생 횟수', fontsize = 15, fontweight = 'semibold')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.tick_params(axis = 'x', length = 0, labelsize = 13)\nax.tick_params(axis = 'y', length = 0, labelsize = 13)\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc = 'best', fontsize = '13')\n\nfor p in ax.patches:\n    ax.annotate('{:,.0f}건'.format(p.get_height()),\n                (p.get_x() + p.get_width() / 2, p.get_height()),\n                ha = 'center', va = 'bottom', xytext = (0, 5),\n                textcoords = 'offset points', fontsize = 14,\n                weight = 'semibold')\n\nplt.title('연도별 지진 발생 횟수', fontsize = 17, fontweight = 'bold')\nplt.show()\n\n\n\n\n\n\n\n(2) 2018~2023년 연도별 지진 규모\n\n\nCode\nfig, ax = plt.subplots(figsize = (10, 8))\n\nsns.boxplot(x = '연도', y = '규모', data = EQ_korea, ax = ax, palette = 'Set2')\nax.axhline(y = 3, color = 'orange', linestyle = 'dotted')\nax.axhline(y = 4, color = 'red', linestyle = 'dotted')\n\nax.set_xticklabels([f'{i}년' for i in range(2018, 2024)])\nax.set_xlabel('연도', fontsize = 15, fontweight = 'semibold')\nax.set_ylabel('규모', fontsize = 15, fontweight = 'semibold')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(True)\nax.spines['left'].set_visible(True)\nax.tick_params(axis = 'x', length = 0, labelsize = 13)\nax.tick_params(axis = 'y', length = 0, labelsize = 13)\n\nmedians = EQ_korea.groupby('연도')['규모'].median()\nfor year, median in enumerate(medians) :\n    ax.text(year, median, f'{median}', ha = 'center', va = 'bottom',\n            fontsize = 14, color = 'white', fontweight = 'semibold')\n\nplt.title('연도별 지진 규모', fontsize = 17, fontweight = 'bold')\nplt.show()\n\n\n\n\n\n\n\n(3) 2018~2023년 지역별 지진 발생 횟수\n\n\nCode\nEQ_region_count = EQ_korea.groupby('지역', as_index = False).size().rename(columns = {'size' : '지진발생횟수'})\nEQ_region_count = EQ_region_count[EQ_region_count['지역'] != '북한']\nEQ_region_count.loc[EQ_region_count['지역'] == '세종시', '지역'] = '세종'\nEQ_region_count.sort_values(by = '지진발생횟수', ascending = False, inplace = True)\nEQ_region_count['지진발생횟수_SQRT'] = np.sqrt(EQ_region_count['지진발생횟수'])\n\nfig, ax = plt.subplots(figsize = (16, 12))\n\nsizes = EQ_region_count['지진발생횟수_SQRT']\nlabels = [f'{region}\\n({count:,}건)' for region, count in zip(EQ_region_count['지역'], EQ_region_count['지진발생횟수'])]\ncolors = sns.color_palette('GnBu_r', len(EQ_region_count))\n\nsquarify.plot(sizes = sizes, label = labels, color = colors, alpha = 0.7,\n              text_kwargs = {'fontweight' : 'semibold'})\n\nfor label, square in zip(ax.texts, ax.patches) :\n    size_ratio = square.get_width() / max(sizes/10)\n    label.set_fontsize(15 + 2 * size_ratio)\n\nplt.title('시도별 지진 발생 횟수', fontsize = 25, fontweight = 'bold')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n(4) 지역별 지진 대피 시설 수\n\n\nCode\nshelter_count = shelter_all.groupby('시도명', as_index = False).size()\nshelter_count.columns = ['name', '시설 수']\n\nsido_dict = {'경상북도' : '경북', '경상남도' : '경남', '전라북도' : '전북', '전라남도' : '전남', '충청북도' : '충북', '충청남도' : '충남'}\nshelter_count['시도'] = shelter_count['name'].map(lambda x: sido_dict.get(x, x[:2]))\nshelter_count.sort_values(by = '시설 수', ascending = False, inplace = True)\n\nsido_boundaries = gpd.read_file(sido_boundaries_url)\nsido_shelter_count = sido_boundaries.merge(shelter_count, on = 'name', how = 'left').fillna(0)\nsido_shelter_count['시설 수'] = sido_shelter_count['시설 수'].astype('int')\n\nfig, ax = plt.subplots(figsize = (16, 16))\n\nsido_shelter_count.plot(column = '시설 수', legend = False, ax = ax,\n                        cmap = 'GnBu', linewidth = 0.3, edgecolor = 'gray')\n\nfor x, y, label, sido in zip(sido_shelter_count.geometry.centroid.x, sido_shelter_count.geometry.centroid.y,\n                             sido_shelter_count['시설 수'], sido_shelter_count['시도']) :\n    if sido == '경기' :\n        ax.text(x + 0.2, y - 0.2, f'{sido}\\n{label:,}개', fontsize = 20, weight = 'semibold', ha = 'center', va = 'center', color = 'white')\n    elif sido == '대전' or sido == '대구' or sido == '전남' :\n        ax.text(x, y - 0.1, f'{sido}\\n{label:,}개', fontsize = 20, weight = 'semibold', ha = 'center', va = 'center')\n    elif sido == '충북' or sido == '인천' :\n        ax.text(x - 0.1, y + 0.1, f'{sido}\\n{label:,}개', fontsize = 20, weight = 'semibold', ha = 'center', va = 'center')\n    elif sido == '서울' or sido == '경북' : \n        ax.text(x, y, f'{sido}\\n{label:,}개', fontsize = 20, weight = 'semibold', ha = 'center', va = 'center', color = 'white')\n    else:\n        ax.text(x, y, f'{sido}\\n{label:,}개', fontsize = 20, weight = 'semibold', ha = 'center', va = 'center')\n    \nax.set_xlim([125, 131])\nax.set_xticklabels([f'{label.get_text()}°E' for label in ax.get_xticklabels()])\nax.set_yticklabels([f'{label.get_text()}°N' for label in ax.get_yticklabels()])\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.tick_params(axis = 'x', length = 0, labelsize = 17)\nax.tick_params(axis = 'y', length = 0, labelsize = 17)\n\nplt.xlabel('경도', fontsize = 17, fontweight = 'semibold')\nplt.ylabel('위도', fontsize = 17, fontweight = 'semibold')\nplt.title('시도별 지진 대피 시설 수', fontsize = 25, fontweight = 'bold')\n\ncax = fig.add_axes([0.5, 0.25, 0.25, 0.02])\nsm = plt.cm.ScalarMappable(cmap = 'GnBu', norm = plt.Normalize(vmin = 0, vmax = 2000))\nsm.set_array(sido_shelter_count['시설 수'])\ncbar = plt.colorbar(sm, cax = cax, shrink = 0.6, orientation = 'horizontal')\ncbar.ax.tick_params(length = 0, labelsize = 15)\n\nticks = [0, 500, 1000, 1500, 2000]\ncax.xaxis.set_major_locator(ticker.FixedLocator(ticks))\ndef format_thousands(value, pos) : return f'{int(value):,}개'\ncax.xaxis.set_major_formatter(ticker.FuncFormatter(format_thousands))\ncbar.set_label('시설 수', position = (0.5, 1.5), fontsize = 20, fontweight = 'semibold')\n\ncax.xaxis.set_label_coords(0.5, 2.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n(5) 지진 발생 & 지진 대피 시설 & 지진 판 경계 & 대한민국 행정구역 경계 위치 시각화\n\n\nCode\nm = folium.Map(location = [36.5, 128], tiles = 'cartodbpositron', zoom_start = 7)\n\nEQ_korea['Color'] = ['#00CC33' if magnitude < 2 else '#FF9900' if magnitude < 3 else '#FF6666' \\\n    if magnitude < 4 else '#FF0000' for magnitude in EQ_korea['규모']]\n\nfolium.Choropleth(name = 'Plate Boundaries', geo_data = boundaries,\n                  line_color = 'black', line_weight = 2, overlay = True).add_to(m)\n\nfolium.Choropleth(name = 'Korea Boundaries', geo_data = sido_boundaries, line_color = 'gray', line_weight = 2,\n                  fill_color = 'white', fill_opacity = 0.1, overlay = True).add_to(m)\n\nfor idx, row in EQ_korea.iterrows() :\n    CircleMarker(location = [row['위도'], row['경도']], radius = row['규모'] ** 2,\n                 fill = True, fill_opacity = 1, color = row['Color'], fill_color = row['Color'],\n                 tooltip = f'발생 위치 : {str(row[\"위치\"])} <br> 발생 시각 : {str(row[\"발생시각\"])} <br> 규모 : {str(row[\"규모\"])}').add_to(m)\n\nmc = MarkerCluster(name = 'Shelter')\nfor idx, row in shelter_all.iterrows() :\n    if not math.isnan(row['경도']) and not math.isnan(row['위도']) :\n        mc.add_child(Marker(location = [row['위도'], row['경도']], tooltip = row['대피소명'],\n                            icon = CustomIcon('https://www.mercyrelief.org/site/wp-content/uploads/shelter-icon.png', icon_size = (30, 30))))\nm.add_child(mc)\n\nlayer_control = LayerControl()\nlayer_control.add_to(m)\n\nlegend_dict =  {'< 2': '#00CC33', '2 ~ 3': '#FF9900', '3 ~ 4': '#FF6666', '> 4': '#FF0000'}\nlegend_html = '''\n<div style = \"position: fixed; \n     top : 50px; right : 150px; width : 110px; height : 140px; \n     border : 2px solid grey; z-index : 9999; font-size : 18px;\n     background-color : white;\n     \">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b><span style = \"font-size : 20px;\"> 규모 </b> <br>\n&nbsp; <i class = \"fa fa-circle\" style = \"color : #00CC33\"></i> &nbsp;: &lt; 2 &nbsp;<br>\n&nbsp; <i class = \"fa fa-circle\" style = \"color : #FF9900\"></i> &nbsp;: 2 ~ 3 &nbsp;<br>\n&nbsp; <i class = \"fa fa-circle\" style = \"color : #FF6666\"></i> &nbsp;: 3 ~ 4 &nbsp;<br>\n&nbsp; <i class = \"fa fa-circle\" style = \"color : #FF0000\"></i> &nbsp;: &ge; 4 &nbsp;\n</div>\n'''\nm.get_root().html.add_child(folium.Element(legend_html))\n\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Project/Emotion_Classification/Emotion_Classification.html",
    "href": "Project/Emotion_Classification/Emotion_Classification.html",
    "title": "발화자의 감정인식 AI 경진대회",
    "section": "",
    "text": "주제 : 대화 속의 발화자, 발화, 감정 등 요소를 보고 가장 알맞은 감정 분류\n\n\n\n\n\n대회 페이지 바로가기\n\n\n\n \n \n\n최종 순위 : 15등 / 259팀 (상위 6%)\n팀명 : 중꺾마 (중요한 건 꺾이지 않는 마음)\n팀원 : 이진원, 김민규, 양정현, 진동준"
  },
  {
    "objectID": "Project/Jeju_Traffic_Prediction/Jeju_Traffic_Prediction.html",
    "href": "Project/Jeju_Traffic_Prediction/Jeju_Traffic_Prediction.html",
    "title": "제주도 도로 교통량 예측 AI 경진대회",
    "section": "",
    "text": "주제 : 제주도 도로 교통량 예측 AI 알고리즘 개발\n\n\n\n\n대회 페이지 바로가기\n\n\n\n \n\n최종 순위 : 124등 / 1,681팀\n팀명 : 불사조\n팀원 : 이진원, 양소현, 조수지"
  },
  {
    "objectID": "Project/K-Breakwater/K-Breakwater.html",
    "href": "Project/K-Breakwater/K-Breakwater.html",
    "title": "CCTV 영상분석을 통한 도시침수 조기감지 서비스",
    "section": "",
    "text": "주제 : 2022년 심각한 문제인 도시 침수를 CCTV 영상분석을 통해 조기감지하여 피해를 예방하는 플랫폼 구축\n\n \n\n\n\nGithub 페이지 바로가기\n\n\n\n\n\n2022년 데이터 청년 캠퍼스 대상 수상\n산학협력기업 : 한국수자원공사(K-Water)\n팀명 : K-Breakwater\n팀원 : 이다은, 김경민, 김민규, 김현수, 이진원"
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n대한민국, 지진으로부터 안전한가 ?\n\n\n\nPython\n\n\n\n\n\n\n\nJinwon Lee\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n항공편 지연 예측 AI 경진대회\n\n\n\nPython\n\n\n\n\n\n\n\n이진원\n\n\nMay 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n드라이브스루 카페 매장 입지 선정\n\n\n\nPython\n\n\nCrawling\n\n\n\n\n\n\n\nJinwon Lee\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n제주도 도로 교통량 예측 AI 경진대회\n\n\n\nPython\n\n\n\n\n\n\n\n이진원\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n발화자의 감정인식 AI 경진대회\n\n\n\nPython\n\n\n\n\n\n\n\n이진원\n\n\nDec 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCCTV 영상분석을 통한 도시침수 조기감지 서비스\n\n\n\nPython\n\n\nJavascript\n\n\nHTML\n\n\nCSS\n\n\n\n\n\n\n\n이진원\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]